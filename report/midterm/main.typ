#import "./template/neurips.typ": botrule, midrule, paragraph, toprule, url, neurips2024
#import "@preview/timeliney:0.0.1"

#let affls = (
  uw: (
    institution: "University of Wisconsin",
    location: "Madison",
  ),
)

#let authors = (
  (name: "Mondo Jiang", affl: "uw", email: "mondo.jiang@wisc.edu", equal: true),
  (name: "Daniel Smedema", affl: "uw", email: "djsmedema@wisc.edu", equal: true),
)


#show: neurips2024.with(
  title: [
    DevOops: Speeding Up Debugging from Logs\
    #text(size: 0.8em, weight: "light")[Midterm Report]
  ],

  authors: (authors, affls),
  keywords: ("Machine Learning", "DevOps", "CI/CD", "LLM", "Fine Tune"),
  abstract: [
  // I GPTed this, make changes as much as you want! Worth to note this doesn't include any results from our expr so far :p
    In the modern software development landscape, complex tech stacks and automated CI/CD pipelines present unique challenges in error identification and debugging. We explore the potential of leveraging large language models to interpret log streams and assist in debugging, addressing the inefficiencies of current manual methods. We propose a fully-local, fine-tuned language model to identify errors within log texts, preserving proprietary information and providing context-specific insights. We utilize data from the open-source BugSwarm dataset to fine-tune a Llama-3.2 3B model using LoRA. We will show whether the model performs better after fine-tuning by comparing results against a test set of gold-labeled samples, produced by us.
  ],
  bibliography: bibliography("main.bib"),
  bibliography-opts: (title: none, full: true),  // Only for example paper.
  appendix: [],
  accepted: none,
)


// alternative:
// - don't forget to apply a token for bugswarm! (actually not that important for now :p)
// - golden labels from habitate

// Requirement:
// The problem you are solving and its importance.
// Related literature and how your proposal is different.
// Details of concrete methods you have tested so far. One should be able to replicate your results based on the details include in the paper.
// Your experiments and results so far. Explain the setups (models, baselines, metrics, etc.)
// What are the planned methods and experiments for the rest of the semester?
// Proposed timeline. How are you doing based on the initial timeline you had? Propose adjustments to the planned timeline for the rest of the semester.a

#set cite(form: "prose")

= Introduction
// The problem you are solving and its importance.

In the modern era, software applications are developed using complex "tech stacks" that feature multiple interacting components, and these are often deployed into a production application using another automated process, often called a CI/CD pipeline (Continuous Integration/Continuous Deployment). This pipeline itself is code that may fail, and its failure could be due to problems in the pipeline code or problems one or more of the system's constituent components. The error logs generated by these failures can be difficult to read and reason about.

There is often no way to test whether you have fixed a pipeline error other than running the pipeline again, and this may take minutes or even tens of minutes even for relatively small applications. A human troubleshooter will likely not be able to spend these minutes spent waiting on pipeline runs productively on other tasks. Thus it is costly to be wrong about what the actual error is.

With the advent of large language models, the possibility of automated assistance in interpreting semi-structured and dynamic log streams is on the horizon. It is already possible to copy and paste your logs into ChatGPT and ask it questions, often receiving fairly helpful results. However, there are a few downsides to this: a) it may leak your proprietary information to third parties, and b) ChatGPT does not have the full context of your codebase, which may be necessary to properly contextualize the errors.

A fully-local language model avoids the first issue, and fine-tuning it on a single specific codebase addresses the second. In order for this to be economically viable for a small organization, the model would need to be small and based on a pretrained open-source model. For example, the recently released Llama 3.2 with 1B parameters is advertised as fitting onto edge and mobile devices.

As a first step towards generating natural-language "advice" on how to fix issues or even automatically generating fix code, the model must be able to identify the error from within the log text. This will be our focus in this project.

= Related Work
// and how your proposal is different.

Many approaches have been proposed for various code-related tasks. @Sun2024 provided a comprehensive summary of code intelligence, where most efforts focus on building connections between code-to-code and text-to-code generation, enhancement, and understanding. However, there are no well-defined tasks for handling logs generated during build and deployment pipelines.

@Jimenez2023 defined a benchmark to test the code auto-fixing capabilities of models, while @LeGoues2015 focused on automatically repairing bugs already fixed by patches. Their approaches are limited to predefined, well-constructed bugs, rather than identifying issues directly from information generated during building and running processes.

@Kang2023 focused on explainable automated debugging to improve interpretability. Their hypothesis-testing cycle model is particularly insightful, and we may explore applying it to automate the repair process once the core error log is identified.

@Beller2017 and @Tomassi2019 provided datasets that include pipeline logs, historical data, and fix patches, but they do not offer insights into the logs themselves. In the later stages of our experiments, we plan to use their datasets to enhance our modelâ€™s ability to suggest fixes.

Finally, @10017337 applied NLP methods to log summarization and introduced the first gold-standard dataset for this task. However, their approach is not label-efficient and does not leverage the vast knowledge encoded in pretrained LLMs. We will focus on summarizing core error logs and aim to propose a more label-efficient method for generating ground truth.

In summary, our work is different from prior work in that it brings together the power of pre-trained LLMs and label-efficient training techniques on code-related tasks with the domain of build logs, a combination which has not been explored before to our knowledge.

= Method and Progress
// Details of concrete methods you have tested so far. One should be able to replicate your results based on the details include in the paper.

Our main progress so far has been in preparing the technical environment and collecting data. We have elected to use the unsloth python library in order to fine-tune using a Low-Rank Adaptation (LoRA) method (@hu2021loralowrankadaptationlarge). We are using unsloth's version of the Llama-3.2 3B Instruct model, from huggingface as unsloth/Llama-3.2-3B-Instruct-bnb-4bit.

We have set up a training environment in a docker container based on Ubuntu 24.04 image on Mondo's personal computer, using an NVIDIA GeForce RTX 3070 Ti with 8GB memory. An excerpt of our fine-tuning code is in Appendix A.

What is not shown in the excerpt is the details of the dataset being loaded. This is still in-progress, and in fact we have multiple datasets we are still working on processing into a useful form.

== Data Collection
#show "Habitica": [Habitica@link-habitica]

=== Dataset 1 - BugSwarm
In order to test the extent to which generic exposure to DevOps pipeline runs can elicit latent capabilities of the Llama model, we are working on incorporating training using this large and more diverse dataset, BugSwarm (@Tomassi2019). The dataset itself contains raw logs from paired failing and successful runs. We have generated a derived dataset using a diffing program to extract the differences in the log between the successful and failed runs. We believe lines that are added in the diff will be significantly more likely to contain relevant errors.

We managed to download around 4478 reproducible tasks and around 1965 paired raw build logs of passed and failed jobs for the tasks. We also generated diffs with different context window sizes, which will affect how many non-changed lines above or below a changed line are included in the diff output. We are using 0, 2, 4, and 8 as the context window sizes for our current experiments.

We might also adopt another existing database (@Beller2017). We'll continue using these large-scale databases in the first stage of training to focus the model on understanding pipeline logs. After examining the raw logs and diffs, we found that there is a considerable amount of noise in the raw logs, such as progress indicators and temporary/random file name changes. We plan to generate a new set of training data by filtering out this noise using different methods. We will most likely be applying some diversity-based filtering here, following from a number of papers we've read in this class that all emphasize the importance of dataset diversity.

=== Dataset 2 - Single-repo, gold-labeled
In order to reflect the use case of fine-tuning a language model on a single specific codebase to provide specialized insights, we have selected a single open-source project and we will construct a dataset from logs generated exclusively from it. We have selected Habitica as the project, because both authors are familiar with the programming language (JavaScript) and it already has a CI/CD pipeline enabled in GitHub Actions, which can be easily forked and triggered. We are still working on scraping data from its failed GitHub actions. Our current plan is to manually label those logs after adopting some label-efficient methods into our pipeline.

We recently found out that GitHub has released a similar alpha-stage feature called "Explain Errors in Logs," which performs tasks similar to our proposal. However, it focuses more on summarizing the logs and sometimes generates a strong illusion by providing completely wrong answers, as it doesn't give an explicit answer to where the error occurred within the build log. We will still try to compare our results with this feature or use its output as hints in our training process.

=== Dataset 3 - Synthetically-generated
Our original plan was to generate a synthetic dataset by forking our selected open-source project, then creating multiple isolated branches where we intentionally introduced errors into the code. We have postponed this step, likely leaving it out of scope for the project in this class (with a possibility for future work). Since Habitica already has plenty of failed jobs, the data from synthetic bugs are less practical compared to the bugs we obtain from the real history of builds.

== Experiments
// Details of concrete methods you have tested so far. One should be able to replicate your results based on the details include in the paper.

Our primary experimental methodology will be comparing models using a zero-shot prompt, because this better matches the expected final use case of a tool like this (though it is possible a few-shot model could be implemented in a way that is opaque to the user by hiding it within a system prompt). 

Our current template prompt is:

```python
template = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
You are an expert DevOps engineer. Identify the lines from the below error logs that are responsible for the failure of the pipeline.

### Input:
{}

### Response:
{}"""
```

We will conduct experiments comparing a variety of fine-tuning levels, and with the fine-tuning being based on the different datasets, as mentioned above. The different levels of fine-tuning will be achieved by saving training checkpoints, so that we can load in the model for inference from a variety of points along its fine-tuning progression.

== Results
// Your experiments and results so far. Explain the setups (models, baselines, metrics, etc.)

We do not yet have any results to report. The original plan was to evaluate our models' performance using a BLEU score comparing the models' output with the gold label. We are exploring the use of the GitHub feature "Explain Errors in Logs" as a comparison point, as mentioned above.

Our results analysis for models fine-tuned on the diverse dataset from BugSwarm will still be evaluated on the gold labels derived from Habitica. We have seen some results in the papers studied in this class and others which suggest that dataset diversity may be more useful than developing "narrow expertise" in a very specific subset of data.

= Timeline
We are behind our original planned timeline. This is primarily due to the busy middle-of-semester period resulting in the deferral of work on this project. An adjusted timeline is presented below. Columns reflect weeks beginning on a Monday with the listed date.


#timeliney.timeline(
  show-grid: true,
  {
    import timeliney: *
      
    headerline(group(([*Oct*], 3)), group(([*Nov*], 4)), group(([*Dec*], 2)))
    headerline(
      group(strong("14"), strong("21"), strong("28")),
      group(strong("4"), strong("11"), strong("18"), strong("25")),
      group(strong("2"), strong("9"))
    )
  
    taskgroup(title: [*Research*], {
      task([Select Codebase #sym.checkmark], (0, 1), style: (stroke: 2pt + gray))
      task([Literature Review #sym.checkmark], (1, 3), style: (stroke: 2pt + gray))
    })

    taskgroup(title: [*Dataset Development*], {
      task([Diff for existing database #sym.checkmark], (3, 5), style: (stroke: 2pt + gray))
      task([Gold label collection #sym.circle.dotted], (6, 7), style: (stroke: 2pt + gray))
    })
    taskgroup(title: [*Model Fine-Tuning*], {
      task([Write fine-tuning code #sym.checkmark], (4, 5), style: (stroke: 2pt + gray))
      task([Data selection method #sym.circle.dotted], (5, 6), style: (stroke: 2pt + gray))
      task([Fine-tune the model #sym.circle.dotted], (5, 7), style: (stroke: 2pt + gray))
    })

    taskgroup(title: [*Running Experiments*], {
      task([Write experiment code #sym.circle.dotted], (5, 7), style: (stroke: 2pt + gray))
      task("Execute experiments", (6, 8.5), style: (stroke: 2pt + gray))
    })

    taskgroup(title: [*Analyzing Results & Writing Final Report*], {
      task("Analyze Results", (7, 8.5), style: (stroke: 2pt + gray))
      task("Write final report", (8, 9), style: (stroke: 2pt + gray))
    })

    milestone(
      at: 5,
      style: (stroke: (dash: "dashed")),
      align(center, [
        *Mid Report Due*\
        Nov 17
      ])
    )
    milestone(
      at: 9,
      style: (stroke: (dash: "dashed")),
      align(center, [
        *Final Report Due*\
        Dec 15
      ])
    )
  }
)

= Appendix

== A: Training Code Excerpt

```python
    dataset = load_dataset("json", data_files=data_files, split="train")

    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name="unsloth/Llama-3.2-3B-Instruct-bnb-4bit",
        max_seq_length=max_seq_length,
        dtype=None,
        load_in_4bit=True,
    )

    model = FastLanguageModel.get_peft_model(
        model,
        r=16,
        target_modules=[
            "q_proj",
            "k_proj",
            "v_proj",
            "o_proj",
            "gate_proj",
            "up_proj",
            "down_proj",
        ],
        lora_alpha=16,
        lora_dropout=0,  
        bias="none", 
        
        use_gradient_checkpointing="unsloth",
        random_state=3407,
        max_seq_length=max_seq_length,
        use_rslora=False,  
        loftq_config=None, 
    )

    trainer = SFTTrainer(
        model=model,
        train_dataset=dataset,
        dataset_text_field="text",
        max_seq_length=max_seq_length,
        tokenizer=tokenizer,
        args=TrainingArguments(
            per_device_train_batch_size=2,
            gradient_accumulation_steps=4,
            warmup_steps=10,
            max_steps=60,
            fp16=not is_bfloat16_supported(),
            bf16=is_bfloat16_supported(),
            logging_steps=1,
            output_dir="outputs",
            optim="adamw_8bit",
            seed=3407,
        ),
    )

    trainer.train()
```

= References