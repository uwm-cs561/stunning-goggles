#import "./template/neurips.typ": botrule, midrule, paragraph, toprule, url, neurips2024
#import "@preview/timeliney:0.0.1"

#let affls = (
  uw: (
    institution: "University of Wisconsin",
    location: "Madison",
  ),
)

#let authors = (
  (name: "Mondo Jiang", affl: "uw", email: "mondo.jiang@wisc.edu", equal: true),
  (name: "Daniel Smedema", affl: "uw", email: "djsmedema@wisc.edu", equal: true),
)


#show: neurips2024.with(
  title: [DevOops: Speeding Up Debugging from Logs],
  authors: (authors, affls),
  keywords: ("Machine Learning"),
  abstract: none,
  bibliography: bibliography("main.bib"),
  bibliography-opts: (title: none, full: true),  // Only for example paper.
  appendix: [ ],
  accepted: none,
)

// plan:
// - Mondo
// - get bugswarm list of ids for logs - passing and failing
// - download the logs
// - diff each corresponding passing and failing log
//
// - Daniel
// - fine-tuning llama model
// - don't forget to apply a token for bugswarm!
// - confirm prompt for our task
// - base performance test for the finetune one and the basic one.

// alternative:
// - golden labels from habitate

// Requirement:
// The problem you are solving and its importance.
// Related literature and how your proposal is different.
// Details of concrete methods you have tested so far. One should be able to replicate your results based on the details include in the paper.
// Your experiments and results so far. Explain the setups (models, baselines, metrics, etc.)
// What are the planned methods and experiments for the rest of the semester?
// Proposed timeline. How are you doing based on the initial timeline you had? Propose adjustments to the planned timeline for the rest of the semester.a



#set cite(form: "prose")

// Presetation: https://docs.google.com/presentation/d/1j3bSC3jhN_a_Eoe37qeLdfcxwJtxSjugVd_0CeZKNn4/edit#slide=id.p
// Submission: https://canvas.wisc.edu/courses/427899/assignments/2468896

= Introduction
// The problem you are solving and its importance.

In the modern era, software applications are developed using complex "tech stacks" that feature multiple interacting components, and these are often deployed into a production application using another automated process, often called a CI/CD pipeline (Continuous Integration/Continuous Deployment). This pipeline itself is code that may fail, and its failure could be due to problems in the pipeline code or problems one or more of the system's constituent components. The error logs generated by these failures can be difficult to read and reason about.

There is often no way to test whether you have fixed a pipeline error other than running the pipeline again, and this may take minutes or even tens of minutes even for relatively small applications. A human troubleshooter will likely not be able to spend these minutes spent waiting on pipeline runs productively on other tasks. Thus it is costly to be wrong about what the actual error is.

With the advent of large language models, the possibility of automated assistance in interpreting semi-structured and dynamic log streams is on the horizon. It is already possible to copy and paste your logs into ChatGPT and ask it questions, often receiving fairly helpful results. However, there are a few downsides to this: a) it may leak your proprietary information to third parties, and b) ChatGPT does not have the full context of your codebase, which may be necessary to properly contextualize the errors.

A fully-local language model avoids the first issue, and fine-tuning it on a single specific codebase addresses the second. In order for this to be economically viable for a small organization, the model would need to be small and based on a pretrained open-source model. For example, the recently released Llama 3.2 with 1B parameters is advertised as fitting onto edge and mobile devices.

As a first step towards generating natural-language "advice" on how to fix issues or even automatically generating fix code, the model must be able to identify the error from within the log text. This will be our focus in this project.

= Related Work
// and how your proposal is different.

Many approaches have been proposed for various code-related tasks. @Sun2024 provided a comprehensive summary of code intelligence, where most efforts focus on building connections between code-to-code and text-to-code generation, enhancement, and understanding. However, there are no well-defined tasks for handling logs generated during build and deployment pipelines.

@Jimenez2023 defined a benchmark to test the code auto-fixing capabilities of models, while @LeGoues2015 focused on automatically repairing bugs already fixed by patches. Their approaches are limited to predefined, well-constructed bugs, rather than identifying issues directly from information generated during building and running processes.

@Kang2023 focused on explainable automated debugging to improve interpretability. Their hypothesis-testing cycle model is particularly insightful, and we may explore applying it to automate the repair process once the core error log is identified.

@Beller2017 and @Tomassi2019 provided datasets that include pipeline logs, historical data, and fix patches, but they do not offer insights into the logs themselves. In the later stages of our experiments, we plan to use their datasets to enhance our modelâ€™s ability to suggest fixes.

Finally, @10017337 applied NLP methods to log summarization and introduced the first gold-standard dataset for this task. However, their approach is not label-efficient and does not leverage the vast knowledge encoded in pretrained LLMs. We will focus on summarizing core error logs and aim to propose a more label-efficient method for generating ground truth.

In summary, our proposal is different from prior work in that it brings together the power of pre-trained LLMs and label-efficient training techniques on code-related tasks with the domain of build logs, a combination which has not been explored before to our knowledge.

// nice!
// :)

= Method
// (do not have to include every detail.)

== Data
In order to reflect the use case of fine-tuning a language model on a single specific codebase to provide specialized insights, we will select a single open-source project and confine ourselves to only logs generated from it. We have tentatively selected the lodash project, because both authors are familiar with the programming language (JavaScript) and it already has a CI/CD pipeline enabled in GitHub Actions, which can be easily forked and triggered.

We will generate a synthetic dataset by forking the project, then creating multiple isolated branches where we intentionally introduce errors into the code. We might also reproduce bugs that were fixed by previous commits to enhance data diversity. The logs from the resulting pipeline executions will then be a set of unlabeled data.

In order to label the data,  we need to determine which part of the logs contains the error. As will be discussed later, we hope to incorporate label-efficient methods in order to produce fewer labels.

We will also be able to generate a set of data usable for weak supervision by diffing the new logs from the broken code with the old logs from successful pipeline runs. We believe lines that are added in the diff will be significantly more likely to contain relevant errors.

Additionally, we can generate additional (silver) labels for our dataset by feeding the unlabeled examples into existing commercial LLMs such as ChatGPT.

Finally, we may incorporate existing datasets into our process, such as the one compiled in @Beller2017 or @Tomassi2019. One technique we could use is to fine-tune our model in two stages: first, using existing datasets drawn from many projects in order to focus the model towards understanding pipeline logs, and then using our single-repository dataset to specialize the model for that project specifically.

== Experiments
// Your planned experiment setups and/or theoretical arguments.

Our primary experiment will be comparing a fine-tuned model with the base pretrained model. We will use both zero-shot and few-shot methodologies. We will develop a template prompt, such as "In the following logs, quote the part that is the error:" and concatenate the logs. We will save model weight checkpoints in order to compare the performance as we increase the number of fine-tuning iterations.

We may perform additional ablation experiments as time permits and depending on what looks promising.

== Results
// Metric you would use to evaluate your method.

// - line-number approximation coverage // can we do multi output?
// IMO we don't want to mess with this ^
// lol just act as a reference
// I think we have a highly chance to change this after we finish reading...

We will evaluate our models' performance using a BLEU score comparing the models' output with the gold label.

= Timeline
// (detail progress of each week).


#timeliney.timeline(
  show-grid: true,
  {
    import timeliney: *
      
    headerline(group(([*Oct*], 3)), group(([*Nov*], 4)), group(([*Dec*], 2)))
    headerline(
      group(strong("14"), strong("21"), strong("28")),
      group(strong("4"), strong("11"), strong("18"), strong("25")),
      group(strong("2"), strong("9"))
    )
  
    taskgroup(title: [*Research*], {
      task("Select Codebase", (0, 1), style: (stroke: 2pt + gray))
      task("Literature Review", (0, 2), style: (stroke: 2pt + gray))
    })

    taskgroup(title: [*Dataset Development*], {
      task("Develop log generation process", (1, 3), style: (stroke: 2pt + gray))
      task("Generate logs", (2, 3), style: (stroke: 2pt + gray))
      task("Clean & label data", (2, 4), style: (stroke: 2pt + gray))
    })
    taskgroup(title: [*Model Fine-Tuning*], {
      task("Write fine-tuning code", (3, 4), style: (stroke: 2pt + gray))
      task("Fine-tune the model", (3, 5), style: (stroke: 2pt + gray))
    })

    taskgroup(title: [*Running Experiments*], {
      task("Write experiment code", (4, 6), style: (stroke: 2pt + gray))
      task("Execute experiments", (5, 7), style: (stroke: 2pt + gray))
    })

    taskgroup(title: [*Analyzing Results & Writing Final Report*], {
      task("Analyze Results", (6, 8), style: (stroke: 2pt + gray))
      task("Write final report", (7, 8.5), style: (stroke: 2pt + gray))
    })

    milestone(
      at: 8.5,
      style: (stroke: (dash: "dashed")),
      align(center, [
        *Last Class*\
        Dec 11
      ])
    )
  }
)

= References