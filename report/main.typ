#import "./template/neurips.typ": botrule, midrule, paragraph, toprule, url, neurips2024
#import "@preview/timeliney:0.0.1"

#let affls = (
  uw: (
    institution: "University of Wisconsin",
    location: "Madison",
  ),
)

#let authors = (
  (name: "Mondo Jiang", affl: "uw", email: "mondo.jiang@wisc.edu", equal: true),
  (name: "Daniel Smedema", affl: "uw", email: "djsmedema@wisc.edu", equal: true),
)


#show: neurips2024.with(
  title: [
    DevOops: Speeding Up Debugging from Logs\
    #text(size: 0.8em, weight: "light")[Midterm Report]
  ],

  authors: (authors, affls),
  keywords: ("Machine Learning"),
  abstract: [
  // I GPTed this, make changes as much as you want! Worth to note this doesn't include any results from our expr so far :p
    In the modern software development landscape, complex tech stacks and automated CI/CD pipelines present unique challenges in error identification and debugging. This report explores the potential of leveraging large language models to interpret log streams and assist in debugging, addressing the inefficiencies of current manual methods. We propose a fully-local, fine-tuned language model to identify errors within log texts, preserving proprietary information and providing context-specific insights. Our approach differs from existing work by combining the strengths of pre-trained LLMs with label-efficient training techniques tailored for build logs. The project's methodology includes data collection from a selected open-source project, the creation of synthetic datasets, and the generation of data from diffs between failed and passed builds. Our experiments will compare a fine-tuned model against a base pre-trained model using zero-shot and few-shot methodologies. This project aims to enhance the speed and accuracy of debugging from logs, ultimately improving software development efficiency.
  ],
  bibliography: bibliography("main.bib"),
  bibliography-opts: (title: none, full: true),  // Only for example paper.
  appendix: [],
  accepted: none,
)

// plan:
// - Mondo
// - get bugswarm list of ids for logs - passing and failing
// - download the logs
// - diff each corresponding passing and failing log
//
// - Daniel
// - fine-tuning llama model
// - don't forget to apply a token for bugswarm!
// - confirm prompt for our task
// - base performance test for the finetune one and the basic one.

// alternative:
// - golden labels from habitate

// Requirement:
// The problem you are solving and its importance.
// Related literature and how your proposal is different.
// Details of concrete methods you have tested so far. One should be able to replicate your results based on the details include in the paper.
// Your experiments and results so far. Explain the setups (models, baselines, metrics, etc.)
// What are the planned methods and experiments for the rest of the semester?
// Proposed timeline. How are you doing based on the initial timeline you had? Propose adjustments to the planned timeline for the rest of the semester.a



#set cite(form: "prose")

// Presetation: https://docs.google.com/presentation/d/1j3bSC3jhN_a_Eoe37qeLdfcxwJtxSjugVd_0CeZKNn4/edit#slide=id.p
// Submission: https://canvas.wisc.edu/courses/427899/assignments/2468896

= Introduction
// The problem you are solving and its importance.

In the modern era, software applications are developed using complex "tech stacks" that feature multiple interacting components, and these are often deployed into a production application using another automated process, often called a CI/CD pipeline (Continuous Integration/Continuous Deployment). This pipeline itself is code that may fail, and its failure could be due to problems in the pipeline code or problems one or more of the system's constituent components. The error logs generated by these failures can be difficult to read and reason about.

There is often no way to test whether you have fixed a pipeline error other than running the pipeline again, and this may take minutes or even tens of minutes even for relatively small applications. A human troubleshooter will likely not be able to spend these minutes spent waiting on pipeline runs productively on other tasks. Thus it is costly to be wrong about what the actual error is.

With the advent of large language models, the possibility of automated assistance in interpreting semi-structured and dynamic log streams is on the horizon. It is already possible to copy and paste your logs into ChatGPT and ask it questions, often receiving fairly helpful results. However, there are a few downsides to this: a) it may leak your proprietary information to third parties, and b) ChatGPT does not have the full context of your codebase, which may be necessary to properly contextualize the errors.

A fully-local language model avoids the first issue, and fine-tuning it on a single specific codebase addresses the second. In order for this to be economically viable for a small organization, the model would need to be small and based on a pretrained open-source model. For example, the recently released Llama 3.2 with 1B parameters is advertised as fitting onto edge and mobile devices.

As a first step towards generating natural-language "advice" on how to fix issues or even automatically generating fix code, the model must be able to identify the error from within the log text. This will be our focus in this project.

= Related Work
// and how your proposal is different.

Many approaches have been proposed for various code-related tasks. @Sun2024 provided a comprehensive summary of code intelligence, where most efforts focus on building connections between code-to-code and text-to-code generation, enhancement, and understanding. However, there are no well-defined tasks for handling logs generated during build and deployment pipelines.

@Jimenez2023 defined a benchmark to test the code auto-fixing capabilities of models, while @LeGoues2015 focused on automatically repairing bugs already fixed by patches. Their approaches are limited to predefined, well-constructed bugs, rather than identifying issues directly from information generated during building and running processes.

@Kang2023 focused on explainable automated debugging to improve interpretability. Their hypothesis-testing cycle model is particularly insightful, and we may explore applying it to automate the repair process once the core error log is identified.

@Beller2017 and @Tomassi2019 provided datasets that include pipeline logs, historical data, and fix patches, but they do not offer insights into the logs themselves. In the later stages of our experiments, we plan to use their datasets to enhance our modelâ€™s ability to suggest fixes.

Finally, @10017337 applied NLP methods to log summarization and introduced the first gold-standard dataset for this task. However, their approach is not label-efficient and does not leverage the vast knowledge encoded in pretrained LLMs. We will focus on summarizing core error logs and aim to propose a more label-efficient method for generating ground truth.

In summary, our proposal is different from prior work in that it brings together the power of pre-trained LLMs and label-efficient training techniques on code-related tasks with the domain of build logs, a combination which has not been explored before to our knowledge.

// nice!
// :)

= Method and Progress
// Details of concrete methods you have tested so far. One should be able to replicate your results based on the details include in the paper.
// Your experiments and results so far. Explain the setups (models, baselines, metrics, etc.)

== Data Collection
#show "Habitica": [Habitica@link-habitica]

/ Golden Labels: In order to reflect the use case of fine-tuning a language model on a single specific codebase to provide specialized insights, we will select a single open-source project and confine ourselves to only logs generated from it. We have tentatively selected the lodash project, because both authors are familiar with the programming language (JavaScript) and it already has a CI/CD pipeline enabled in GitHub Actions, which can be easily forked and triggered.
  / Progress: We have selected Habitica as the main repository we'll be working on. We are still working on scraping data from its failed GitHub actions. Our current plan is to manually label those logs after adopting some label-efficient methods into our pipeline.
  / Plan: We recently found out that GitHub has released a similar alpha-stage feature called "Explain Errors in Logs," which performs tasks similar to our proposal. However, it focuses more on summarizing the logs and sometimes generates a strong illusion by providing completely wrong answers, as it doesn't give an explicit answer to where the error occurred within the build log. We will still try to compare our results with this feature or use its output as hints in our training process.

/ Synthetic Dataset: We will generate a synthetic dataset by forking the project, then creating multiple isolated branches where we intentionally introduce errors into the code. We might also reproduce bugs that were fixed by previous commits to enhance data diversity. The logs from the resulting pipeline executions will then be a set of unlabeled data.
  / Plan: We have temporarily postponed this step to later stages of our project. Since Habitica already has plenty of failed jobs, the data from synthetic bugs are less practical compared to the bugs we obtain from the real history of builds.


/ Diff between Failed and Passed Builds: We will also be able to generate a set of data usable for weak supervision by diffing the new logs from the broken code with the old logs from successful pipeline runs. We believe lines that are added in the diff will be significantly more likely to contain relevant errors.
  / Progress: We managed to download all reproducible tasks from BugSwarm (Tomassi et al., 2019) and all corresponding raw build logs of passed and failed jobs for each task. We also generated diffs with different context window sizes, which will affect how many non-changed lines above or below a changed line should be included in the patch. We are using 0, 2, 4, and 8 for our current experiments.
  / Plan: We might also adopt another existing database from Beller et al. (2017). We'll continue using these large-scale databases in the first stage of training to focus the model on understanding pipeline logs. After examining the raw logs and diffs, we found that there is a considerable amount of noise in the raw logs, such as progress indicators and temporary/random file name changes. We plan to generate a new set of training data by filtering out this noise using different methods.

== Experiments
// Your planned experiment setups and/or theoretical arguments.

Our primary experiment will be comparing a fine-tuned model with the base pretrained model. We will use both zero-shot and few-shot methodologies. We will develop a template prompt, such as "In the following logs, quote the part that is the error:" and concatenate the logs. We will save model weight checkpoints in order to compare the performance as we increase the number of fine-tuning iterations.

We may perform additional ablation experiments as time permits and depending on what looks promising.

== Results
// Metric you would use to evaluate your method.

// - line-number approximation coverage // can we do multi output?
// IMO we don't want to mess with this ^
// lol just act as a reference
// I think we have a highly chance to change this after we finish reading...

We will evaluate our models' performance using a BLEU score comparing the models' output with the gold label.

= Timeline
// (detail progress of each week).


#timeliney.timeline(
  show-grid: true,
  {
    import timeliney: *
      
    headerline(group(([*Oct*], 3)), group(([*Nov*], 4)), group(([*Dec*], 3)))
    headerline(
      group(strong("14"), strong("21"), strong("28")),
      group(strong("4"), strong("11"), strong("18"), strong("25")),
      group(strong("2"), strong("9"), strong("16"))
    )
  
    taskgroup(title: [*Research*], {
      task([Select Codebase #sym.checkmark], (0, 1), style: (stroke: 2pt + gray))
      task([Literature Review #sym.checkmark], (2, 4), style: (stroke: 2pt + gray))
    })

    taskgroup(title: [*Dataset Development*], {
      task([Diff for existing database #sym.checkmark], (3, 5), style: (stroke: 2pt + gray))
      task([Golden label data collection #sym.circle.dotted], (5, 7), style: (stroke: 2pt + gray))
    })
    taskgroup(title: [*Model Fine-Tuning*], {
      task([Write fine-tuning code #sym.checkmark], (4, 5), style: (stroke: 2pt + gray))
      task([Fine-tune the model #sym.circle.dotted], (5, 6), style: (stroke: 2pt + gray))
    })

    taskgroup(title: [*Running Experiments*], {
      task([Write experiment code #sym.circle.dotted], (5, 7), style: (stroke: 2pt + gray))
      task("Execute experiments", (6, 8), style: (stroke: 2pt + gray))
    })

    taskgroup(title: [*Analyzing Results & Writing Final Report*], {
      task("Analyze Results", (8, 9), style: (stroke: 2pt + gray))
      task("Write final report", (9, 9.5), style: (stroke: 2pt + gray))
    })

    milestone(
      at: 5.5,
      style: (stroke: (dash: "dashed")),
      align(center, [
        *Mid Report Due*\
        Nov 17
      ])
    )
    milestone(
      at: 9.5,
      style: (stroke: (dash: "dashed")),
      align(center, [
        *Final Report Due*\
        Dec 15
      ])
    )
  }
)

= References