#import "./template/neurips.typ": botrule, midrule, paragraph, toprule, url, neurips2024
#import "@preview/timeliney:0.0.1"

#let affls = (
  uw: (
    institution: "University of Wisconsin",
    location: "Madison",
  ),
)

#let authors = (
  (name: "Mondo Jiang", affl: "uw", email: "mondo.jiang@wisc.edu", equal: true),
  (name: "Daniel Smedema", affl: "uw", email: "djsmedema@wisc.edu", equal: true),
)


#show: neurips2024.with(
  title: [
    DevOops: Speeding Up Debugging from Logs\
    #text(size: 0.8em, weight: "light")[Final Report]
  ],

  authors: (authors, affls),
  keywords: ("Machine Learning", "DevOps", "CI/CD", "LLM", "Fine Tune"),
  abstract: [
    In the modern software development landscape, complex tech stacks and automated CI/CD pipelines present unique challenges in error identification and debugging. We explore the potential of leveraging large language models to interpret log streams and assist in debugging, addressing the inefficiencies of current manual methods. We propose a fully-local, fine-tuned language model to identify errors within log texts, preserving proprietary information and providing context-specific insights. We utilize data from the open-source BugSwarm dataset to fine-tune a Llama-3.2 3B model using LoRA. We do not show notable improvement in model performance after fine-tuning. Ultimately, we conclude that alternative approaches will likely be more practical.
  ],
  bibliography: bibliography("main.bib"),
  bibliography-opts: (title: none, full: true),  // Only for example paper.
  appendix: [
    #include "appendix.typ"
  ],
  accepted: none,
)

#set cite(form: "prose")

// The problem you are solving and its importance.
// Related literature and how your proposal is different.
// Formal definition and description of your problem. Details of the methods you used with paper quality descriptions. One should be able to replicate your results based on the details include in the paper.
// Your experiments and results. Include the setup details so that it is replicable by others (models, baselines, metrics, etc.)

= Introduction
// The problem you are solving and its importance.

In the modern era, software applications are developed using complex "tech stacks" that feature multiple interacting components, and these are often deployed into a production application using another automated process, often called a CI/CD pipeline (Continuous Integration/Continuous Deployment). This pipeline itself is code that may fail, and its failure could be due to problems in the pipeline code or problems one or more of the system's constituent components. The error logs generated by these failures can be difficult to read and reason about.

There is often no way to test whether you have fixed a pipeline error other than running the pipeline again, and this may take minutes or even tens of minutes even for relatively small applications. A human troubleshooter will likely not be able to spend these minutes spent waiting on pipeline runs productively on other tasks. Thus it is costly to be wrong about what the actual error is.

With the advent of large language models, the possibility of automated assistance in interpreting semi-structured and dynamic log streams is on the horizon. It is already possible to copy and paste your logs into ChatGPT and ask it questions, often receiving fairly helpful results. However, there are a few downsides to this: a) it may leak your proprietary information to third parties, and b) ChatGPT does not have the full context of your codebase, which may be necessary to properly contextualize the errors.

A fully-local language model avoids the first issue, and fine-tuning it on a single specific codebase addresses the second. In order for this to be economically viable for a small organization, the model would need to be small and based on a pretrained open-source model. For example, the recently released Llama 3.2 with 1B parameters is advertised as fitting onto edge and mobile devices.

As a first step towards generating natural-language "advice" on how to fix issues or even automatically generating fix code, the model must be able to identify the error from within the log text. This will be our focus in this project.

= Related Work
// Related literature and how your proposal is different.

Many approaches have been proposed for various code-related tasks. @Sun2024 provided a comprehensive summary of code intelligence, where most efforts focus on building connections between code-to-code and text-to-code generation, enhancement, and understanding. However, there are no well-defined tasks for handling logs generated during build and deployment pipelines.

@Jimenez2023 defined a benchmark to test the code auto-fixing capabilities of models, while @LeGoues2015 focused on automatically repairing bugs already fixed by patches. Their approaches are limited to predefined, well-constructed bugs, rather than identifying issues directly from information generated during building and running processes.

@Kang2023 focused on explainable automated debugging to improve interpretability. Their hypothesis-testing cycle model is particularly insightful, and we may explore applying it to automate the repair process once the core error log is identified.

@Beller2017 and @Tomassi2019 provided datasets that include pipeline logs, historical data, and fix patches, but they do not offer insights into the logs themselves. In the later stages of our experiments, we plan to use their datasets to enhance our modelâ€™s ability to suggest fixes.

Finally, @10017337 applied NLP methods to log summarization and introduced the first gold-standard dataset for this task. However, their approach is not label-efficient and does not leverage the vast knowledge encoded in pretrained LLMs. We will focus on summarizing core error logs and aim to propose a more label-efficient method for generating ground truth.

In summary, our work is different from prior work in that it brings together the power of pre-trained LLMs and label-efficient training techniques on code-related tasks with the domain of build logs, a combination which has not been explored before to our knowledge.

= Methods
// Formal definition and description of your problem. Details of the methods you used with paper quality descriptions. One should be able to replicate your results based on the details include in the paper.

Our main progress so far has been in preparing the technical environment and collecting data. We have elected to use the unsloth python library in order to fine-tune using a Low-Rank Adaptation (LoRA) method (@hu2021loralowrankadaptationlarge). We are using unsloth's version of the Llama-3.2 3B Instruct model, from huggingface as unsloth/Llama-3.2-3B-Instruct-bnb-4bit.

We have set up a training environment in a docker container based on Ubuntu 24.04 image on Mondo's personal computer, using an NVIDIA GeForce RTX 3070 Ti with 8GB memory. An excerpt of our fine-tuning code is in Appendix A.

What is not shown in the excerpt is the details of the dataset being loaded. This is still in-progress, and in fact we have multiple datasets we are still working on processing into a useful form.

== Data: BugSwarm

In order to test the extent to which generic exposure to DevOps pipeline runs can elicit latent capabilities of the Llama model, we are working on incorporating training using this large and more diverse dataset, BugSwarm (@Tomassi2019). The dataset itself contains raw logs from paired failing and successful runs. We have generated a derived dataset using a diffing program to extract the differences in the log between the successful and failed runs. We believe lines that are added in the diff will be significantly more likely to contain relevant errors.

We managed to download around 4478 reproducible tasks and around 1965 paired raw build logs of passed and failed jobs for the tasks. We also generated diffs with different context window sizes, which will affect how many non-changed lines above or below a changed line are included in the diff output. We are using 0, 2, 4, and 8 as the context window sizes for our current experiments.

We might also adopt another existing database (@Beller2017). We'll continue using these large-scale databases in the first stage of training to focus the model on understanding pipeline logs. After examining the raw logs and diffs, we found that there is a considerable amount of noise in the raw logs, such as progress indicators and temporary/random file name changes. We plan to generate a new set of training data by filtering out this noise using different methods. We will most likely be applying some diversity-based filtering here, following from a number of papers we've read in this class that all emphasize the importance of dataset diversity.

== Data Generation & Distillation

We mainly build our training and testing dataset on top of BugSwarm. The database contains build jobs from various Python and Java open-source projects.  It includes raw logs from both paired failing and successful runs. We have imported 4455 pairs of build logs from BugSwarm, and generate our silver label by distilling the raw logs into diff hunks which only include "changes" from failed logs to passed logs.

#table(
  columns: (auto, auto, auto, auto),
  align: center,
  table.header(
    table.cell(rowspan: 2, []), table.cell(colspan: 2, [*Sourcecode Type*]), table.cell(rowspan: 2, [Total]),
    [Python], [Java], 
    [Avg. Line Count(passed)],[2613],[8258],[5655],
    [Avg. Line Count(passed, distilled)],[893],[4836],[3032 #text(fill: green)[-]],
    [Avg. Line Count(failed)],[2803],[5196],[4084],
    [Avg. Line Count(failed, distilled)],[1107],[3117],[2198],
    [Avg. Char Count(passed)],[202301],[740882],[492095],
    [Avg. Char Count(passed, distilled)],[65735],[450832],[274139],
    [Avg. Char Count(failed)],[215747],[484425],[359556],
    [Avg. Char Count(failed, distilled)],[78639],[287113],[191431],
    [Avg. Hunk Count],[1.76],[1.76],[1.76],
    [Avg. Hunk Char Size],[180],[476],[338],
    [Avg. Hunk Char Size],[246],[629],[452],
  ),
)
)

=== Timestamp Removal

=== ASCII Control Sequence Detection

By inspecting the logs, we found several obvious sources of noise that need to be cleaned:
Timestamps
Progress indicators
Execution runner information
Random paths
By leveraging the ASCII escape characters in the raw log files and using pattern matching, we successfully obtained relatively cleaner data.


=== Label Generation

Our label generation is based on the difference between the failed log and the passed log. 
We treat the 'removed part' from failed to passed logs as the 'root cause' of the failure. 
For each log pair, we generate multiple 'diff hunks' and their corresponding 'contexts.'
We use context window lengths of 20 and 50.






== Fine-tuning

== Experiment: Zero-shot prompting

Our primary experimental methodology was to compare models using a zero-shot prompt, because this better matches the expected final use case of a tool like this (though it is possible a few-shot model could be implemented in a way that is opaque to the user by hiding it within a system prompt). 

Our prompt template is:

```python
prompt_template = """You are an expert DevOps engineer. You are examining some logs. Your first task is to find which lines, if any, contain errors. Examine the following logs.

### Logs:
{}

### Instructions:
Find the lines that contain the errors and repeat them verbatim, and then stop. Do not include the irrelevant lines that do not contain any errors.

### Response:
{}"""
```

= Results
// Your experiments and results. Include the setup details so that it is replicable by others (models, baselines, metrics, etc.)

We used a standard 90/10 train/test split on the log diff samples (as discussed above/*TODO - discuss above*/). We then took the first 512 of the test samples and evaluated the LM's performance on these.

First, a non-negligible fraction of samples caused our LM to crash during inference, both the base model and the fine-tuned one. Some of our samples were simply too large for the memory that was allocated on the device we were using. The fine-tuned model crashed with a slightly more diverse set of errors, but the overall number of crashes was almost identical.

In addition, both models had a tendency to produce results that were evaluated to a BLEU score of 0, indicating no overlap between the generated output and the reference text. After our fine-tuning, the model produced notably more 0 results. Anecdotally, we saw an increase in very short negative responses, like "None yet. Please proceed to examine the logs." and "No errors found." We did not analyze the output systematically to determine any trends among low-scoring responses. However, it is possible that there was a systemic qualitative difference in the kind of response generated by the LMs when it did not match the label that would not be captured by a BLEU score. Therefore we also report the mean BLEU score when excluding scores of 0.

#figure(
  table(
  columns: (auto, auto, auto, auto, auto),
  table.header(
    [], [*Error count* (approx. fraction)], [*Zero count* (approx. fraction)], [*Mean BLEU* excl. errors], [*Mean BLEU* excl. errors and zeros]
  ),
  [*Llama-3b*], [*41* (0.08)], [*242* (0.47)], [*0.076*], [*0.156*],
  [*Fine-tuned*], [*39* (0.08)], [*326* (0.64)], [*0.054*], [*0.174*]
),
caption: [lorem ipsum]
)

The distribution of scores for the base and fine-tuned models 

#figure(
  grid(columns: 2,
  image("base-hist.png"),
  image("fine-hist.png"),
),
caption: [Histogram of BLEU scores, with errors and scores of 0 excluded]
)

= Discussion & Future Work
// Discussion and conclusion of your results and proposal of future work.


= References