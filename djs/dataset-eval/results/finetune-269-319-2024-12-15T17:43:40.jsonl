{"bleu": 0.0, "precisions": [0.2, 0.0, 0.0, 0.0], "brevity_penalty": 0.029207395438494873, "length_ratio": 0.22058823529411764, "translation_length": 15, "reference_length": 68}
{"bleu": 0.036300454924701187, "precisions": [0.07794871794871795, 0.039014373716632446, 0.02774922918807811, 0.0205761316872428], "brevity_penalty": 1.0, "length_ratio": 7.677165354330708, "translation_length": 975, "reference_length": 127}
{"bleu": 0.0, "precisions": [0.0017064846416382253, 0.0, 0.0, 0.0], "brevity_penalty": 0.6112062477288437, "length_ratio": 0.6700971983990852, "translation_length": 1172, "reference_length": 1749}
{"bleu": 0.26988673956521764, "precisions": [0.6675824175824175, 0.4718019257221458, 0.43388429752066116, 0.40551724137931033], "brevity_penalty": 0.5562494348298312, "length_ratio": 0.6303030303030303, "translation_length": 728, "reference_length": 1155}
{"bleu": 0.014542319471982283, "precisions": [0.0296220633299285, 0.016359918200409, 0.011258955987717503, 0.00819672131147541], "brevity_penalty": 1.0, "length_ratio": 15.061538461538461, "translation_length": 979, "reference_length": 65}
{"bleu": 0.0, "precisions": [0.07692307692307693, 0.0, 0.0, 0.0], "brevity_penalty": 0.5404329964865341, "length_ratio": 0.6190476190476191, "translation_length": 13, "reference_length": 21}
{"bleu": 0.09917991757461825, "precisions": [0.18544366899302095, 0.09081836327345309, 0.08091908091908091, 0.071], "brevity_penalty": 1.0, "length_ratio": 2.3937947494033414, "translation_length": 1003, "reference_length": 419}
{"bleu": 0.023408325247067825, "precisions": [0.21235521235521235, 0.015503875968992248, 0.011673151750972763, 0.0078125], "brevity_penalty": 1.0, "length_ratio": 1.85, "translation_length": 259, "reference_length": 140}
{"bleu": 0.0, "precisions": [0.12, 0.0, 0.0, 0.0], "brevity_penalty": 0.4867522559599717, "length_ratio": 0.5813953488372093, "translation_length": 25, "reference_length": 43}
{"error": "CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 578.69 MiB is free. Including non-PyTorch memory, this process has 10.18 GiB memory in use. Of the allocated memory 9.91 GiB is allocated by PyTorch, and 62.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"error": "CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 138.69 MiB is free. Including non-PyTorch memory, this process has 10.61 GiB memory in use. Of the allocated memory 10.35 GiB is allocated by PyTorch, and 53.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"bleu": 0.0, "precisions": [0.027283511269276393, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 3.0654545454545454, "translation_length": 843, "reference_length": 275}
{"bleu": 0.0, "precisions": [0.04, 0.0, 0.0, 0.0], "brevity_penalty": 0.039163895098987066, "length_ratio": 0.2358490566037736, "translation_length": 25, "reference_length": 106}
{"bleu": 0.22240022547047036, "precisions": [0.31654676258992803, 0.2318840579710145, 0.19708029197080293, 0.16911764705882354], "brevity_penalty": 1.0, "length_ratio": 2.106060606060606, "translation_length": 139, "reference_length": 66}
{"error": "Unsloth: input length 241975 + max_new_tokens 1024 exceeds the maximum sequence length of 131072!\nYou will need to do long context extension by increasing the `max_seq_length` in `FastLanguageModel.from_pretrained`."}
{"bleu": 0.5421923934334224, "precisions": [0.6540880503144654, 0.5253164556962026, 0.5095541401273885, 0.4935897435897436], "brevity_penalty": 1.0, "length_ratio": 1.2045454545454546, "translation_length": 159, "reference_length": 132}
{"error": "CUDA out of memory. Tried to allocate 304.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 120.69 MiB is free. Including non-PyTorch memory, this process has 10.62 GiB memory in use. Of the allocated memory 10.19 GiB is allocated by PyTorch, and 236.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"bleu": 0.0, "precisions": [0.03597883597883598, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 9.264705882352942, "translation_length": 945, "reference_length": 102}
{"bleu": 0.0, "precisions": [0.0, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 3.2857142857142856, "translation_length": 23, "reference_length": 7}
{"bleu": 0.0, "precisions": [0.11764705882352941, 0.0, 0.0, 0.0], "brevity_penalty": 6.571890466284153e-07, "length_ratio": 0.06563706563706563, "translation_length": 17, "reference_length": 259}
{"bleu": 0.0, "precisions": [0.34615384615384615, 0.0, 0.0, 0.0], "brevity_penalty": 4.1115786849912614e-47, "length_ratio": 0.00927577595433464, "translation_length": 26, "reference_length": 2803}
{"bleu": 0.0304249951608961, "precisions": [0.08655126498002663, 0.02666666666666667, 0.021361815754339118, 0.017379679144385027], "brevity_penalty": 1.0, "length_ratio": 3.3083700440528636, "translation_length": 751, "reference_length": 227}
{"bleu": 0.0, "precisions": [0.09523809523809523, 0.0, 0.0, 0.0], "brevity_penalty": 0.8668778997501817, "length_ratio": 0.875, "translation_length": 21, "reference_length": 24}
{"bleu": 0.0, "precisions": [0.13333333333333333, 0.0, 0.0, 0.0], "brevity_penalty": 0.1266071027890836, "length_ratio": 0.32608695652173914, "translation_length": 15, "reference_length": 46}
{"bleu": 0.0, "precisions": [0.2, 0.0, 0.0, 0.0], "brevity_penalty": 5.7346784092083264e-18, "length_ratio": 0.02457002457002457, "translation_length": 10, "reference_length": 407}
{"bleu": 0.17294551898767674, "precisions": [0.3592233009708738, 0.1587473002159827, 0.14702702702702702, 0.13528138528138528], "brevity_penalty": 0.9423946191458162, "length_ratio": 0.9439918533604889, "translation_length": 927, "reference_length": 982}
{"bleu": 0.0, "precisions": [0.27906976744186046, 0.0, 0.0, 0.0], "brevity_penalty": 0.7390973893525953, "length_ratio": 0.7678571428571429, "translation_length": 43, "reference_length": 56}
{"bleu": 0.1707197862545001, "precisions": [0.19906542056074766, 0.17586529466791395, 0.16385767790262173, 0.14807872539831302], "brevity_penalty": 1.0, "length_ratio": 4.421487603305785, "translation_length": 1070, "reference_length": 242}
{"bleu": 0.007251944954967816, "precisions": [0.011363636363636364, 0.007585335018963337, 0.006329113924050633, 0.005069708491761723], "brevity_penalty": 1.0, "length_ratio": 29.333333333333332, "translation_length": 792, "reference_length": 27}
{"bleu": 0.0, "precisions": [0.15384615384615385, 0.0, 0.0, 0.0], "brevity_penalty": 0.005777142164111217, "length_ratio": 0.1625, "translation_length": 13, "reference_length": 80}
{"bleu": 0.0, "precisions": [0.07692307692307693, 0.0, 0.0, 0.0], "brevity_penalty": 0.09949058049485841, "length_ratio": 0.3023255813953488, "translation_length": 13, "reference_length": 43}
{"bleu": 0.0, "precisions": [0.11764705882352941, 0.0, 0.0, 0.0], "brevity_penalty": 0.5235834657149969, "length_ratio": 0.6071428571428571, "translation_length": 17, "reference_length": 28}
{"bleu": 0.0, "precisions": [0.07692307692307693, 0.0, 0.0, 0.0], "brevity_penalty": 0.1988141887380742, "length_ratio": 0.38235294117647056, "translation_length": 13, "reference_length": 34}
{"bleu": 6.941604834797188e-12, "precisions": [0.3856837606837607, 0.014973262032085561, 0.0053533190578158455, 0.0010718113612004287], "brevity_penalty": 5.145025971171134e-10, "length_ratio": 0.044667143879742306, "translation_length": 936, "reference_length": 20955}
{"bleu": 0.0, "precisions": [0.15384615384615385, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 1.0655737704918034, "translation_length": 65, "reference_length": 61}
{"bleu": 0.0, "precisions": [0.05555555555555555, 0.0, 0.0, 0.0], "brevity_penalty": 0.846481724890614, "length_ratio": 0.8571428571428571, "translation_length": 18, "reference_length": 21}
{"bleu": 0.0, "precisions": [0.11363636363636363, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 1.5172413793103448, "translation_length": 44, "reference_length": 29}
{"bleu": 0.050230736506209794, "precisions": [0.05926544240400668, 0.05179615705931495, 0.047658862876254184, 0.04351464435146443], "brevity_penalty": 1.0, "length_ratio": 14.790123456790123, "translation_length": 1198, "reference_length": 81}
{"bleu": 0.0, "precisions": [0.4993894993894994, 0.0, 0.0, 0.0], "brevity_penalty": 0.0002555012458305531, "length_ratio": 0.1078483012904925, "translation_length": 819, "reference_length": 7594}
{"bleu": 0.09278143271417989, "precisions": [0.19729206963249515, 0.09390125847047434, 0.07364341085271318, 0.05431619786614937], "brevity_penalty": 1.0, "length_ratio": 1.3842034805890227, "translation_length": 1034, "reference_length": 747}
{"bleu": 0.0, "precisions": [0.13333333333333333, 0.0, 0.0, 0.0], "brevity_penalty": 0.28176928909495835, "length_ratio": 0.4411764705882353, "translation_length": 15, "reference_length": 34}
{"error": "CUDA out of memory. Tried to allocate 236.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 238.69 MiB is free. Including non-PyTorch memory, this process has 10.51 GiB memory in use. Of the allocated memory 10.11 GiB is allocated by PyTorch, and 196.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"bleu": 0.0, "precisions": [0.35714285714285715, 0.0, 0.0, 0.0], "brevity_penalty": 6.599138626593368e-06, "length_ratio": 0.07734806629834254, "translation_length": 14, "reference_length": 181}
{"bleu": 0.2722716491946002, "precisions": [0.555439330543933, 0.38848167539267014, 0.36163522012578614, 0.3410283315844701], "brevity_penalty": 0.6741170864186505, "length_ratio": 0.7171792948237059, "translation_length": 956, "reference_length": 1333}
{"bleu": 0.0, "precisions": [0.07692307692307693, 0.0, 0.0, 0.0], "brevity_penalty": 0.08530361363583897, "length_ratio": 0.28888888888888886, "translation_length": 13, "reference_length": 45}
{"bleu": 0.036998067665729356, "precisions": [0.9882629107981221, 0.9776733254994124, 0.9670588235294117, 0.9564193168433451], "brevity_penalty": 0.038052848223348396, "length_ratio": 0.234259004674182, "translation_length": 852, "reference_length": 3637}
{"bleu": 0.0, "precisions": [0.2797270955165692, 0.018536585365853658, 0.0, 0.0], "brevity_penalty": 0.7837503681165506, "length_ratio": 0.8040752351097179, "translation_length": 1026, "reference_length": 1276}
{"bleu": 0.09753682726684024, "precisions": [0.10093457943925234, 0.09822263797942002, 0.09644194756554307, 0.09465791940018745], "brevity_penalty": 1.0, "length_ratio": 9.385964912280702, "translation_length": 1070, "reference_length": 114}
{"bleu": 0.0, "precisions": [0.05, 0.0, 0.0, 0.0], "brevity_penalty": 0.951229424500714, "length_ratio": 0.9523809523809523, "translation_length": 20, "reference_length": 21}
{"error": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"}
