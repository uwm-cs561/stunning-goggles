{"bleu": 0.0, "precisions": [0.11764705882352941, 0.0, 0.0, 0.0], "brevity_penalty": 2.8358372778421286e-05, "length_ratio": 0.08717948717948718, "translation_length": 17, "reference_length": 195}
{"bleu": 0.0, "precisions": [0.0076726342710997444, 0.0012804097311139564, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 35.54545454545455, "translation_length": 782, "reference_length": 22}
{"bleu": 0.020582678006071685, "precisions": [0.02208835341365462, 0.021105527638190954, 0.02012072434607646, 0.019133937562940583], "brevity_penalty": 1.0, "length_ratio": 34.3448275862069, "translation_length": 996, "reference_length": 29}
{"bleu": 0.00630842796415088, "precisions": [0.974477958236659, 0.9558652729384437, 0.9383720930232559, 0.9208381839348079], "brevity_penalty": 0.006660232213976452, "length_ratio": 0.16634504052489385, "translation_length": 862, "reference_length": 5182}
{"bleu": 0.19732468154588123, "precisions": [0.22580645161290322, 0.19705882352941176, 0.1887905604719764, 0.1804733727810651], "brevity_penalty": 1.0, "length_ratio": 3.3106796116504853, "translation_length": 341, "reference_length": 103}
{"bleu": 0.18016677236791984, "precisions": [0.20202020202020202, 0.17766497461928935, 0.17346938775510204, 0.16923076923076924], "brevity_penalty": 1.0, "length_ratio": 4.604651162790698, "translation_length": 198, "reference_length": 43}
{"bleu": 0.00740477586327462, "precisions": [0.8177777777777778, 0.6919642857142857, 0.6771300448430493, 0.6666666666666666], "brevity_penalty": 0.010415664079745424, "length_ratio": 0.17971246006389777, "translation_length": 225, "reference_length": 1252}
{"bleu": 0.0, "precisions": [0.010438413361169102, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 3.0031347962382444, "translation_length": 958, "reference_length": 319}
{"bleu": 0.01588531600414573, "precisions": [0.018005540166204988, 0.016643550624133148, 0.015277777777777777, 0.013908205841446454], "brevity_penalty": 1.0, "length_ratio": 34.38095238095238, "translation_length": 722, "reference_length": 21}
{"bleu": 0.0, "precisions": [0.054110301768990635, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 4.511737089201878, "translation_length": 961, "reference_length": 213}
{"bleu": 1.726368111074697e-15, "precisions": [0.46938775510204084, 0.2268041237113402, 0.21875, 0.21052631578947367], "brevity_penalty": 6.524152689675035e-15, "length_ratio": 0.029705971506517128, "translation_length": 98, "reference_length": 3299}
{"bleu": 0.03919040531801734, "precisions": [0.043933054393305436, 0.0387434554973822, 0.03773584905660377, 0.03672612801678909], "brevity_penalty": 1.0, "length_ratio": 21.244444444444444, "translation_length": 956, "reference_length": 45}
{"error": "CUDA out of memory. Tried to allocate 352.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 276.69 MiB is free. Including non-PyTorch memory, this process has 10.47 GiB memory in use. Of the allocated memory 9.94 GiB is allocated by PyTorch, and 328.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"bleu": 0.0, "precisions": [0.009211873080859774, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 14.367647058823529, "translation_length": 977, "reference_length": 68}
{"bleu": 0.0, "precisions": [0.06728538283062645, 0.004645760743321719, 0.002325581395348837, 0.0], "brevity_penalty": 1.0, "length_ratio": 6.78740157480315, "translation_length": 862, "reference_length": 127}
{"bleu": 0.05712606962192785, "precisions": [0.33486660533578655, 0.10957642725598526, 0.07649769585253456, 0.043357933579335796], "brevity_penalty": 0.5438859856065016, "length_ratio": 0.6214979988564894, "translation_length": 1087, "reference_length": 1749}
{"bleu": 0.2520780026803547, "precisions": [0.5885111371629543, 0.31338028169014087, 0.30434782608695654, 0.2964705882352941], "brevity_penalty": 0.7018437004683081, "length_ratio": 0.7385281385281385, "translation_length": 853, "reference_length": 1155}
{"bleu": 0.0, "precisions": [0.08737864077669903, 0.01951219512195122, 0.00980392156862745, 0.0], "brevity_penalty": 1.0, "length_ratio": 3.169230769230769, "translation_length": 206, "reference_length": 65}
{"bleu": 0.0, "precisions": [0.005213764337851929, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 45.666666666666664, "translation_length": 959, "reference_length": 21}
{"bleu": 0.4618727573071787, "precisions": [0.48866498740554154, 0.45649432534678436, 0.4532828282828283, 0.450063211125158], "brevity_penalty": 1.0, "length_ratio": 1.8949880668257757, "translation_length": 794, "reference_length": 419}
{"bleu": 0.0, "precisions": [0.18932038834951456, 0.004878048780487805, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 1.4714285714285715, "translation_length": 206, "reference_length": 140}
{"bleu": 0.12242168195603138, "precisions": [0.13240418118466898, 0.12237762237762238, 0.11929824561403508, 0.11619718309859155], "brevity_penalty": 1.0, "length_ratio": 6.674418604651163, "translation_length": 287, "reference_length": 43}
{"error": "CUDA out of memory. Tried to allocate 1.50 GiB. GPU 0 has a total capacity of 10.75 GiB of which 1.35 GiB is free. Including non-PyTorch memory, this process has 9.40 GiB memory in use. Of the allocated memory 8.87 GiB is allocated by PyTorch, and 327.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"bleu": 3.420868879266291e-07, "precisions": [0.358974358974359, 0.14705882352941177, 0.14022140221402213, 0.13703703703703704], "brevity_penalty": 1.916835009119318e-06, "length_ratio": 0.07059736229635376, "translation_length": 273, "reference_length": 3867}
{"bleu": 0.26824139256020607, "precisions": [0.2693467336683417, 0.2686116700201207, 0.2678751258811682, 0.26713709677419356], "brevity_penalty": 1.0, "length_ratio": 3.618181818181818, "translation_length": 995, "reference_length": 275}
{"bleu": 0.0, "precisions": [0.08169934640522876, 0.02618657937806874, 0.013114754098360656, 0.0], "brevity_penalty": 1.0, "length_ratio": 5.773584905660377, "translation_length": 612, "reference_length": 106}
{"bleu": 0.04181160429749226, "precisions": [0.051142546245919476, 0.04357298474945534, 0.03925845147219193, 0.034934497816593885], "brevity_penalty": 1.0, "length_ratio": 13.924242424242424, "translation_length": 919, "reference_length": 66}
{"error": "Unsloth: input length 241975 + max_new_tokens 1024 exceeds the maximum sequence length of 131072!\nYou will need to do long context extension by increasing the `max_seq_length` in `FastLanguageModel.from_pretrained`."}
{"bleu": 0.7611322544186286, "precisions": [0.8211382113821138, 0.819672131147541, 0.8181818181818182, 0.8166666666666667], "brevity_penalty": 0.9294421312368021, "length_ratio": 0.9318181818181818, "translation_length": 123, "reference_length": 132}
{"error": "CUDA out of memory. Tried to allocate 304.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 280.69 MiB is free. Including non-PyTorch memory, this process has 10.47 GiB memory in use. Of the allocated memory 9.91 GiB is allocated by PyTorch, and 362.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"bleu": 0.11146002267119895, "precisions": [0.1166077738515901, 0.11084905660377359, 0.10979929161747344, 0.10874704491725769], "brevity_penalty": 1.0, "length_ratio": 8.323529411764707, "translation_length": 849, "reference_length": 102}
{"bleu": 0.0, "precisions": [0.0, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 120.42857142857143, "translation_length": 843, "reference_length": 7}
{"bleu": 0.40925320786124664, "precisions": [0.4155844155844156, 0.408130081300813, 0.40716612377850164, 0.4061990212071778], "brevity_penalty": 1.0, "length_ratio": 2.3783783783783785, "translation_length": 616, "reference_length": 259}
{"bleu": 1.9299729942918552e-16, "precisions": [0.5128205128205128, 0.24675324675324675, 0.23684210526315788, 0.22666666666666666], "brevity_penalty": 6.722526505837248e-16, "length_ratio": 0.027827327863003924, "translation_length": 78, "reference_length": 2803}
{"bleu": 0.0, "precisions": [0.04484304932735426, 0.0, 0.0, 0.0], "brevity_penalty": 0.9822226946277374, "length_ratio": 0.9823788546255506, "translation_length": 223, "reference_length": 227}
{"bleu": 0.07313602010724818, "precisions": [0.08362369337979095, 0.07692307692307693, 0.07017543859649122, 0.06338028169014084], "brevity_penalty": 1.0, "length_ratio": 11.958333333333334, "translation_length": 287, "reference_length": 24}
{"bleu": 0.15223172555672337, "precisions": [0.15853658536585366, 0.15510204081632653, 0.15163934426229508, 0.1440329218106996], "brevity_penalty": 1.0, "length_ratio": 5.3478260869565215, "translation_length": 246, "reference_length": 46}
{"bleu": 0.6897741609138042, "precisions": [0.7121212121212122, 0.6888045540796964, 0.6825095057034221, 0.6761904761904762], "brevity_penalty": 1.0, "length_ratio": 1.2972972972972974, "translation_length": 528, "reference_length": 407}
{"bleu": 0.0, "precisions": [0.12033462033462033, 0.000643915003219575, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 1.5824847250509164, "translation_length": 1554, "reference_length": 982}
{"bleu": 0.10334489339546334, "precisions": [0.10629067245119306, 0.10434782608695652, 0.10239651416122005, 0.10043668122270742], "brevity_penalty": 1.0, "length_ratio": 8.232142857142858, "translation_length": 461, "reference_length": 56}
{"bleu": 0.20972746830421526, "precisions": [0.21629213483146068, 0.21180880974695407, 0.20919324577861162, 0.20187793427230047], "brevity_penalty": 1.0, "length_ratio": 4.413223140495868, "translation_length": 1068, "reference_length": 242}
{"bleu": 0.04853147260934299, "precisions": [0.05235602094240838, 0.049868766404199474, 0.04736842105263158, 0.044854881266490766], "brevity_penalty": 1.0, "length_ratio": 14.148148148148149, "translation_length": 382, "reference_length": 27}
{"bleu": 0.09698186288984552, "precisions": [0.19230769230769232, 0.09032258064516129, 0.07792207792207792, 0.06535947712418301], "brevity_penalty": 1.0, "length_ratio": 1.95, "translation_length": 156, "reference_length": 80}
{"bleu": 0.0, "precisions": [0.05660377358490566, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 7.395348837209302, "translation_length": 318, "reference_length": 43}
{"bleu": 0.0, "precisions": [0.008159564823209429, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 39.392857142857146, "translation_length": 1103, "reference_length": 28}
{"bleu": 0.027451317017456605, "precisions": [0.029850746268656716, 0.02774813233724653, 0.026709401709401708, 0.025668449197860963], "brevity_penalty": 1.0, "length_ratio": 27.58823529411765, "translation_length": 938, "reference_length": 34}
{"bleu": 5.610671272851586e-50, "precisions": [0.5645161290322581, 0.17297297297297298, 0.10326086956521739, 0.09289617486338798], "brevity_penalty": 3.2071389091352195e-49, "length_ratio": 0.008876163206871868, "translation_length": 186, "reference_length": 20955}
{"bleu": 0.0, "precisions": [0.06878306878306878, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 6.19672131147541, "translation_length": 378, "reference_length": 61}
{"bleu": 0.013831259030551888, "precisions": [0.015681544028950542, 0.014492753623188406, 0.013301088270858524, 0.012106537530266344], "brevity_penalty": 1.0, "length_ratio": 39.476190476190474, "translation_length": 829, "reference_length": 21}
{"bleu": 0.02011741788585334, "precisions": [0.021589793915603533, 0.0206286836935167, 0.01966568338249754, 0.018700787401574805], "brevity_penalty": 1.0, "length_ratio": 35.13793103448276, "translation_length": 1019, "reference_length": 29}
{"bleu": 0.24441432692460408, "precisions": [0.2730496453900709, 0.24555160142348753, 0.2357142857142857, 0.22580645161290322], "brevity_penalty": 1.0, "length_ratio": 3.4814814814814814, "translation_length": 282, "reference_length": 81}
{"bleu": 3.0806231270978173e-07, "precisions": [0.45849056603773586, 0.1720226843100189, 0.13825757575757575, 0.11764705882352941], "brevity_penalty": 1.6277662418307366e-06, "length_ratio": 0.06979194100605741, "translation_length": 530, "reference_length": 7594}
{"bleu": 0.0, "precisions": [0.06326530612244897, 0.0040858018386108275, 0.0010224948875255625, 0.0], "brevity_penalty": 1.0, "length_ratio": 1.3119143239625168, "translation_length": 980, "reference_length": 747}
{"bleu": 0.08865812010723298, "precisions": [0.1188118811881188, 0.09950248756218906, 0.08, 0.06532663316582915], "brevity_penalty": 1.0, "length_ratio": 5.9411764705882355, "translation_length": 202, "reference_length": 34}
{"error": "CUDA out of memory. Tried to allocate 236.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 218.69 MiB is free. Including non-PyTorch memory, this process has 10.53 GiB memory in use. Of the allocated memory 9.90 GiB is allocated by PyTorch, and 432.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"bleu": 0.1864102045754634, "precisions": [0.20308788598574823, 0.19143876337693222, 0.18095238095238095, 0.17163289630512515], "brevity_penalty": 1.0, "length_ratio": 4.651933701657459, "translation_length": 842, "reference_length": 181}
{"bleu": 0.0011650613220757298, "precisions": [0.9142857142857143, 0.867816091954023, 0.8554913294797688, 0.8488372093023255], "brevity_penalty": 0.0013372461960775104, "length_ratio": 0.1312828207051763, "translation_length": 175, "reference_length": 1333}
{"bleu": 0.0, "precisions": [0.02753441802252816, 0.007518796992481203, 0.0037641154328732747, 0.0], "brevity_penalty": 1.0, "length_ratio": 17.755555555555556, "translation_length": 799, "reference_length": 45}
{"bleu": 0.03641472153047181, "precisions": [0.9835294117647059, 0.9717314487632509, 0.9610849056603774, 0.9504132231404959], "brevity_penalty": 0.03767255139512742, "length_ratio": 0.2337091009073412, "translation_length": 850, "reference_length": 3637}
{"bleu": 0.002995257088738101, "precisions": [0.71, 0.6432160804020101, 0.6313131313131313, 0.6192893401015228], "brevity_penalty": 0.004607821929992752, "length_ratio": 0.15673981191222572, "translation_length": 200, "reference_length": 1276}
{"bleu": 0.0643678974166947, "precisions": [0.078125, 0.06689834926151172, 0.06086956521739131, 0.053959965187119235], "brevity_penalty": 1.0, "length_ratio": 10.105263157894736, "translation_length": 1152, "reference_length": 114}
{"bleu": 0.1404470376023391, "precisions": [0.1566265060240964, 0.14634146341463414, 0.13580246913580246, 0.125], "brevity_penalty": 1.0, "length_ratio": 3.9523809523809526, "translation_length": 83, "reference_length": 21}
{"error": "CUDA out of memory. Tried to allocate 296.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 276.69 MiB is free. Including non-PyTorch memory, this process has 10.47 GiB memory in use. Of the allocated memory 9.84 GiB is allocated by PyTorch, and 437.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"bleu": 0.027517113090027596, "precisions": [0.041312272174969626, 0.029197080291970802, 0.024360535931790498, 0.01951219512195122], "brevity_penalty": 1.0, "length_ratio": 13.063492063492063, "translation_length": 823, "reference_length": 63}
{"bleu": 0.02766366275340203, "precisions": [0.0308839190628328, 0.02771855010660981, 0.026680896478121666, 0.02564102564102564], "brevity_penalty": 1.0, "length_ratio": 27.61764705882353, "translation_length": 939, "reference_length": 34}
{"bleu": 3.2848225084192735e-11, "precisions": [0.8210526315789474, 0.7407407407407407, 0.7340425531914894, 0.7272727272727273], "brevity_penalty": 4.3515899588147174e-11, "length_ratio": 0.04022866821935211, "translation_length": 190, "reference_length": 4723}
{"bleu": 0.0, "precisions": [0.12903225806451613, 0.0, 0.0, 0.0], "brevity_penalty": 0.6790253796134038, "length_ratio": 0.7209302325581395, "translation_length": 31, "reference_length": 43}
{"error": "CUDA out of memory. Tried to allocate 248.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 218.69 MiB is free. Including non-PyTorch memory, this process has 10.53 GiB memory in use. Of the allocated memory 9.97 GiB is allocated by PyTorch, and 356.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"bleu": 0.018507065315655404, "precisions": [0.020202020202020204, 0.019101123595505618, 0.01799775028121485, 0.016891891891891893], "brevity_penalty": 1.0, "length_ratio": 35.64, "translation_length": 891, "reference_length": 25}
{"bleu": 3.5834201018873244e-07, "precisions": [0.6267995570321152, 0.328159645232816, 0.23640399556048836, 0.1711111111111111], "brevity_penalty": 1.1864815959204358e-06, "length_ratio": 0.06828493647912885, "translation_length": 903, "reference_length": 13224}
{"bleu": 0.002458543412293202, "precisions": [0.6666666666666666, 0.4682713347921225, 0.4194961664841183, 0.4144736842105263], "brevity_penalty": 0.005093544013985104, "length_ratio": 0.15924121127741037, "translation_length": 915, "reference_length": 5746}
{"bleu": 0.32350609753115434, "precisions": [0.33962264150943394, 0.3291139240506329, 0.3184713375796178, 0.3076923076923077], "brevity_penalty": 1.0, "length_ratio": 2.6065573770491803, "translation_length": 159, "reference_length": 61}
{"bleu": 0.4201326124031386, "precisions": [0.9035532994923858, 0.8979591836734694, 0.8923076923076924, 0.8865979381443299], "brevity_penalty": 0.4693787920014868, "length_ratio": 0.569364161849711, "translation_length": 197, "reference_length": 346}
{"bleu": 0.0, "precisions": [0.015151515151515152, 0.0011668611435239206, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 29.586206896551722, "translation_length": 858, "reference_length": 29}
{"bleu": 0.0, "precisions": [0.05319148936170213, 0.0, 0.0, 0.0], "brevity_penalty": 0.7345401379008187, "length_ratio": 0.7642276422764228, "translation_length": 94, "reference_length": 123}
{"error": "CUDA out of memory. Tried to allocate 304.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 80.69 MiB is free. Including non-PyTorch memory, this process has 10.66 GiB memory in use. Of the allocated memory 9.99 GiB is allocated by PyTorch, and 480.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"error": "CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 40.69 MiB is free. Including non-PyTorch memory, this process has 10.70 GiB memory in use. Of the allocated memory 10.05 GiB is allocated by PyTorch, and 460.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"bleu": 9.007877532672936e-07, "precisions": [0.7530120481927711, 0.6072507552870091, 0.5666666666666667, 0.5258358662613982], "brevity_penalty": 1.482639300890043e-06, "length_ratio": 0.06934001670843776, "translation_length": 332, "reference_length": 4788}
{"bleu": 1.0026509485122883e-07, "precisions": [0.424, 0.24497991967871485, 0.2217741935483871, 0.2145748987854251], "brevity_penalty": 3.781405065289377e-07, "length_ratio": 0.06333924499619964, "translation_length": 250, "reference_length": 3947}
{"bleu": 5.670513073270871e-07, "precisions": [0.8637873754152824, 0.7633333333333333, 0.7491638795986622, 0.7449664429530202], "brevity_penalty": 7.280541821794729e-07, "length_ratio": 0.06608122941822174, "translation_length": 301, "reference_length": 4555}
{"bleu": 0.4985616492902614, "precisions": [0.5027932960893855, 0.5, 0.4971751412429379, 0.4943181818181818], "brevity_penalty": 1.0, "length_ratio": 1.8453608247422681, "translation_length": 179, "reference_length": 97}
{"bleu": 0.06951071702697166, "precisions": [0.277720207253886, 0.07053941908713693, 0.04776739356178609, 0.02494802494802495], "brevity_penalty": 1.0, "length_ratio": 1.5976821192052981, "translation_length": 965, "reference_length": 604}
{"bleu": 0.0, "precisions": [0.08123791102514506, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 5.068627450980392, "translation_length": 517, "reference_length": 102}
{"bleu": 0.57102436861364, "precisions": [0.574468085106383, 0.5721925133689839, 0.5698924731182796, 0.5675675675675675], "brevity_penalty": 1.0, "length_ratio": 1.6347826086956523, "translation_length": 188, "reference_length": 115}
{"bleu": 0.0, "precisions": [0.25225225225225223, 0.03636363636363636, 0.01834862385321101, 0.0], "brevity_penalty": 0.7495454759063911, "length_ratio": 0.7762237762237763, "translation_length": 111, "reference_length": 143}
{"bleu": 0.006667082992312013, "precisions": [0.41711711711711713, 0.2651036970243463, 0.2328519855595668, 0.21138211382113822], "brevity_penalty": 0.024545979469724692, "length_ratio": 0.21244019138755982, "translation_length": 1110, "reference_length": 5225}
{"bleu": 3.863804283221494e-06, "precisions": [0.9452554744525548, 0.9267399267399268, 0.9227941176470589, 0.922509225092251], "brevity_penalty": 4.157856217279912e-06, "length_ratio": 0.07467974925047698, "translation_length": 274, "reference_length": 3669}
{"bleu": 6.155211988555288e-14, "precisions": [0.2490118577075099, 0.027777777777777776, 0.01195219123505976, 0.008], "brevity_penalty": 2.158386683588973e-12, "length_ratio": 0.03589161583203291, "translation_length": 253, "reference_length": 7049}
{"bleu": 0.00017898756801653805, "precisions": [0.441747572815534, 0.24878048780487805, 0.19607843137254902, 0.15270935960591134], "brevity_penalty": 0.0007473110009456005, "length_ratio": 0.12196566015393724, "translation_length": 206, "reference_length": 1689}
{"bleu": 0.7308462286283286, "precisions": [0.88671875, 0.8470588235294118, 0.8267716535433071, 0.8063241106719368], "brevity_penalty": 0.8688150562628432, "length_ratio": 0.8767123287671232, "translation_length": 256, "reference_length": 292}
{"bleu": 0.14132531712717103, "precisions": [0.23985239852398524, 0.12199630314232902, 0.11851851851851852, 0.1150278293135436], "brevity_penalty": 1.0, "length_ratio": 1.0168855534709194, "translation_length": 542, "reference_length": 533}
{"error": "CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 258.69 MiB is free. Including non-PyTorch memory, this process has 10.49 GiB memory in use. Of the allocated memory 9.81 GiB is allocated by PyTorch, and 480.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"error": "CUDA out of memory. Tried to allocate 34.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 32.69 MiB is free. Including non-PyTorch memory, this process has 10.71 GiB memory in use. Of the allocated memory 10.06 GiB is allocated by PyTorch, and 452.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"error": "CUDA out of memory. Tried to allocate 166.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 78.69 MiB is free. Including non-PyTorch memory, this process has 10.67 GiB memory in use. Of the allocated memory 9.92 GiB is allocated by PyTorch, and 552.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"bleu": 0.011594504811215246, "precisions": [0.039735099337748346, 0.008839779005524863, 0.007743362831858407, 0.006644518272425249], "brevity_penalty": 1.0, "length_ratio": 11.185185185185185, "translation_length": 906, "reference_length": 81}
{"bleu": 0.08073127663220792, "precisions": [0.30275229357798167, 0.09387755102040816, 0.054136874361593465, 0.027607361963190184], "brevity_penalty": 1.0, "length_ratio": 1.0628385698808234, "translation_length": 981, "reference_length": 923}
{"bleu": 0.044921346868775705, "precisions": [0.5017667844522968, 0.29245283018867924, 0.256198347107438, 0.23995271867612294], "brevity_penalty": 0.1457598239900897, "length_ratio": 0.3417874396135266, "translation_length": 849, "reference_length": 2484}
{"bleu": 0.35375506225370973, "precisions": [0.5361744301288405, 0.3888888888888889, 0.3644488579940417, 0.34095427435387676], "brevity_penalty": 0.8817319372039844, "length_ratio": 0.8882042253521126, "translation_length": 1009, "reference_length": 1136}
{"bleu": 0.0, "precisions": [0.005813953488372093, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 9.45054945054945, "translation_length": 860, "reference_length": 91}
{"bleu": 0.06272579842487798, "precisions": [0.08117249154453213, 0.060948081264108354, 0.0576271186440678, 0.05429864253393665], "brevity_penalty": 1.0, "length_ratio": 7.990990990990991, "translation_length": 887, "reference_length": 111}
{"bleu": 0.0, "precisions": [0.002902757619738752, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 28.708333333333332, "translation_length": 689, "reference_length": 24}
{"bleu": 0.021856770721122562, "precisions": [0.03799019607843137, 0.022085889570552148, 0.018427518427518427, 0.014760147601476014], "brevity_penalty": 1.0, "length_ratio": 20.4, "translation_length": 816, "reference_length": 40}
{"bleu": 0.0, "precisions": [0.3119266055045872, 0.05069124423963134, 0.018518518518518517, 0.0], "brevity_penalty": 3.842471750125922e-15, "length_ratio": 0.029246042393345856, "translation_length": 218, "reference_length": 7454}
{"bleu": 1.751781167701329e-05, "precisions": [0.9988452655889145, 0.9872832369942196, 0.9791666666666666, 0.9721900347624566], "brevity_penalty": 1.779684066072189e-05, "length_ratio": 0.08377672438812034, "translation_length": 866, "reference_length": 10337}
{"bleu": 0.03531265190609549, "precisions": [0.10197368421052631, 0.033003300330033, 0.023178807947019868, 0.019933554817275746], "brevity_penalty": 1.0, "length_ratio": 3.234042553191489, "translation_length": 304, "reference_length": 94}
{"error": "CUDA out of memory. Tried to allocate 206.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 98.69 MiB is free. Including non-PyTorch memory, this process has 10.65 GiB memory in use. Of the allocated memory 9.99 GiB is allocated by PyTorch, and 462.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"error": "CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 58.69 MiB is free. Including non-PyTorch memory, this process has 10.69 GiB memory in use. Of the allocated memory 10.04 GiB is allocated by PyTorch, and 445.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"bleu": 0.07417216996949606, "precisions": [0.07582938388625593, 0.07473309608540925, 0.07363420427553444, 0.07253269916765755], "brevity_penalty": 1.0, "length_ratio": 11.887323943661972, "translation_length": 844, "reference_length": 71}
{"bleu": 0.0, "precisions": [0.02403846153846154, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 9.904761904761905, "translation_length": 208, "reference_length": 21}
{"bleu": 4.843538626403197e-05, "precisions": [0.6276595744680851, 0.41935483870967744, 0.3804347826086957, 0.3626373626373626], "brevity_penalty": 0.00011095531794566167, "length_ratio": 0.09894736842105263, "translation_length": 94, "reference_length": 950}
{"bleu": 0.049911565025411296, "precisions": [0.5929203539823009, 0.4732142857142857, 0.4594594594594595, 0.44545454545454544], "brevity_penalty": 0.10195886404613562, "length_ratio": 0.3045822102425876, "translation_length": 113, "reference_length": 371}
{"bleu": 0.016801936015399546, "precisions": [0.4279141104294479, 0.0629800307219662, 0.055384615384615386, 0.05084745762711865], "brevity_penalty": 0.18001341080763839, "length_ratio": 0.36836158192090396, "translation_length": 652, "reference_length": 1770}
{"bleu": 0.3784070606629313, "precisions": [0.9148629148629148, 0.8945086705202312, 0.894356005788712, 0.8942028985507247], "brevity_penalty": 0.4207144818306733, "length_ratio": 0.5359628770301624, "translation_length": 693, "reference_length": 1293}
{"bleu": 0.0, "precisions": [0.06635071090047394, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 4.906976744186046, "translation_length": 211, "reference_length": 43}
{"bleu": 0.0006994994746075039, "precisions": [0.4586693548387097, 0.10494450050454086, 0.07777777777777778, 0.05156723963599596], "brevity_penalty": 0.00593424109193165, "length_ratio": 0.163211582757486, "translation_length": 992, "reference_length": 6078}
{"bleu": 0.05358536423904161, "precisions": [0.20320855614973263, 0.07526881720430108, 0.043243243243243246, 0.021739130434782608], "brevity_penalty": 0.8701954536871993, "length_ratio": 0.8779342723004695, "translation_length": 187, "reference_length": 213}
{"bleu": 0.0, "precisions": [0.002176278563656148, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 70.6923076923077, "translation_length": 919, "reference_length": 13}
{"bleu": 0.017885780782498455, "precisions": [0.031425364758698095, 0.017977528089887642, 0.014623172103487065, 0.012387387387387387], "brevity_penalty": 1.0, "length_ratio": 6.456521739130435, "translation_length": 891, "reference_length": 138}
{"bleu": 0.0, "precisions": [0.00404040404040404, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 41.25, "translation_length": 990, "reference_length": 24}
{"bleu": 0.0, "precisions": [0.06666666666666667, 0.0, 0.0, 0.0], "brevity_penalty": 0.1266071027890836, "length_ratio": 0.32608695652173914, "translation_length": 15, "reference_length": 46}
{"bleu": 0.11248869066424795, "precisions": [0.1328191945158526, 0.10806174957118353, 0.10643776824034334, 0.10481099656357389], "brevity_penalty": 1.0, "length_ratio": 4.290441176470588, "translation_length": 1167, "reference_length": 272}
{"bleu": 0.3121173188036847, "precisions": [0.9914004914004914, 0.990159901599016, 0.9901477832512315, 0.9901356350184957], "brevity_penalty": 0.31512334203180986, "length_ratio": 0.4640820980615735, "translation_length": 814, "reference_length": 1754}
{"bleu": 0.059596574232930244, "precisions": [0.06437768240343347, 0.06236559139784946, 0.05818965517241379, 0.05399568034557235], "brevity_penalty": 1.0, "length_ratio": 15.533333333333333, "translation_length": 466, "reference_length": 30}
{"bleu": 0.11401614125185468, "precisions": [0.11927710843373494, 0.11338962605548854, 0.11231884057971014, 0.11124546553808948], "brevity_penalty": 1.0, "length_ratio": 8.137254901960784, "translation_length": 830, "reference_length": 102}
{"error": "CUDA out of memory. Tried to allocate 504.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 396.69 MiB is free. Including non-PyTorch memory, this process has 10.36 GiB memory in use. Of the allocated memory 9.77 GiB is allocated by PyTorch, and 389.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"bleu": 0.39205870869577936, "precisions": [0.42953020134228187, 0.40540540540540543, 0.38095238095238093, 0.3561643835616438], "brevity_penalty": 1.0, "length_ratio": 2.0135135135135136, "translation_length": 149, "reference_length": 74}
{"bleu": 0.0, "precisions": [0.07865168539325842, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 2.5428571428571427, "translation_length": 89, "reference_length": 35}
{"bleu": 0.0, "precisions": [0.10084033613445378, 0.01694915254237288, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 4.103448275862069, "translation_length": 119, "reference_length": 29}
{"bleu": 0.5562312462265013, "precisions": [0.5686274509803921, 0.5589123867069486, 0.5521936459909228, 0.5454545454545454], "brevity_penalty": 1.0, "length_ratio": 1.753968253968254, "translation_length": 663, "reference_length": 378}
{"bleu": 0.0, "precisions": [0.027906976744186046, 0.00186219739292365, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 13.271604938271604, "translation_length": 1075, "reference_length": 81}
{"bleu": 0.0, "precisions": [0.10810810810810811, 0.00909090909090909, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 2.22, "translation_length": 111, "reference_length": 50}
{"bleu": 0.0, "precisions": [0.033707865168539325, 0.011363636363636364, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 6.357142857142857, "translation_length": 89, "reference_length": 14}
{"bleu": 3.0357460649451624e-06, "precisions": [0.4207708779443255, 0.31189710610932475, 0.30150214592274677, 0.2996777658431794], "brevity_penalty": 9.199515456751404e-06, "length_ratio": 0.0793880152996175, "translation_length": 934, "reference_length": 11765}
{"bleu": 0.0, "precisions": [0.17177914110429449, 0.0010235414534288639, 0.0, 0.0], "brevity_penalty": 0.00038867744979987376, "length_ratio": 0.11295911295911296, "translation_length": 978, "reference_length": 8658}
{"bleu": 0.0, "precisions": [0.009018036072144289, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 32.193548387096776, "translation_length": 998, "reference_length": 31}
{"bleu": 0.0038871526593692823, "precisions": [0.7053571428571429, 0.6155810983397191, 0.5843989769820972, 0.5633802816901409], "brevity_penalty": 0.006321646957670748, "length_ratio": 0.16491375683634835, "translation_length": 784, "reference_length": 4754}
{"bleu": 0.031655983911173344, "precisions": [0.04667328699106256, 0.02982107355864811, 0.027860696517412936, 0.025896414342629483], "brevity_penalty": 1.0, "length_ratio": 9.5, "translation_length": 1007, "reference_length": 106}
{"bleu": 0.18602080642245603, "precisions": [0.922077922077922, 0.9130434782608695, 0.9126637554585153, 0.9122807017543859], "brevity_penalty": 0.2032997896403625, "length_ratio": 0.38564273789649417, "translation_length": 231, "reference_length": 599}
{"bleu": 0.008389136021760906, "precisions": [0.015277777777777777, 0.008344923504867872, 0.006963788300835654, 0.005578800557880056], "brevity_penalty": 1.0, "length_ratio": 51.42857142857143, "translation_length": 720, "reference_length": 14}
{"bleu": 0.0, "precisions": [0.17307692307692307, 0.0, 0.0, 0.0], "brevity_penalty": 0.14337270762769766, "length_ratio": 0.33986928104575165, "translation_length": 52, "reference_length": 153}
{"bleu": 0.0, "precisions": [0.00819672131147541, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 6.777777777777778, "translation_length": 122, "reference_length": 18}
{"bleu": 0.0, "precisions": [0.05263157894736842, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 4.885714285714286, "translation_length": 171, "reference_length": 35}
{"bleu": 0.02975973387714559, "precisions": [0.43539630836047777, 0.25217391304347825, 0.20783460282916214, 0.1786492374727669], "brevity_penalty": 0.11777498635533175, "length_ratio": 0.3185748875821515, "translation_length": 921, "reference_length": 2891}
{"bleu": 0.2507312868292768, "precisions": [0.51, 0.3634085213032581, 0.32663316582914576, 0.28967254408060455], "brevity_penalty": 0.6890096515578809, "length_ratio": 0.7285974499089253, "translation_length": 400, "reference_length": 549}
{"bleu": 4.151533312718176e-06, "precisions": [0.24041811846689895, 0.07441860465116279, 0.05820721769499418, 0.05128205128205128], "brevity_penalty": 4.856359075584466e-05, "length_ratio": 0.09146924466163817, "translation_length": 861, "reference_length": 9413}
{"bleu": 0.03152815676029721, "precisions": [0.034954407294832825, 0.0319634703196347, 0.03048780487804878, 0.02900763358778626], "brevity_penalty": 1.0, "length_ratio": 22.689655172413794, "translation_length": 658, "reference_length": 29}
{"bleu": 0.06137442277140537, "precisions": [0.30988786952089703, 0.1846938775510204, 0.1491317671092952, 0.12576687116564417], "brevity_penalty": 0.33906975040884235, "length_ratio": 0.480411361410382, "translation_length": 981, "reference_length": 2042}
{"bleu": 0.0, "precisions": [0.053518334985133795, 0.00496031746031746, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 7.94488188976378, "translation_length": 1009, "reference_length": 127}
{"bleu": 0.2774989543373628, "precisions": [0.2847682119205298, 0.28, 0.2751677852348993, 0.2702702702702703], "brevity_penalty": 1.0, "length_ratio": 3.02, "translation_length": 151, "reference_length": 50}
{"bleu": 0.007290532200987469, "precisions": [0.01045751633986928, 0.007853403141361256, 0.00655307994757536, 0.005249343832020997], "brevity_penalty": 1.0, "length_ratio": 95.625, "translation_length": 765, "reference_length": 8}
{"bleu": 0.7681140495106317, "precisions": [0.7921108742004265, 0.7705442902881536, 0.7596153846153846, 0.7508021390374332], "brevity_penalty": 1.0, "length_ratio": 1.2197659297789336, "translation_length": 938, "reference_length": 769}
{"bleu": 0.13640422388312504, "precisions": [0.4306569343065693, 0.3088235294117647, 0.2962962962962963, 0.2835820895522388], "brevity_penalty": 0.419532980032414, "length_ratio": 0.53515625, "translation_length": 137, "reference_length": 256}
{"bleu": 2.3027866966339003e-08, "precisions": [0.782258064516129, 0.6829268292682927, 0.6557377049180327, 0.6528925619834711], "brevity_penalty": 3.3298884617735764e-08, "length_ratio": 0.054891544931385566, "translation_length": 124, "reference_length": 2259}
{"bleu": 0.0, "precisions": [0.03636363636363636, 0.012195121951219513, 0.006134969325153374, 0.0], "brevity_penalty": 1.0, "length_ratio": 9.705882352941176, "translation_length": 165, "reference_length": 17}
{"bleu": 0.03774135845930576, "precisions": [0.044642857142857144, 0.040268456375838924, 0.03587443946188341, 0.03146067415730337], "brevity_penalty": 1.0, "length_ratio": 21.333333333333332, "translation_length": 448, "reference_length": 21}
{"bleu": 0.0, "precisions": [0.17567567567567569, 0.0136986301369863, 0.0, 0.0], "brevity_penalty": 0.00030108796056705224, "length_ratio": 0.10979228486646884, "translation_length": 74, "reference_length": 674}
{"bleu": 0.0, "precisions": [0.2535211267605634, 0.07142857142857142, 0.028985507246376812, 0.0], "brevity_penalty": 0.009056927430866746, "length_ratio": 0.17530864197530865, "translation_length": 71, "reference_length": 405}
{"bleu": 0.12341817432473641, "precisions": [0.25876288659793817, 0.13415892672858618, 0.09504132231404959, 0.0703205791106515], "brevity_penalty": 1.0, "length_ratio": 2.5797872340425534, "translation_length": 970, "reference_length": 376}
{"bleu": 0.013816650210796321, "precisions": [0.018730489073881373, 0.013541666666666667, 0.01251303441084463, 0.011482254697286013], "brevity_penalty": 1.0, "length_ratio": 15.754098360655737, "translation_length": 961, "reference_length": 61}
{"error": "CUDA out of memory. Tried to allocate 296.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 258.69 MiB is free. Including non-PyTorch memory, this process has 10.49 GiB memory in use. Of the allocated memory 9.79 GiB is allocated by PyTorch, and 501.56 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"bleu": 0.6449964147315165, "precisions": [0.9381563593932322, 0.9182242990654206, 0.9099415204678363, 0.9039812646370023], "brevity_penalty": 0.7030044442586033, "length_ratio": 0.7394305435720449, "translation_length": 857, "reference_length": 1159}
{"bleu": 0.14657826265217572, "precisions": [0.6514619883040935, 0.5526932084309133, 0.5146541617819461, 0.4765258215962441], "brevity_penalty": 0.26889070248870633, "length_ratio": 0.4322548028311426, "translation_length": 855, "reference_length": 1978}
{"bleu": 0.0, "precisions": [0.01757469244288225, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 16.257142857142856, "translation_length": 569, "reference_length": 35}
{"bleu": 0.4966262650292515, "precisions": [0.5109489051094891, 0.49415204678362573, 0.49194729136163984, 0.4897360703812317], "brevity_penalty": 1.0, "length_ratio": 1.793193717277487, "translation_length": 685, "reference_length": 382}
{"bleu": 0.0, "precisions": [0.05517241379310345, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 4.67741935483871, "translation_length": 145, "reference_length": 31}
{"bleu": 0.03314792279541704, "precisions": [0.03474484256243214, 0.03369565217391304, 0.03264417845484222, 0.03159041394335512], "brevity_penalty": 1.0, "length_ratio": 23.615384615384617, "translation_length": 921, "reference_length": 39}
{"bleu": 0.0, "precisions": [0.09262948207171315, 0.006979062811565304, 0.001996007984031936, 0.0], "brevity_penalty": 1.0, "length_ratio": 3.207667731629393, "translation_length": 1004, "reference_length": 313}
{"bleu": 1.4016888729451954e-05, "precisions": [0.5234375, 0.43137254901960786, 0.42913385826771655, 0.4268774703557312], "brevity_penalty": 3.1081236463601456e-05, "length_ratio": 0.08788190868520426, "translation_length": 256, "reference_length": 2913}
{"bleu": 0.1073529892461746, "precisions": [0.11320754716981132, 0.10636182902584493, 0.1054726368159204, 0.10458167330677291], "brevity_penalty": 1.0, "length_ratio": 7.929133858267717, "translation_length": 1007, "reference_length": 127}
{"bleu": 0.272594625081231, "precisions": [0.28125, 0.2755905511811024, 0.2698412698412698, 0.264], "brevity_penalty": 1.0, "length_ratio": 2.9767441860465116, "translation_length": 128, "reference_length": 43}
{"bleu": 0.0, "precisions": [0.047619047619047616, 0.00954653937947494, 0.004784688995215311, 0.0], "brevity_penalty": 1.0, "length_ratio": 9.767441860465116, "translation_length": 420, "reference_length": 43}
{"bleu": 0.10151819441086926, "precisions": [0.10976948408342481, 0.1043956043956044, 0.09900990099009901, 0.09361233480176212], "brevity_penalty": 1.0, "length_ratio": 8.133928571428571, "translation_length": 911, "reference_length": 112}
{"bleu": 7.942133849109843e-20, "precisions": [0.7272727272727273, 0.5473684210526316, 0.4823943661971831, 0.43109540636042404], "brevity_penalty": 1.480638494940595e-19, "length_ratio": 0.02254453728519628, "translation_length": 286, "reference_length": 12686}
{"error": "CUDA out of memory. Tried to allocate 288.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 168.69 MiB is free. Including non-PyTorch memory, this process has 10.58 GiB memory in use. Of the allocated memory 9.93 GiB is allocated by PyTorch, and 456.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"bleu": 0.0079585242986188, "precisions": [0.01270772238514174, 0.007827788649706457, 0.0068560235063663075, 0.0058823529411764705], "brevity_penalty": 1.0, "length_ratio": 37.888888888888886, "translation_length": 1023, "reference_length": 27}
{"bleu": 0.0, "precisions": [0.06766917293233082, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 5.911111111111111, "translation_length": 266, "reference_length": 45}
{"bleu": 0.0, "precisions": [0.02240325865580448, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 7.383458646616542, "translation_length": 982, "reference_length": 133}
{"bleu": 0.051064139141322136, "precisions": [0.6042780748663101, 0.5067024128686327, 0.5026881720430108, 0.5013477088948787], "brevity_penalty": 0.09688557858917293, "length_ratio": 0.29991980753809144, "translation_length": 374, "reference_length": 1247}
{"error": "CUDA out of memory. Tried to allocate 244.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 96.69 MiB is free. Including non-PyTorch memory, this process has 10.65 GiB memory in use. Of the allocated memory 9.92 GiB is allocated by PyTorch, and 534.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"bleu": 0.0, "precisions": [0.06857142857142857, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 3.5714285714285716, "translation_length": 175, "reference_length": 49}
{"bleu": 0.03728024973949057, "precisions": [0.04923413566739606, 0.03504928806133625, 0.03399122807017544, 0.03293084522502744], "brevity_penalty": 1.0, "length_ratio": 13.246376811594203, "translation_length": 914, "reference_length": 69}
{"bleu": 0.0, "precisions": [0.023185483870967742, 0.0010090817356205853, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 14.805970149253731, "translation_length": 992, "reference_length": 67}
{"bleu": 0.0, "precisions": [0.1326530612244898, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 1.0425531914893618, "translation_length": 196, "reference_length": 188}
{"bleu": 0.12422025740041291, "precisions": [0.21100917431192662, 0.11734693877551021, 0.10112359550561797, 0.0950920245398773], "brevity_penalty": 1.0, "length_ratio": 2.165562913907285, "translation_length": 981, "reference_length": 453}
{"error": "Unsloth: input length 307022 + max_new_tokens 1024 exceeds the maximum sequence length of 131072!\nYou will need to do long context extension by increasing the `max_seq_length` in `FastLanguageModel.from_pretrained`."}
{"error": "Unsloth: input length 661167 + max_new_tokens 1024 exceeds the maximum sequence length of 131072!\nYou will need to do long context extension by increasing the `max_seq_length` in `FastLanguageModel.from_pretrained`."}
{"bleu": 6.490843575320691e-10, "precisions": [0.8155339805825242, 0.696078431372549, 0.6138613861386139, 0.54], "brevity_penalty": 9.855077969775608e-10, "length_ratio": 0.04600267976775346, "translation_length": 103, "reference_length": 2239}
{"bleu": 0.0, "precisions": [0.0975609756097561, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 1.8222222222222222, "translation_length": 82, "reference_length": 45}
{"bleu": 0.0, "precisions": [0.042111506524317915, 0.0017804154302670622, 0.0005938242280285036, 0.0], "brevity_penalty": 1.0, "length_ratio": 12.129496402877697, "translation_length": 1686, "reference_length": 139}
{"bleu": 0.0, "precisions": [0.01216089803554724, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 7.032894736842105, "translation_length": 1069, "reference_length": 152}
{"bleu": 0.04046213474149278, "precisions": [0.04535637149028078, 0.04, 0.03896103896103896, 0.03791982665222102], "brevity_penalty": 1.0, "length_ratio": 20.57777777777778, "translation_length": 926, "reference_length": 45}
{"error": "Unsloth: input length 241184 + max_new_tokens 1024 exceeds the maximum sequence length of 131072!\nYou will need to do long context extension by increasing the `max_seq_length` in `FastLanguageModel.from_pretrained`."}
{"bleu": 0.015819447224530173, "precisions": [0.01793103448275862, 0.016574585635359115, 0.015214384508990318, 0.013850415512465374], "brevity_penalty": 1.0, "length_ratio": 34.523809523809526, "translation_length": 725, "reference_length": 21}
{"bleu": 0.6128990955361217, "precisions": [0.811088295687885, 0.7564234326824255, 0.7479423868312757, 0.7435633367662204], "brevity_penalty": 0.8019257794598402, "length_ratio": 0.8191757779646762, "translation_length": 974, "reference_length": 1189}
{"error": "CUDA out of memory. Tried to allocate 1.41 GiB. GPU 0 has a total capacity of 10.75 GiB of which 1.31 GiB is free. Including non-PyTorch memory, this process has 9.44 GiB memory in use. Of the allocated memory 8.84 GiB is allocated by PyTorch, and 404.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"bleu": 0.0388655676682954, "precisions": [0.7777777777777778, 0.6551724137931034, 0.591304347826087, 0.5614035087719298], "brevity_penalty": 0.06060251995116927, "length_ratio": 0.26292134831460673, "translation_length": 117, "reference_length": 445}
{"bleu": 0.0, "precisions": [0.19148936170212766, 0.0, 0.0, 0.0], "brevity_penalty": 0.2303661898473039, "length_ratio": 0.4051724137931034, "translation_length": 47, "reference_length": 116}
{"bleu": 0.0033474734439613257, "precisions": [0.0064995357474466105, 0.0037174721189591076, 0.0027906976744186047, 0.00186219739292365], "brevity_penalty": 1.0, "length_ratio": 67.3125, "translation_length": 1077, "reference_length": 16}
{"bleu": 0.00029590099322680563, "precisions": [0.4777777777777778, 0.03064066852367688, 0.013966480446927373, 0.0056022408963585435], "brevity_penalty": 0.009044887884114838, "length_ratio": 0.17526777020447906, "translation_length": 360, "reference_length": 2054}
{"bleu": 0.08094970600963317, "precisions": [0.10177514792899409, 0.08056872037914692, 0.07473309608540925, 0.07007125890736342], "brevity_penalty": 1.0, "length_ratio": 6.1231884057971016, "translation_length": 845, "reference_length": 138}
{"bleu": 1.568562063574002e-22, "precisions": [0.5105263157894737, 0.19047619047619047, 0.12234042553191489, 0.0855614973262032], "brevity_penalty": 8.78162543297525e-22, "length_ratio": 0.020208466283769412, "translation_length": 190, "reference_length": 9402}
{"bleu": 0.0, "precisions": [0.011988011988011988, 0.002, 0.001001001001001001, 0.0], "brevity_penalty": 1.0, "length_ratio": 32.29032258064516, "translation_length": 1001, "reference_length": 31}
{"bleu": 0.0, "precisions": [0.03875968992248062, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 8.6, "translation_length": 258, "reference_length": 30}
{"bleu": 0.0, "precisions": [1.0, 0.0, 0.0, 0.0], "brevity_penalty": 0.0, "length_ratio": 6.678242286630158e-05, "translation_length": 1, "reference_length": 14974}
{"bleu": 0.0, "precisions": [0.08653846153846154, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 4.622222222222222, "translation_length": 208, "reference_length": 45}
{"bleu": 0.0, "precisions": [0.0, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 24.0, "translation_length": 168, "reference_length": 7}
{"bleu": 0.0, "precisions": [0.1103448275862069, 0.0, 0.0, 0.0], "brevity_penalty": 0.21334865230243835, "length_ratio": 0.39295392953929537, "translation_length": 145, "reference_length": 369}
{"bleu": 0.4598029235908175, "precisions": [0.5044642857142857, 0.45739910313901344, 0.44594594594594594, 0.4343891402714932], "brevity_penalty": 1.0, "length_ratio": 1.160621761658031, "translation_length": 224, "reference_length": 193}
{"bleu": 0.02160844966164622, "precisions": [0.027422303473491772, 0.023809523809523808, 0.02018348623853211, 0.016544117647058824], "brevity_penalty": 1.0, "length_ratio": 36.46666666666667, "translation_length": 547, "reference_length": 15}
{"bleu": 0.0, "precisions": [0.001949317738791423, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 48.857142857142854, "translation_length": 1026, "reference_length": 21}
{"bleu": 0.017709350494380018, "precisions": [0.020792079207920793, 0.018830525272547076, 0.016865079365079364, 0.014895729890764648], "brevity_penalty": 1.0, "length_ratio": 34.827586206896555, "translation_length": 1010, "reference_length": 29}
{"bleu": 0.0, "precisions": [0.23717948717948717, 0.01935483870967742, 0.006493506493506494, 0.0], "brevity_penalty": 0.6634801052198983, "length_ratio": 0.7090909090909091, "translation_length": 156, "reference_length": 220}
{"bleu": 0.10071799942327922, "precisions": [0.11331444759206799, 0.09943181818181818, 0.09686609686609686, 0.09428571428571429], "brevity_penalty": 1.0, "length_ratio": 8.209302325581396, "translation_length": 353, "reference_length": 43}
{"bleu": 0.0, "precisions": [0.17204301075268819, 0.021739130434782608, 0.01098901098901099, 0.0], "brevity_penalty": 0.14128314984652754, "length_ratio": 0.3381818181818182, "translation_length": 93, "reference_length": 275}
{"bleu": 0.012756910737634195, "precisions": [0.04205607476635514, 0.014084507042253521, 0.009433962264150943, 0.004739336492890996], "brevity_penalty": 1.0, "length_ratio": 11.88888888888889, "translation_length": 214, "reference_length": 18}
{"bleu": 0.0, "precisions": [0.19148936170212766, 0.0, 0.0, 0.0], "brevity_penalty": 9.560177712212092e-05, "length_ratio": 0.0975103734439834, "translation_length": 94, "reference_length": 964}
{"bleu": 0.28060855416197433, "precisions": [0.43315508021390375, 0.2570281124497992, 0.24262734584450402, 0.2295302013422819], "brevity_penalty": 1.0, "length_ratio": 1.3800738007380073, "translation_length": 748, "reference_length": 542}
{"bleu": 2.7567399599288857e-11, "precisions": [0.5277777777777778, 0.3380281690140845, 0.32857142857142857, 0.3188405797101449], "brevity_penalty": 7.455794971071121e-11, "length_ratio": 0.041119360365505425, "translation_length": 72, "reference_length": 1751}
{"bleu": 0.04063284624713107, "precisions": [0.1868629671574179, 0.02834467120181406, 0.02383654937570942, 0.02159090909090909], "brevity_penalty": 1.0, "length_ratio": 2.597058823529412, "translation_length": 883, "reference_length": 340}
{"bleu": 0.0, "precisions": [0.0910209102091021, 0.0049261083743842365, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 1.1291666666666667, "translation_length": 813, "reference_length": 720}
{"bleu": 0.020285239613532784, "precisions": [0.7466666666666667, 0.6363636363636364, 0.6058981233243967, 0.5806451612903226], "brevity_penalty": 0.031724479673501, "length_ratio": 0.22468544038346316, "translation_length": 375, "reference_length": 1669}
{"bleu": 0.012043216760695906, "precisions": [0.043121149897330596, 0.014388489208633094, 0.00823045267489712, 0.004119464469618949], "brevity_penalty": 1.0, "length_ratio": 9.643564356435643, "translation_length": 974, "reference_length": 101}
{"bleu": 2.526051608125611e-11, "precisions": [0.6408934707903781, 0.1153184165232358, 0.03103448275862069, 0.017271157167530225], "brevity_penalty": 3.1840479751432447e-10, "length_ratio": 0.0437298068975881, "translation_length": 582, "reference_length": 13309}
{"bleu": 0.0, "precisions": [0.05828220858895705, 0.0, 0.0, 0.0], "brevity_penalty": 0.7358356651299203, "length_ratio": 0.7652582159624414, "translation_length": 978, "reference_length": 1278}
{"bleu": 0.03477513220851544, "precisions": [0.09572072072072071, 0.02593010146561443, 0.024830699774266364, 0.023728813559322035], "brevity_penalty": 1.0, "length_ratio": 5.584905660377358, "translation_length": 888, "reference_length": 159}
{"error": "Unsloth: input length 161192 + max_new_tokens 1024 exceeds the maximum sequence length of 131072!\nYou will need to do long context extension by increasing the `max_seq_length` in `FastLanguageModel.from_pretrained`."}
{"bleu": 0.007281836103556038, "precisions": [0.376, 0.16316316316316315, 0.13827655310621242, 0.11534603811434303], "brevity_penalty": 0.04117187093906772, "length_ratio": 0.2386634844868735, "translation_length": 1000, "reference_length": 4190}
{"bleu": 0.12299141558830978, "precisions": [0.8842443729903537, 0.832258064516129, 0.8090614886731392, 0.788961038961039], "brevity_penalty": 0.14856207248025305, "length_ratio": 0.3440265486725664, "translation_length": 311, "reference_length": 904}
{"bleu": 0.1095204644585799, "precisions": [0.4208955223880597, 0.2275449101796407, 0.21621621621621623, 0.21385542168674698], "brevity_penalty": 0.42455385349919117, "length_ratio": 0.5385852090032154, "translation_length": 335, "reference_length": 622}
{"bleu": 5.558359204800186e-10, "precisions": [0.6693548387096774, 0.4796747967479675, 0.4426229508196721, 0.4049586776859504], "brevity_penalty": 1.1348399677305291e-09, "length_ratio": 0.04630321135175504, "translation_length": 124, "reference_length": 2678}
{"bleu": 0.0, "precisions": [0.275, 0.07692307692307693, 0.0, 0.0], "brevity_penalty": 0.927743486328553, "length_ratio": 0.9302325581395349, "translation_length": 40, "reference_length": 43}
{"bleu": 0.0, "precisions": [0.1311806256306761, 0.0, 0.0, 0.0], "brevity_penalty": 1.227202578225599e-08, "length_ratio": 0.052040119729034294, "translation_length": 991, "reference_length": 19043}
{"bleu": 0.20368207587936382, "precisions": [0.26153846153846155, 0.1937984496124031, 0.1875, 0.18110236220472442], "brevity_penalty": 1.0, "length_ratio": 1.5116279069767442, "translation_length": 130, "reference_length": 86}
{"bleu": 1.003476919377579e-07, "precisions": [0.2915129151291513, 0.17777777777777778, 0.16356877323420074, 0.14925373134328357], "brevity_penalty": 5.320680379255252e-07, "length_ratio": 0.06473960821786909, "translation_length": 271, "reference_length": 4186}
{"bleu": 0.0, "precisions": [0.014184397163120567, 0.001183431952662722, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 24.88235294117647, "translation_length": 846, "reference_length": 34}
{"bleu": 0.11158790811984559, "precisions": [0.7175, 0.606516290726817, 0.5904522613065326, 0.5869017632241813], "brevity_penalty": 0.17906614791149328, "length_ratio": 0.36764705882352944, "translation_length": 400, "reference_length": 1088}
{"bleu": 0.0, "precisions": [0.012048192771084338, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 3.608695652173913, "translation_length": 83, "reference_length": 23}
{"bleu": 0.05622808486311756, "precisions": [0.06600985221674877, 0.054240631163708086, 0.05330700888450148, 0.05237154150197629], "brevity_penalty": 1.0, "length_ratio": 8.319672131147541, "translation_length": 1015, "reference_length": 122}
{"bleu": 0.051721045633773825, "precisions": [0.23333333333333334, 0.1397288842544317, 0.13465553235908143, 0.13166144200626959], "brevity_penalty": 0.33356528800084606, "length_ratio": 0.4766633565044687, "translation_length": 960, "reference_length": 2014}
{"bleu": 0.0, "precisions": [0.14893617021276595, 0.0, 0.0, 0.0], "brevity_penalty": 2.931582734097728e-07, "length_ratio": 0.0623342175066313, "translation_length": 47, "reference_length": 754}
{"bleu": 0.07630655266988359, "precisions": [0.0778727445394112, 0.07699619771863118, 0.07611798287345385, 0.07428571428571429], "brevity_penalty": 1.0, "length_ratio": 11.831460674157304, "translation_length": 1053, "reference_length": 89}
{"bleu": 0.0, "precisions": [0.29190056134723336, 0.0898876404494382, 0.04497991967871486, 0.0], "brevity_penalty": 0.6024103874138526, "length_ratio": 0.6636508781266631, "translation_length": 1247, "reference_length": 1879}
{"bleu": 0.3409858981263285, "precisions": [0.8731343283582089, 0.8646616541353384, 0.8560606060606061, 0.8473282442748091], "brevity_penalty": 0.3963834871912119, "length_ratio": 0.5193798449612403, "translation_length": 134, "reference_length": 258}
{"bleu": 0.0, "precisions": [0.009900990099009901, 0.003303964757709251, 0.0011025358324145535, 0.0], "brevity_penalty": 1.0, "length_ratio": 33.666666666666664, "translation_length": 909, "reference_length": 27}
{"bleu": 0.34642848072931204, "precisions": [0.35195530726256985, 0.34831460674157305, 0.3446327683615819, 0.3409090909090909], "brevity_penalty": 1.0, "length_ratio": 2.557142857142857, "translation_length": 179, "reference_length": 70}
{"bleu": 0.0, "precisions": [0.16372549019607843, 0.0, 0.0, 0.0], "brevity_penalty": 0.8760317528329519, "length_ratio": 0.8831168831168831, "translation_length": 1020, "reference_length": 1155}
{"bleu": 0.152315776959082, "precisions": [0.27205882352941174, 0.14814814814814814, 0.12686567164179105, 0.10526315789473684], "brevity_penalty": 1.0, "length_ratio": 1.6790123456790123, "translation_length": 136, "reference_length": 81}
{"bleu": 0.015946328589201423, "precisions": [0.8759036144578313, 0.7551266586248492, 0.7125603864734299, 0.6674727932285369], "brevity_penalty": 0.02129255941866922, "length_ratio": 0.2062111801242236, "translation_length": 830, "reference_length": 4025}
{"bleu": 3.895785320330613e-14, "precisions": [0.7952218430034129, 0.5547945205479452, 0.5154639175257731, 0.5103448275862069], "brevity_penalty": 6.674587087833038e-14, "length_ratio": 0.03191025920278806, "translation_length": 293, "reference_length": 9182}
{"bleu": 0.27457743482540153, "precisions": [0.41368078175895767, 0.2908496732026144, 0.25573770491803277, 0.23355263157894737], "brevity_penalty": 0.9430538228126156, "length_ratio": 0.9446153846153846, "translation_length": 307, "reference_length": 325}
{"bleu": 0.08362379466352252, "precisions": [0.10817031070195628, 0.08294930875576037, 0.07612456747404844, 0.07159353348729793], "brevity_penalty": 1.0, "length_ratio": 6.076923076923077, "translation_length": 869, "reference_length": 143}
{"bleu": 0.011378487351719611, "precisions": [0.09403437815975733, 0.012145748987854251, 0.010131712259371834, 0.008113590263691683], "brevity_penalty": 0.6500291185592153, "length_ratio": 0.6989399293286219, "translation_length": 989, "reference_length": 1415}
{"bleu": 0.0, "precisions": [0.0, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 125.85714285714286, "translation_length": 881, "reference_length": 7}
{"bleu": 0.02317391164170999, "precisions": [0.8238557558945908, 0.5305555555555556, 0.5104311543810849, 0.49164345403899723], "brevity_penalty": 0.04026771408441369, "length_ratio": 0.23740533421139282, "translation_length": 721, "reference_length": 3037}
{"bleu": 0.3730217626811023, "precisions": [0.3793103448275862, 0.3751438434982739, 0.3709677419354839, 0.36678200692041524], "brevity_penalty": 1.0, "length_ratio": 2.5588235294117645, "translation_length": 870, "reference_length": 340}
{"bleu": 0.025334766052465207, "precisions": [0.03271028037383177, 0.028169014084507043, 0.02358490566037736, 0.018957345971563982], "brevity_penalty": 1.0, "length_ratio": 15.285714285714286, "translation_length": 214, "reference_length": 14}
