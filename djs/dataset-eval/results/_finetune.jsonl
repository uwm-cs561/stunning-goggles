{"bleu": 0.0, "precisions": [0.09090909090909091, 0.0, 0.0, 0.0], "brevity_penalty": 0.1623206111818482, "length_ratio": 0.3548387096774194, "translation_length": 11, "reference_length": 31}
{"bleu": 0.0, "precisions": [0.06666666666666667, 0.0, 0.0, 0.0], "brevity_penalty": 0.8187307530779819, "length_ratio": 0.8333333333333334, "translation_length": 15, "reference_length": 18}
{"bleu": 0.0, "precisions": [0.4, 0.0, 0.0, 0.0], "brevity_penalty": 5.588331392518268e-08, "length_ratio": 0.05649717514124294, "translation_length": 10, "reference_length": 177}
{"bleu": 0.0, "precisions": [0.21052631578947367, 0.0, 0.0, 0.0], "brevity_penalty": 0.15035789770837657, "length_ratio": 0.34545454545454546, "translation_length": 19, "reference_length": 55}
{"bleu": 0.2878383211956101, "precisions": [0.5958904109589042, 0.5103448275862069, 0.5, 0.48951048951048953], "brevity_penalty": 0.5510716670979153, "length_ratio": 0.6266094420600858, "translation_length": 146, "reference_length": 233}
{"bleu": 0.2870986368067365, "precisions": [0.8423423423423423, 0.7413533834586467, 0.7213855421686747, 0.7043740573152338], "brevity_penalty": 0.3825251175932902, "length_ratio": 0.5099540581929556, "translation_length": 666, "reference_length": 1306}
{"error": "CUDA out of memory. Tried to allocate 336.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 116.69 MiB is free. Including non-PyTorch memory, this process has 10.63 GiB memory in use. Of the allocated memory 10.24 GiB is allocated by PyTorch, and 189.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"bleu": 0.034957052200575815, "precisions": [0.06613226452905811, 0.03614457831325301, 0.028169014084507043, 0.02217741935483871], "brevity_penalty": 1.0, "length_ratio": 6.835616438356165, "translation_length": 499, "reference_length": 73}
{"bleu": 0.0, "precisions": [0.037037037037037035, 0.0, 0.0, 0.0], "brevity_penalty": 0.21107208779109024, "length_ratio": 0.391304347826087, "translation_length": 27, "reference_length": 69}
{"bleu": 0.0009042671501694322, "precisions": [0.4323181049069374, 0.12785774767146485, 0.11440677966101695, 0.10093299406276506], "brevity_penalty": 0.005689091340364492, "length_ratio": 0.162095447065277, "translation_length": 1182, "reference_length": 7292}
{"bleu": 0.0, "precisions": [0.01381509032943677, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 25.43243243243243, "translation_length": 941, "reference_length": 37}
{"bleu": 3.822462363647482e-06, "precisions": [0.9853249475890985, 0.7310924369747899, 0.4884210526315789, 0.2552742616033755], "brevity_penalty": 6.9824032141979536e-06, "length_ratio": 0.07768729641693811, "translation_length": 477, "reference_length": 6140}
{"bleu": 0.005615620989948031, "precisions": [0.5735567970204841, 0.37371854613233924, 0.3218283582089552, 0.28851540616246496], "brevity_penalty": 0.014950963757648796, "length_ratio": 0.19219756621331424, "translation_length": 1074, "reference_length": 5588}
{"bleu": 0.0, "precisions": [0.0074142724745134385, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 59.94444444444444, "translation_length": 1079, "reference_length": 18}
{"bleu": 0.0, "precisions": [0.047619047619047616, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 1.0, "translation_length": 21, "reference_length": 21}
{"bleu": 0.0, "precisions": [0.2503311258278146, 0.0, 0.0, 0.0], "brevity_penalty": 0.500216021037585, "length_ratio": 0.5907668231611893, "translation_length": 755, "reference_length": 1278}
{"bleu": 8.696831400861647e-05, "precisions": [0.3684210526315789, 0.08928571428571429, 0.05454545454545454, 0.018518518518518517], "brevity_penalty": 0.0011454819373363377, "length_ratio": 0.12866817155756208, "translation_length": 57, "reference_length": 443}
{"bleu": 0.0026251614141795153, "precisions": [0.007643312101910828, 0.002548582351067219, 0.0019120458891013384, 0.001275103602167676], "brevity_penalty": 1.0, "length_ratio": 39.25, "translation_length": 3140, "reference_length": 80}
{"bleu": 0.0, "precisions": [0.2, 0.0, 0.0, 0.0], "brevity_penalty": 0.0003707435404590882, "length_ratio": 0.11235955056179775, "translation_length": 10, "reference_length": 89}
{"bleu": 0.6858544034878273, "precisions": [0.9988095238095238, 0.9976162097735399, 0.9976133651551312, 0.997610513739546], "brevity_penalty": 0.6872892787909722, "length_ratio": 0.7272727272727273, "translation_length": 840, "reference_length": 1155}
{"bleu": 2.556627461081838e-12, "precisions": [0.9076923076923077, 0.84375, 0.8412698412698413, 0.8387096774193549], "brevity_penalty": 2.9818967086621128e-12, "length_ratio": 0.036312849162011177, "translation_length": 65, "reference_length": 1790}
{"bleu": 0.02975893516960933, "precisions": [0.03902439024390244, 0.030525030525030524, 0.02689486552567237, 0.02447980416156671], "brevity_penalty": 1.0, "length_ratio": 19.069767441860463, "translation_length": 820, "reference_length": 43}
{"bleu": 0.0, "precisions": [0.0, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 4.142857142857143, "translation_length": 29, "reference_length": 7}
{"bleu": 0.0, "precisions": [0.05128205128205128, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 9.176470588235293, "translation_length": 936, "reference_length": 102}
{"bleu": 0.0, "precisions": [0.009940357852882704, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 21.869565217391305, "translation_length": 1006, "reference_length": 46}
{"error": "Unsloth: input length 144544 + max_new_tokens 1024 exceeds the maximum sequence length of 131072!\nYou will need to do long context extension by increasing the `max_seq_length` in `FastLanguageModel.from_pretrained`."}
{"bleu": 1.6633849051443236e-06, "precisions": [0.973651191969887, 0.8969849246231156, 0.8477987421383648, 0.7997481108312342], "brevity_penalty": 1.89619991327123e-06, "length_ratio": 0.07054345901929546, "translation_length": 797, "reference_length": 11298}
{"bleu": 0.0, "precisions": [0.004025764895330112, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 69.0, "translation_length": 1242, "reference_length": 18}
{"bleu": 0.03432946070925595, "precisions": [0.0888468809073724, 0.030303030303030304, 0.024667931688804556, 0.02091254752851711], "brevity_penalty": 1.0, "length_ratio": 3.9477611940298507, "translation_length": 529, "reference_length": 134}
{"bleu": 0.08777588842406146, "precisions": [0.5629268292682926, 0.2763671875, 0.2590420332355816, 0.24266144814090018], "brevity_penalty": 0.27912444394062585, "length_ratio": 0.43934847835405055, "translation_length": 1025, "reference_length": 2333}
{"bleu": 0.017096635068622097, "precisions": [0.01857282502443793, 0.01761252446183953, 0.01665034280117532, 0.01568627450980392], "brevity_penalty": 1.0, "length_ratio": 39.34615384615385, "translation_length": 1023, "reference_length": 26}
{"bleu": 0.19631283618171633, "precisions": [0.3874709976798144, 0.23255813953488372, 0.1958041958041958, 0.17523364485981308], "brevity_penalty": 0.8325228054464412, "length_ratio": 0.8450980392156863, "translation_length": 431, "reference_length": 510}
{"bleu": 0.06508109895471781, "precisions": [0.3082039911308204, 0.1453940066592675, 0.1111111111111111, 0.0778642936596218], "brevity_penalty": 0.46380425363777744, "length_ratio": 0.5655172413793104, "translation_length": 902, "reference_length": 1595}
{"bleu": 0.0, "precisions": [0.1, 0.0, 0.0, 0.0], "brevity_penalty": 0.7046880897187133, "length_ratio": 0.7407407407407407, "translation_length": 20, "reference_length": 27}
{"bleu": 0.009301790219252534, "precisions": [0.012195121951219513, 0.009768009768009768, 0.008557457212713936, 0.0073439412484700125], "brevity_penalty": 1.0, "length_ratio": 45.55555555555556, "translation_length": 820, "reference_length": 18}
{"bleu": 0.0, "precisions": [0.1836734693877551, 0.00684931506849315, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 1.2352941176470589, "translation_length": 147, "reference_length": 119}
{"bleu": 0.9586374686102386, "precisions": [0.9595505617977528, 0.9583802024746907, 0.9583333333333334, 0.9582863585118376], "brevity_penalty": 1.0, "length_ratio": 1.0348837209302326, "translation_length": 890, "reference_length": 860}
{"bleu": 0.005114262266982457, "precisions": [0.01486988847583643, 0.004962779156327543, 0.0037267080745341614, 0.0024875621890547263], "brevity_penalty": 1.0, "length_ratio": 6.897435897435898, "translation_length": 807, "reference_length": 117}
{"bleu": 0.006672586034652837, "precisions": [0.43333333333333335, 0.023952095808383235, 0.012842465753424657, 0.011139674378748929], "brevity_penalty": 0.19114922854669986, "length_ratio": 0.37669027688345136, "translation_length": 1170, "reference_length": 3106}
{"bleu": 0.0, "precisions": [0.05, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 1.1111111111111112, "translation_length": 20, "reference_length": 18}
{"bleu": 0.3744346206891329, "precisions": [0.3835294117647059, 0.3745583038869258, 0.3714622641509434, 0.36835891381345925], "brevity_penalty": 1.0, "length_ratio": 2.119700748129676, "translation_length": 850, "reference_length": 401}
{"bleu": 0.0, "precisions": [0.004296455424274973, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 28.21212121212121, "translation_length": 931, "reference_length": 33}
{"bleu": 0.0, "precisions": [0.07692307692307693, 0.0, 0.0, 0.0], "brevity_penalty": 0.1160370022477601, "length_ratio": 0.3170731707317073, "translation_length": 13, "reference_length": 41}
{"bleu": 0.0, "precisions": [0.0273972602739726, 0.0031645569620253164, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 6.8768115942028984, "translation_length": 949, "reference_length": 138}
{"bleu": 0.03912589187502022, "precisions": [0.078125, 0.03937007874015748, 0.031746031746031744, 0.024], "brevity_penalty": 1.0, "length_ratio": 2.782608695652174, "translation_length": 128, "reference_length": 46}
{"bleu": 0.0, "precisions": [0.0, 0.0, 0.0, 0.0], "brevity_penalty": 0.17377394345044514, "length_ratio": 0.36363636363636365, "translation_length": 8, "reference_length": 22}
{"bleu": 0.0, "precisions": [0.058823529411764705, 0.0, 0.0, 0.0], "brevity_penalty": 0.27413964557012743, "length_ratio": 0.4358974358974359, "translation_length": 17, "reference_length": 39}
{"bleu": 0.0, "precisions": [0.0029069767441860465, 0.0, 0.0, 0.0], "brevity_penalty": 0.002841645216305768, "length_ratio": 0.14570097416349004, "translation_length": 688, "reference_length": 4722}
{"bleu": 0.0, "precisions": [0.11538461538461539, 0.0, 0.0, 0.0], "brevity_penalty": 0.22313016014842982, "length_ratio": 0.4, "translation_length": 26, "reference_length": 65}
{"bleu": 0.0, "precisions": [0.06451612903225806, 0.0, 0.0, 0.0], "brevity_penalty": 0.004153221554304382, "length_ratio": 0.15422885572139303, "translation_length": 31, "reference_length": 201}
{"bleu": 0.0, "precisions": [0.15447154471544716, 0.014466546112115732, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 1.0938735177865613, "translation_length": 1107, "reference_length": 1012}
{"bleu": 0.4405233517325766, "precisions": [0.7258064516129032, 0.5901639344262295, 0.5, 0.4067796610169492], "brevity_penalty": 0.8108457669377431, "length_ratio": 0.8266666666666667, "translation_length": 62, "reference_length": 75}
{"bleu": 0.0, "precisions": [0.29508196721311475, 0.03333333333333333, 0.01694915254237288, 0.0], "brevity_penalty": 0.41943258927433064, "length_ratio": 0.5350877192982456, "translation_length": 61, "reference_length": 114}
{"bleu": 0.0, "precisions": [0.05257393209200438, 0.0021929824561403508, 0.0010976948408342481, 0.0], "brevity_penalty": 1.0, "length_ratio": 4.022026431718062, "translation_length": 913, "reference_length": 227}
{"bleu": 0.0, "precisions": [0.3333333333333333, 0.0, 0.0, 0.0], "brevity_penalty": 7.912794612036177e-05, "length_ratio": 0.09574468085106383, "translation_length": 18, "reference_length": 188}
{"bleu": 0.0, "precisions": [0.07692307692307693, 0.0, 0.0, 0.0], "brevity_penalty": 0.1988141887380742, "length_ratio": 0.38235294117647056, "translation_length": 13, "reference_length": 34}
{"bleu": 0.005054752105574485, "precisions": [0.012416427889207259, 0.004780114722753346, 0.003827751196172249, 0.0028735632183908046], "brevity_penalty": 1.0, "length_ratio": 22.27659574468085, "translation_length": 1047, "reference_length": 47}
{"bleu": 0.0, "precisions": [0.18518518518518517, 0.0, 0.0, 0.0], "brevity_penalty": 0.6904785504771092, "length_ratio": 0.7297297297297297, "translation_length": 27, "reference_length": 37}
{"bleu": 0.0, "precisions": [0.038461538461538464, 0.0, 0.0, 0.0], "brevity_penalty": 0.825052966980536, "length_ratio": 0.8387096774193549, "translation_length": 26, "reference_length": 31}
{"bleu": 0.07578561550553055, "precisions": [0.21445221445221446, 0.07817969661610269, 0.056074766355140186, 0.03508771929824561], "brevity_penalty": 1.0, "length_ratio": 2.2, "translation_length": 858, "reference_length": 390}
{"bleu": 0.17473420943935888, "precisions": [0.1762545899632803, 0.17524509803921567, 0.17423312883435582, 0.1732186732186732], "brevity_penalty": 1.0, "length_ratio": 5.410596026490066, "translation_length": 817, "reference_length": 151}
{"bleu": 0.0, "precisions": [0.0, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 1.8571428571428572, "translation_length": 13, "reference_length": 7}
{"bleu": 0.0, "precisions": [0.05970149253731343, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 7.106060606060606, "translation_length": 938, "reference_length": 132}
{"bleu": 1.4684812220211325e-12, "precisions": [0.42671394799054374, 0.06153846153846154, 0.0509478672985782, 0.03914590747330961], "brevity_penalty": 1.7262132088735545e-11, "length_ratio": 0.03878598936365304, "translation_length": 846, "reference_length": 21812}
{"bleu": 0.0, "precisions": [0.15384615384615385, 0.0, 0.0, 0.0], "brevity_penalty": 0.0016615572731739354, "length_ratio": 0.13513513513513514, "translation_length": 65, "reference_length": 481}
{"bleu": 0.0, "precisions": [0.001927457508323112, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 61.365591397849464, "translation_length": 5707, "reference_length": 93}
{"bleu": 0.2697982653557989, "precisions": [0.3484848484848485, 0.26903553299492383, 0.24489795918367346, 0.23076923076923078], "brevity_penalty": 1.0, "length_ratio": 1.2531645569620253, "translation_length": 198, "reference_length": 158}
{"bleu": 0.14324050793586465, "precisions": [0.3040488922841864, 0.13073394495412843, 0.10558530986993114, 0.1003062787136294], "brevity_penalty": 1.0, "length_ratio": 1.4014989293361884, "translation_length": 1309, "reference_length": 934}
{"bleu": 0.5161589181818746, "precisions": [0.9488636363636364, 0.9314285714285714, 0.9195402298850575, 0.9075144508670521], "brevity_penalty": 0.5569792612305547, "length_ratio": 0.6308243727598566, "translation_length": 176, "reference_length": 279}
{"bleu": 0.0, "precisions": [0.013888888888888888, 0.0012642225031605564, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 17.217391304347824, "translation_length": 792, "reference_length": 46}
{"bleu": 0.12543556541472844, "precisions": [0.12871287128712872, 0.12655086848635236, 0.12437810945273632, 0.12219451371571072], "brevity_penalty": 1.0, "length_ratio": 6.8474576271186445, "translation_length": 404, "reference_length": 59}
{"bleu": 0.0, "precisions": [0.000983767830791933, 0.0, 0.0, 0.0], "brevity_penalty": 0.9279602741500526, "length_ratio": 0.9304347826086956, "translation_length": 2033, "reference_length": 2185}
{"bleu": 0.0, "precisions": [0.002178649237472767, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 41.72727272727273, "translation_length": 918, "reference_length": 22}
{"bleu": 0.0, "precisions": [0.3, 0.0, 0.0, 0.0], "brevity_penalty": 5.459109571503346e-115, "length_ratio": 0.003786444528587656, "translation_length": 10, "reference_length": 2641}
{"bleu": 0.37563555880944566, "precisions": [0.3766968325791855, 0.37599093997734995, 0.37528344671201813, 0.3745743473325766], "brevity_penalty": 1.0, "length_ratio": 2.6, "translation_length": 884, "reference_length": 340}
{"bleu": 0.0, "precisions": [0.09090909090909091, 0.0, 0.0, 0.0], "brevity_penalty": 0.0014366309200863887, "length_ratio": 0.13253012048192772, "translation_length": 33, "reference_length": 249}
{"bleu": 0.0, "precisions": [0.2, 0.0, 0.0, 0.0], "brevity_penalty": 0.11080315836233387, "length_ratio": 0.3125, "translation_length": 15, "reference_length": 48}
{"bleu": 0.0, "precisions": [0.07257203842049093, 0.011752136752136752, 0.0053475935828877, 0.0], "brevity_penalty": 1.0, "length_ratio": 5.092391304347826, "translation_length": 937, "reference_length": 184}
{"bleu": 0.7072839848104847, "precisions": [0.7551219512195122, 0.703125, 0.6911045943304008, 0.6819960861056752], "brevity_penalty": 1.0, "length_ratio": 1.1129207383279045, "translation_length": 1025, "reference_length": 921}
{"bleu": 0.0, "precisions": [0.2727272727272727, 0.0, 0.0, 0.0], "brevity_penalty": 0.0008326400759127053, "length_ratio": 0.12359550561797752, "translation_length": 11, "reference_length": 89}
{"bleu": 6.48125600249167e-31, "precisions": [0.949748743718593, 0.9242424242424242, 0.8984771573604061, 0.8877551020408163], "brevity_penalty": 7.085336240178284e-31, "length_ratio": 0.014200085628657056, "translation_length": 199, "reference_length": 14014}
{"bleu": 0.0, "precisions": [0.06666666666666667, 0.0, 0.0, 0.0], "brevity_penalty": 0.6703200460356393, "length_ratio": 0.7142857142857143, "translation_length": 15, "reference_length": 21}
{"bleu": 7.129233301173925e-05, "precisions": [0.05058365758754864, 0.025341130604288498, 0.01953125, 0.009784735812133072], "brevity_penalty": 0.0032045273843044017, "length_ratio": 0.14829774956722447, "translation_length": 514, "reference_length": 3466}
{"bleu": 0.0, "precisions": [0.04673721340388007, 0.00176522506619594, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 6.096774193548387, "translation_length": 1134, "reference_length": 186}
{"bleu": 0.0, "precisions": [0.2, 0.0, 0.0, 0.0], "brevity_penalty": 5.05653134833552e-08, "length_ratio": 0.056179775280898875, "translation_length": 5, "reference_length": 89}
{"bleu": 0.0, "precisions": [0.047619047619047616, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 1.0, "translation_length": 21, "reference_length": 21}
{"bleu": 0.0, "precisions": [0.3327974276527331, 0.0, 0.0, 0.0], "brevity_penalty": 0.0012297251003524732, "length_ratio": 0.12985386221294362, "translation_length": 622, "reference_length": 4790}
{"bleu": 0.0, "precisions": [0.0, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 1.0, "translation_length": 13, "reference_length": 13}
{"bleu": 0.0, "precisions": [0.15384615384615385, 0.0, 0.0, 0.0], "brevity_penalty": 1.0209603615263827e-11, "length_ratio": 0.038011695906432746, "translation_length": 13, "reference_length": 342}
{"bleu": 0.30229264040392706, "precisions": [0.5367965367965368, 0.3901734104046243, 0.3256150506512301, 0.26231884057971017], "brevity_penalty": 0.8265654376242381, "length_ratio": 0.84, "translation_length": 693, "reference_length": 825}
{"bleu": 0.0, "precisions": [0.0, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 1.6522801302931596, "translation_length": 2029, "reference_length": 1228}
{"bleu": 0.0, "precisions": [0.012658227848101266, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 21.066666666666666, "translation_length": 948, "reference_length": 45}
{"bleu": 0.0, "precisions": [0.0, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 18.136363636363637, "translation_length": 1995, "reference_length": 110}
{"bleu": 0.0, "precisions": [0.25, 0.0, 0.0, 0.0], "brevity_penalty": 3.726653172078671e-06, "length_ratio": 0.07407407407407407, "translation_length": 8, "reference_length": 108}
{"bleu": 0.0, "precisions": [0.0019455252918287938, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 4.145161290322581, "translation_length": 514, "reference_length": 124}
{"bleu": 0.003864681752164025, "precisions": [0.4868421052631579, 0.38666666666666666, 0.36486486486486486, 0.3424657534246575], "brevity_penalty": 0.009868365395945105, "length_ratio": 0.17798594847775176, "translation_length": 76, "reference_length": 427}
{"bleu": 0.0910939708768429, "precisions": [0.3706111833550065, 0.08984375, 0.05867014341590613, 0.03524804177545692], "brevity_penalty": 1.0, "length_ratio": 1.0970042796005706, "translation_length": 769, "reference_length": 701}
{"error": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"}
{"bleu": 0.0, "precisions": [0.05183585313174946, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 7.291338582677166, "translation_length": 926, "reference_length": 127}
{"bleu": 0.0, "precisions": [0.3181818181818182, 0.0, 0.0, 0.0], "brevity_penalty": 9.249795720916071e-06, "length_ratio": 0.07942238267148015, "translation_length": 22, "reference_length": 277}
{"bleu": 0.0, "precisions": [0.0, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 19.523809523809526, "translation_length": 2050, "reference_length": 105}
{"bleu": 0.21780195099221325, "precisions": [0.22007722007722008, 0.218568665377176, 0.21705426356589147, 0.21553398058252426], "brevity_penalty": 1.0, "length_ratio": 4.2809917355371905, "translation_length": 518, "reference_length": 121}
{"bleu": 0.0, "precisions": [0.15, 0.0, 0.0, 0.0], "brevity_penalty": 0.2865047968601901, "length_ratio": 0.4444444444444444, "translation_length": 20, "reference_length": 45}
{"bleu": 0.3161188859985317, "precisions": [0.36661698956780925, 0.30895522388059704, 0.29895366218236175, 0.2949101796407186], "brevity_penalty": 1.0, "length_ratio": 1.5112612612612613, "translation_length": 671, "reference_length": 444}
{"error": "CUDA out of memory. Tried to allocate 218.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 136.69 MiB is free. Including non-PyTorch memory, this process has 10.61 GiB memory in use. Of the allocated memory 10.27 GiB is allocated by PyTorch, and 131.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"error": "CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 36.69 MiB is free. Including non-PyTorch memory, this process has 10.71 GiB memory in use. Of the allocated memory 10.40 GiB is allocated by PyTorch, and 99.86 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"bleu": 0.0, "precisions": [0.05263157894736842, 0.0, 0.0, 0.0], "brevity_penalty": 0.005753800207388146, "length_ratio": 0.1623931623931624, "translation_length": 19, "reference_length": 117}
{"bleu": 0.23710642410780072, "precisions": [0.9970472440944882, 0.9950738916256158, 0.9940828402366864, 0.9930898321816387], "brevity_penalty": 0.23834046123821676, "length_ratio": 0.4108370400323494, "translation_length": 1016, "reference_length": 2473}
{"bleu": 0.0, "precisions": [0.010815307820299502, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 27.953488372093023, "translation_length": 1202, "reference_length": 43}
{"bleu": 0.0, "precisions": [0.15789473684210525, 0.0, 0.0, 0.0], "brevity_penalty": 0.008766285528368277, "length_ratio": 0.1743119266055046, "translation_length": 19, "reference_length": 109}
{"bleu": 1.6388109718151197e-05, "precisions": [0.9711729622266402, 0.8766169154228856, 0.8296812749003984, 0.7826520438683948], "brevity_penalty": 1.9005629220554216e-05, "length_ratio": 0.08424049572935857, "translation_length": 1006, "reference_length": 11942}
{"bleu": 0.023509576662750265, "precisions": [0.05942857142857143, 0.021739130434782608, 0.01718213058419244, 0.013761467889908258], "brevity_penalty": 1.0, "length_ratio": 5.645161290322581, "translation_length": 875, "reference_length": 155}
{"bleu": 0.0037066488629248413, "precisions": [0.010721944245889922, 0.004291845493562232, 0.002863278453829635, 0.0014326647564469914], "brevity_penalty": 1.0, "length_ratio": 24.54385964912281, "translation_length": 1399, "reference_length": 57}
{"bleu": 0.0, "precisions": [0.14285714285714285, 0.0, 0.0, 0.0], "brevity_penalty": 0.03243324089479551, "length_ratio": 0.22580645161290322, "translation_length": 21, "reference_length": 93}
{"bleu": 0.12378586480311624, "precisions": [0.4625935162094763, 0.2833957553058677, 0.23, 0.20525657071339173], "brevity_penalty": 0.4413332092894863, "length_ratio": 0.5500685871056241, "translation_length": 802, "reference_length": 1458}
{"bleu": 0.02319083975912523, "precisions": [0.17575757575757575, 0.06875631951466127, 0.06477732793522267, 0.060790273556231005], "brevity_penalty": 0.2792193582593762, "length_ratio": 0.4394141145139814, "translation_length": 990, "reference_length": 2253}
{"bleu": 0.3805335055352986, "precisions": [0.7615894039735099, 0.5663129973474801, 0.5258964143426295, 0.4853723404255319], "brevity_penalty": 0.6606245943326371, "length_ratio": 0.7069288389513109, "translation_length": 755, "reference_length": 1068}
{"bleu": 0.0, "precisions": [0.05364511691884457, 0.011019283746556474, 0.0, 0.0], "brevity_penalty": 0.002579628090868572, "length_ratio": 0.14367588932806324, "translation_length": 727, "reference_length": 5060}
{"bleu": 2.102272535042146e-05, "precisions": [0.996822033898305, 0.9936373276776246, 0.9915074309978769, 0.9893730074388948], "brevity_penalty": 2.1174522573067607e-05, "length_ratio": 0.08501440922190202, "translation_length": 944, "reference_length": 11104}
{"bleu": 0.0, "precisions": [0.022988505747126436, 0.009868421052631578, 0.004942339373970346, 0.0], "brevity_penalty": 1.0, "length_ratio": 14.853658536585366, "translation_length": 609, "reference_length": 41}
{"bleu": 0.0, "precisions": [0.011916583912611719, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 23.976190476190474, "translation_length": 1007, "reference_length": 42}
{"bleu": 0.02762004556384654, "precisions": [0.0642939150401837, 0.02528735632183908, 0.020713463751438434, 0.01728110599078341], "brevity_penalty": 1.0, "length_ratio": 6.311594202898551, "translation_length": 871, "reference_length": 138}
{"bleu": 0.0, "precisions": [0.602803738317757, 0.0011695906432748538, 0.0, 0.0], "brevity_penalty": 2.683775388877991e-05, "length_ratio": 0.08676261909588485, "translation_length": 856, "reference_length": 9866}
{"bleu": 0.0, "precisions": [0.30472103004291845, 0.0, 0.0, 0.0], "brevity_penalty": 0.6898751531725069, "length_ratio": 0.729264475743349, "translation_length": 932, "reference_length": 1278}
{"error": "CUDA out of memory. Tried to allocate 296.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 270.69 MiB is free. Including non-PyTorch memory, this process has 10.48 GiB memory in use. Of the allocated memory 10.00 GiB is allocated by PyTorch, and 275.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"error": "CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 30.69 MiB is free. Including non-PyTorch memory, this process has 10.71 GiB memory in use. Of the allocated memory 10.29 GiB is allocated by PyTorch, and 219.83 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"error": "CUDA out of memory. Tried to allocate 250.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 234.69 MiB is free. Including non-PyTorch memory, this process has 10.51 GiB memory in use. Of the allocated memory 10.02 GiB is allocated by PyTorch, and 289.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"error": "CUDA out of memory. Tried to allocate 34.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 34.69 MiB is free. Including non-PyTorch memory, this process has 10.71 GiB memory in use. Of the allocated memory 10.21 GiB is allocated by PyTorch, and 302.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"bleu": 0.03003505254184306, "precisions": [0.05520833333333333, 0.03128258602711158, 0.025052192066805846, 0.018808777429467086], "brevity_penalty": 1.0, "length_ratio": 10.786516853932584, "translation_length": 960, "reference_length": 89}
{"bleu": 0.0, "precisions": [0.15384615384615385, 0.0, 0.0, 0.0], "brevity_penalty": 1.3259858681472e-05, "length_ratio": 0.08176100628930817, "translation_length": 13, "reference_length": 159}
{"bleu": 0.09239447333792435, "precisions": [0.12762237762237763, 0.08756567425569177, 0.0824561403508772, 0.07908611599297012], "brevity_penalty": 1.0, "length_ratio": 3.575, "translation_length": 572, "reference_length": 160}
{"bleu": 0.0, "precisions": [0.23529411764705882, 0.0, 0.0, 0.0], "brevity_penalty": 4.3189744300836694e-13, "length_ratio": 0.033932135728542916, "translation_length": 17, "reference_length": 501}
{"bleu": 0.0008294797506577631, "precisions": [0.75, 0.6857142857142857, 0.6470588235294118, 0.6060606060606061], "brevity_penalty": 0.0012377693329126938, "length_ratio": 0.1299638989169675, "translation_length": 36, "reference_length": 277}
{"bleu": 0.0, "precisions": [0.003499562554680665, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 63.5, "translation_length": 1143, "reference_length": 18}
{"bleu": 0.0, "precisions": [0.18181818181818182, 0.0, 0.0, 0.0], "brevity_penalty": 0.3849870989234837, "length_ratio": 0.5116279069767442, "translation_length": 22, "reference_length": 43}
{"bleu": 0.0, "precisions": [0.0037285607755406414, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 74.5, "translation_length": 1341, "reference_length": 18}
{"bleu": 0.06737746795861518, "precisions": [0.0784741144414169, 0.0697928026172301, 0.06382978723404255, 0.05895196506550218], "brevity_penalty": 1.0, "length_ratio": 3.1529209621993126, "translation_length": 1835, "reference_length": 582}
{"bleu": 3.0930610029701715e-57, "precisions": [0.8333333333333334, 0.7142857142857143, 0.6764705882352942, 0.6363636363636364], "brevity_penalty": 4.3473763398885453e-57, "length_ratio": 0.0076465590484282074, "translation_length": 36, "reference_length": 4708}
{"bleu": 0.0, "precisions": [0.0024962556165751375, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 69.06896551724138, "translation_length": 2003, "reference_length": 29}
{"bleu": 0.0, "precisions": [0.15789473684210525, 0.0, 0.0, 0.0], "brevity_penalty": 0.5907775139012316, "length_ratio": 0.6551724137931034, "translation_length": 19, "reference_length": 29}
{"bleu": 0.2918505938238597, "precisions": [0.400679117147708, 0.304421768707483, 0.26405451448040884, 0.22525597269624573], "brevity_penalty": 1.0, "length_ratio": 1.5259067357512954, "translation_length": 589, "reference_length": 386}
{"bleu": 0.0, "precisions": [0.031598513011152414, 0.0037209302325581397, 0.00186219739292365, 0.0], "brevity_penalty": 1.0, "length_ratio": 14.346666666666666, "translation_length": 1076, "reference_length": 75}
{"bleu": 0.0, "precisions": [0.0, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 1.2352941176470589, "translation_length": 21, "reference_length": 17}
{"bleu": 0.0, "precisions": [0.1891891891891892, 0.0, 0.0, 0.0], "brevity_penalty": 0.5088125121973882, "length_ratio": 0.5967741935483871, "translation_length": 37, "reference_length": 62}
{"bleu": 0.0, "precisions": [0.12258064516129032, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 1.9135802469135803, "translation_length": 155, "reference_length": 81}
{"bleu": 0.0, "precisions": [0.0625, 0.0, 0.0, 0.0], "brevity_penalty": 0.2096113871510978, "length_ratio": 0.3902439024390244, "translation_length": 16, "reference_length": 41}
{"bleu": 0.5161589181818746, "precisions": [0.9488636363636364, 0.9314285714285714, 0.9195402298850575, 0.9075144508670521], "brevity_penalty": 0.5569792612305547, "length_ratio": 0.6308243727598566, "translation_length": 176, "reference_length": 279}
{"bleu": 0.09773805512913322, "precisions": [0.1406926406926407, 0.10725893824485373, 0.08568329718004339, 0.07057546145494029], "brevity_penalty": 1.0, "length_ratio": 5.848101265822785, "translation_length": 924, "reference_length": 158}
{"bleu": 0.0, "precisions": [0.06666666666666667, 0.0, 0.0, 0.0], "brevity_penalty": 0.6703200460356393, "length_ratio": 0.7142857142857143, "translation_length": 15, "reference_length": 21}
{"bleu": 0.0, "precisions": [0.15384615384615385, 0.0, 0.0, 0.0], "brevity_penalty": 0.32361358035074045, "length_ratio": 0.46987951807228917, "translation_length": 39, "reference_length": 83}
{"bleu": 0.07189063174435464, "precisions": [0.09933774834437085, 0.06778797145769623, 0.06425293217746048, 0.06173469387755102], "brevity_penalty": 1.0, "length_ratio": 1.1761533852606352, "translation_length": 1963, "reference_length": 1669}
{"bleu": 0.0, "precisions": [0.0004878048780487805, 0.0, 0.0, 0.0], "brevity_penalty": 0.021109147946541895, "length_ratio": 0.20584396023697157, "translation_length": 2050, "reference_length": 9959}
{"bleu": 0.005463158484414075, "precisions": [0.813953488372093, 0.7421875, 0.7086614173228346, 0.6746031746031746], "brevity_penalty": 0.007452357853939362, "length_ratio": 0.16951379763469118, "translation_length": 129, "reference_length": 761}
{"error": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"}
{"bleu": 0.08606370696387736, "precisions": [0.10377358490566038, 0.08530805687203792, 0.08095238095238096, 0.07655502392344497], "brevity_penalty": 1.0, "length_ratio": 5.72972972972973, "translation_length": 212, "reference_length": 37}
{"bleu": 0.0, "precisions": [0.045454545454545456, 0.0, 0.0, 0.0], "brevity_penalty": 0.6642538428642955, "length_ratio": 0.7096774193548387, "translation_length": 22, "reference_length": 31}
{"bleu": 3.246640816431977e-06, "precisions": [0.9579375848032564, 0.9144021739130435, 0.8734693877551021, 0.832425068119891], "brevity_penalty": 3.6342719208890118e-06, "length_ratio": 0.07393659711075441, "translation_length": 737, "reference_length": 9968}
{"bleu": 0.0, "precisions": [0.003918495297805642, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 70.88888888888889, "translation_length": 1276, "reference_length": 18}
{"bleu": 0.0, "precisions": [0.21428571428571427, 0.0, 0.0, 0.0], "brevity_penalty": 4.5399929762484854e-05, "length_ratio": 0.09090909090909091, "translation_length": 14, "reference_length": 154}
{"bleu": 0.0166419543478683, "precisions": [0.018993839835728953, 0.016435541859270673, 0.01593011305241521, 0.015424164524421594], "brevity_penalty": 1.0, "length_ratio": 7.521235521235521, "translation_length": 1948, "reference_length": 259}
{"bleu": 0.06922359434777223, "precisions": [0.15498154981549817, 0.08494921514312095, 0.05545286506469501, 0.03145235892691952], "brevity_penalty": 1.0, "length_ratio": 1.8187919463087248, "translation_length": 1084, "reference_length": 596}
{"bleu": 0.007609289398573618, "precisions": [0.025210084033613446, 0.008415147265077139, 0.0056179775280898875, 0.0028129395218002813], "brevity_penalty": 1.0, "length_ratio": 17.414634146341463, "translation_length": 714, "reference_length": 41}
{"bleu": 0.15954330564351757, "precisions": [0.8333333333333334, 0.8113207547169812, 0.8076923076923077, 0.803921568627451], "brevity_penalty": 0.1960021540757468, "length_ratio": 0.38028169014084506, "translation_length": 54, "reference_length": 142}
{"error": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"}
{"bleu": 0.0, "precisions": [0.004142502071251036, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 67.05555555555556, "translation_length": 1207, "reference_length": 18}
{"bleu": 0.0, "precisions": [0.4411764705882353, 0.0, 0.0, 0.0], "brevity_penalty": 8.709743203983368e-09, "length_ratio": 0.05112781954887218, "translation_length": 34, "reference_length": 665}
{"bleu": 0.20250030070396946, "precisions": [0.37823834196891193, 0.22916666666666666, 0.16753926701570682, 0.11578947368421053], "brevity_penalty": 1.0, "length_ratio": 1.8037383177570094, "translation_length": 193, "reference_length": 107}
{"bleu": 0.0, "precisions": [0.4166666666666667, 0.0, 0.0, 0.0], "brevity_penalty": 6.033249137014046e-115, "length_ratio": 0.003787878787878788, "translation_length": 12, "reference_length": 3168}
{"bleu": 0.07128947344816641, "precisions": [0.2502708559046587, 0.08134490238611713, 0.04668838219326819, 0.02717391304347826], "brevity_penalty": 1.0, "length_ratio": 1.883673469387755, "translation_length": 923, "reference_length": 490}
{"bleu": 0.0, "precisions": [0.2631578947368421, 0.0, 0.0, 0.0], "brevity_penalty": 2.611735571603125e-11, "length_ratio": 0.03941908713692946, "translation_length": 19, "reference_length": 482}
{"bleu": 0.18148810037050597, "precisions": [0.3724832214765101, 0.226890756302521, 0.1936026936026936, 0.16188870151770657], "brevity_penalty": 0.7999913511652337, "length_ratio": 0.8175582990397805, "translation_length": 596, "reference_length": 729}
{"bleu": 0.006203400695686625, "precisions": [0.038752362948960305, 0.008514664143803218, 0.004734848484848485, 0.0009478672985781991], "brevity_penalty": 1.0, "length_ratio": 7.666666666666667, "translation_length": 1058, "reference_length": 138}
{"bleu": 0.007733127372133742, "precisions": [0.06130268199233716, 0.010546500479386385, 0.005758157389635317, 0.0009606147934678194], "brevity_penalty": 1.0, "length_ratio": 6.649681528662421, "translation_length": 1044, "reference_length": 157}
{"bleu": 0.0, "precisions": [0.006355932203389831, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 16.56140350877193, "translation_length": 944, "reference_length": 57}
{"bleu": 0.0027152656289041, "precisions": [0.004093118444614991, 0.0028147389969293756, 0.002303557716918352, 0.002048131080389145], "brevity_penalty": 1.0, "length_ratio": 114.97058823529412, "translation_length": 3909, "reference_length": 34}
{"bleu": 0.01391155134775051, "precisions": [0.22341696535244923, 0.0215311004784689, 0.01437125748502994, 0.01079136690647482], "brevity_penalty": 0.47335521013775184, "length_ratio": 0.5721120984278879, "translation_length": 837, "reference_length": 1463}
{"bleu": 0.05906843063239191, "precisions": [0.6167076167076168, 0.3836065573770492, 0.34618539786710417, 0.34318555008210183], "brevity_penalty": 0.14426290193286143, "length_ratio": 0.34058577405857743, "translation_length": 1221, "reference_length": 3585}
{"bleu": 0.0009026854698712102, "precisions": [0.9220462850182704, 0.7865853658536586, 0.7081807081807082, 0.6308068459657702], "brevity_penalty": 0.0011964696406996443, "length_ratio": 0.12939322301024428, "translation_length": 821, "reference_length": 6345}
{"bleu": 0.0, "precisions": [0.038797284190106696, 0.000970873786407767, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 1.5861538461538462, "translation_length": 1031, "reference_length": 650}
{"bleu": 0.0, "precisions": [0.04407443682664055, 0.00196078431372549, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 2.4781553398058254, "translation_length": 1021, "reference_length": 412}
{"bleu": 0.11990744501374691, "precisions": [0.9311163895486936, 0.8632580261593341, 0.8226190476190476, 0.7854588796185935], "brevity_penalty": 0.1412470695694249, "length_ratio": 0.3381526104417671, "translation_length": 842, "reference_length": 2490}
{"bleu": 0.0, "precisions": [0.0, 0.0, 0.0, 0.0], "brevity_penalty": 0.06392786120670757, "length_ratio": 0.26666666666666666, "translation_length": 8, "reference_length": 30}
{"bleu": 0.0, "precisions": [0.5977011494252874, 0.1636828644501279, 0.06658130601792574, 0.0], "brevity_penalty": 0.0003566708044408626, "length_ratio": 0.11187312473210459, "translation_length": 783, "reference_length": 6999}
{"error": "CUDA out of memory. Tried to allocate 226.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 158.69 MiB is free. Including non-PyTorch memory, this process has 10.59 GiB memory in use. Of the allocated memory 10.29 GiB is allocated by PyTorch, and 95.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"error": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 18.69 MiB is free. Including non-PyTorch memory, this process has 10.72 GiB memory in use. Of the allocated memory 10.39 GiB is allocated by PyTorch, and 130.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"bleu": 0.0, "precisions": [0.0, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 3.0476190476190474, "translation_length": 512, "reference_length": 168}
{"bleu": 0.014334889916501668, "precisions": [0.04777777777777778, 0.017797552836484983, 0.011135857461024499, 0.004459308807134894], "brevity_penalty": 1.0, "length_ratio": 9.89010989010989, "translation_length": 900, "reference_length": 91}
{"bleu": 0.0, "precisions": [0.07692307692307693, 0.0, 0.0, 0.0], "brevity_penalty": 1.7320500046289415e-28, "length_ratio": 0.015402843601895734, "translation_length": 13, "reference_length": 844}
{"bleu": 0.0, "precisions": [0.25, 0.0, 0.0, 0.0], "brevity_penalty": 0.0008047330101246132, "length_ratio": 0.12307692307692308, "translation_length": 8, "reference_length": 65}
{"bleu": 0.0, "precisions": [0.15, 0.0, 0.0, 0.0], "brevity_penalty": 0.8187307530779819, "length_ratio": 0.8333333333333334, "translation_length": 20, "reference_length": 24}
{"bleu": 0.0, "precisions": [0.12, 0.0, 0.0, 0.0], "brevity_penalty": 0.3985190410845142, "length_ratio": 0.5208333333333334, "translation_length": 25, "reference_length": 48}
{"bleu": 0.01326277767136418, "precisions": [0.08959537572254335, 0.011571841851494697, 0.007722007722007722, 0.003864734299516908], "brevity_penalty": 1.0, "length_ratio": 3.0262390670553936, "translation_length": 1038, "reference_length": 343}
{"bleu": 0.7548694465000921, "precisions": [0.8782505910165485, 0.8331360946745562, 0.79739336492891, 0.763938315539739], "brevity_penalty": 0.9238586294338812, "length_ratio": 0.9266155531215772, "translation_length": 846, "reference_length": 913}
{"bleu": 0.017337587290563602, "precisions": [0.41872791519434627, 0.22281167108753316, 0.20353982300884957, 0.18511957484499558], "brevity_penalty": 0.0712025854723288, "length_ratio": 0.27455736114479745, "translation_length": 1132, "reference_length": 4123}
{"bleu": 0.0, "precisions": [0.006147540983606557, 0.0010256410256410256, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 21.68888888888889, "translation_length": 976, "reference_length": 45}
{"bleu": 0.0, "precisions": [0.3, 0.0, 0.0, 0.0], "brevity_penalty": 4.248354255291589e-18, "length_ratio": 0.024390243902439025, "translation_length": 10, "reference_length": 410}
{"bleu": 0.0, "precisions": [0.25, 0.0, 0.0, 0.0], "brevity_penalty": 7.991959892953932e-11, "length_ratio": 0.041237113402061855, "translation_length": 4, "reference_length": 97}
{"bleu": 0.0, "precisions": [0.05064935064935065, 0.0, 0.0, 0.0], "brevity_penalty": 0.2965366629971733, "length_ratio": 0.451348182883939, "translation_length": 770, "reference_length": 1706}
{"bleu": 0.0054228966888139035, "precisions": [0.9970873786407767, 0.8736637512147716, 0.8375486381322957, 0.8062317429406037], "brevity_penalty": 0.00619219271833164, "length_ratio": 0.1643529599489389, "translation_length": 1030, "reference_length": 6267}
{"bleu": 0.0, "precisions": [0.3505654281098546, 0.13592233009708737, 0.06807131280388978, 0.0], "brevity_penalty": 0.2207993722277981, "length_ratio": 0.3983268983268983, "translation_length": 619, "reference_length": 1554}
{"bleu": 0.15041900646168094, "precisions": [0.16094674556213018, 0.1504739336492891, 0.14709371293001186, 0.14370546318289787], "brevity_penalty": 1.0, "length_ratio": 6.1231884057971016, "translation_length": 845, "reference_length": 138}
{"bleu": 0.0, "precisions": [0.3413173652694611, 0.018072289156626505, 0.006060606060606061, 0.0], "brevity_penalty": 0.4222002932036978, "length_ratio": 0.5369774919614148, "translation_length": 167, "reference_length": 311}
{"bleu": 1.5414633373325209e-06, "precisions": [0.9779735682819384, 0.9349503858875413, 0.9083885209713024, 0.8861878453038674], "brevity_penalty": 1.6641977193802572e-06, "length_ratio": 0.06989992301770592, "translation_length": 908, "reference_length": 12990}
{"bleu": 0.0, "precisions": [0.2, 0.0, 0.0, 0.0], "brevity_penalty": 0.016572675401761255, "length_ratio": 0.19607843137254902, "translation_length": 10, "reference_length": 51}
{"bleu": 0.0, "precisions": [0.06666666666666667, 0.0, 0.0, 0.0], "brevity_penalty": 0.6703200460356393, "length_ratio": 0.7142857142857143, "translation_length": 15, "reference_length": 21}
{"bleu": 0.0, "precisions": [0.16989737742303307, 0.0, 0.0, 0.0], "brevity_penalty": 0.00021235724301562318, "length_ratio": 0.10573908849770919, "translation_length": 877, "reference_length": 8294}
{"error": "CUDA out of memory. Tried to allocate 210.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 156.69 MiB is free. Including non-PyTorch memory, this process has 10.59 GiB memory in use. Of the allocated memory 10.16 GiB is allocated by PyTorch, and 229.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"bleu": 0.17085611396479522, "precisions": [0.1750841750841751, 0.17229729729729729, 0.1694915254237288, 0.16666666666666666], "brevity_penalty": 1.0, "length_ratio": 5.033898305084746, "translation_length": 297, "reference_length": 59}
{"bleu": 0.004703829607552013, "precisions": [0.40611620795107034, 0.13769889840881272, 0.1316595223515003, 0.12806372549019607], "brevity_penalty": 0.026843354238518483, "length_ratio": 0.2165562913907285, "translation_length": 1635, "reference_length": 7550}
{"bleu": 0.0, "precisions": [0.0003255208333333333, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 2.3063063063063063, "translation_length": 3072, "reference_length": 1332}
{"bleu": 0.0, "precisions": [0.18487394957983194, 0.025423728813559324, 0.008547008547008548, 0.0], "brevity_penalty": 0.9833337223669275, "length_ratio": 0.9834710743801653, "translation_length": 119, "reference_length": 121}
{"bleu": 0.0, "precisions": [0.4309500489715965, 0.00196078431372549, 0.0, 0.0], "brevity_penalty": 0.0004522509823697183, "length_ratio": 0.11492570914002702, "translation_length": 1021, "reference_length": 8884}
{"bleu": 0.03963449710047971, "precisions": [0.2601726263871763, 0.03827160493827161, 0.022249690976514216, 0.011138613861386138], "brevity_penalty": 1.0, "length_ratio": 1.4353982300884955, "translation_length": 811, "reference_length": 565}
{"bleu": 0.3834567864001215, "precisions": [0.384526558891455, 0.3838150289017341, 0.38310185185185186, 0.3823870220162225], "brevity_penalty": 1.0, "length_ratio": 2.5470588235294116, "translation_length": 866, "reference_length": 340}
{"bleu": 5.968407873690272e-05, "precisions": [0.5852842809364549, 0.2897822445561139, 0.21644295302013422, 0.20168067226890757], "brevity_penalty": 0.00020346836901064417, "length_ratio": 0.10526315789473684, "translation_length": 598, "reference_length": 5681}
{"bleu": 0.0, "precisions": [0.0199468085106383, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 13.192982456140351, "translation_length": 752, "reference_length": 57}
{"bleu": 0.0011781186182756773, "precisions": [0.896551724137931, 0.8245614035087719, 0.8214285714285714, 0.8181818181818182], "brevity_penalty": 0.0014032482977192804, "length_ratio": 0.13211845102505695, "translation_length": 58, "reference_length": 439}
{"bleu": 0.0, "precisions": [0.019715224534501644, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 7.939130434782609, "translation_length": 913, "reference_length": 115}
{"bleu": 0.0, "precisions": [0.125, 0.0, 0.0, 0.0], "brevity_penalty": 0.0011708796207911744, "length_ratio": 0.12903225806451613, "translation_length": 8, "reference_length": 62}
{"bleu": 0.33815561453031523, "precisions": [0.5085995085995086, 0.4440344403444034, 0.4408866995073892, 0.43773119605425403], "brevity_penalty": 0.7400905014665101, "length_ratio": 0.7686496694995278, "translation_length": 814, "reference_length": 1059}
{"bleu": 0.0, "precisions": [0.31633986928104574, 0.017015706806282723, 0.0, 0.0], "brevity_penalty": 0.30396288519530734, "length_ratio": 0.4564439140811456, "translation_length": 765, "reference_length": 1676}
{"bleu": 9.056390276991312e-69, "precisions": [0.5714285714285714, 0.25925925925925924, 0.23076923076923078, 0.2], "brevity_penalty": 3.149409248288149e-68, "length_ratio": 0.006392694063926941, "translation_length": 28, "reference_length": 4380}
{"bleu": 0.0, "precisions": [0.375, 0.0, 0.0, 0.0], "brevity_penalty": 4.161262970056734e-39, "length_ratio": 0.011188811188811189, "translation_length": 8, "reference_length": 715}
{"bleu": 0.0, "precisions": [0.391304347826087, 0.045454545454545456, 0.0, 0.0], "brevity_penalty": 2.7829127483359325e-36, "length_ratio": 0.012067156348373556, "translation_length": 23, "reference_length": 1906}
{"bleu": 0.0030497436138698653, "precisions": [0.9027777777777778, 0.8450704225352113, 0.8428571428571429, 0.8405797101449275], "brevity_penalty": 0.003556818231724972, "length_ratio": 0.1506276150627615, "translation_length": 72, "reference_length": 478}
{"bleu": 0.3107348764250831, "precisions": [0.8528183716075156, 0.7345872518286312, 0.7165271966527197, 0.7015706806282722], "brevity_penalty": 0.41480023074509503, "length_ratio": 0.5319267073847862, "translation_length": 958, "reference_length": 1801}
{"bleu": 0.0, "precisions": [0.2857142857142857, 0.0, 0.0, 0.0], "brevity_penalty": 1.6507419337718225e-06, "length_ratio": 0.06986027944111776, "translation_length": 35, "reference_length": 501}
{"bleu": 0.08680257318401176, "precisions": [0.991304347826087, 0.9817232375979112, 0.9790940766550522, 0.976460331299041], "brevity_penalty": 0.08838199942195084, "length_ratio": 0.2918781725888325, "translation_length": 1150, "reference_length": 3940}
{"bleu": 0.8318071540745801, "precisions": [0.9686274509803922, 0.9645669291338582, 0.9644268774703557, 0.9642857142857143], "brevity_penalty": 0.8615522215525229, "length_ratio": 0.8703071672354948, "translation_length": 255, "reference_length": 293}
{"bleu": 0.0, "precisions": [0.0, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 1.4285714285714286, "translation_length": 10, "reference_length": 7}
{"bleu": 0.6858544034878273, "precisions": [0.9988095238095238, 0.9976162097735399, 0.9976133651551312, 0.997610513739546], "brevity_penalty": 0.6872892787909722, "length_ratio": 0.7272727272727273, "translation_length": 840, "reference_length": 1155}
{"bleu": 0.0, "precisions": [0.17391304347826086, 0.0, 0.0, 0.0], "brevity_penalty": 0.41913374169932255, "length_ratio": 0.5348837209302325, "translation_length": 23, "reference_length": 43}
{"bleu": 0.0, "precisions": [0.004906771344455349, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 36.392857142857146, "translation_length": 1019, "reference_length": 28}
{"bleu": 0.049938575758490346, "precisions": [0.05892116182572614, 0.05149501661129568, 0.04738154613466334, 0.04326123128119801], "brevity_penalty": 1.0, "length_ratio": 14.518072289156626, "translation_length": 1205, "reference_length": 83}
{"bleu": 0.0, "precisions": [0.058823529411764705, 0.0, 0.0, 0.0], "brevity_penalty": 0.7903383629814982, "length_ratio": 0.8095238095238095, "translation_length": 17, "reference_length": 21}
{"error": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"}
{"bleu": 0.0, "precisions": [0.09523809523809523, 0.0, 0.0, 0.0], "brevity_penalty": 0.8668778997501817, "length_ratio": 0.875, "translation_length": 21, "reference_length": 24}
{"error": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"}
{"bleu": 0.0, "precisions": [0.001949317738791423, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 31.09090909090909, "translation_length": 1026, "reference_length": 33}
{"bleu": 0.12167453031221911, "precisions": [0.125, 0.12280701754385964, 0.12060301507537688, 0.11838790931989925], "brevity_penalty": 1.0, "length_ratio": 7.017543859649122, "translation_length": 400, "reference_length": 57}
{"bleu": 0.12167453031221911, "precisions": [0.125, 0.12280701754385964, 0.12060301507537688, 0.11838790931989925], "brevity_penalty": 1.0, "length_ratio": 7.017543859649122, "translation_length": 400, "reference_length": 57}
{"bleu": 0.5550767071419432, "precisions": [0.9522546419098143, 0.925531914893617, 0.8986666666666666, 0.8716577540106952], "brevity_penalty": 0.6089487189470616, "length_ratio": 0.6684397163120568, "translation_length": 377, "reference_length": 564}
{"bleu": 0.0007036220011850221, "precisions": [0.501834862385321, 0.10651974288337925, 0.09834558823529412, 0.09291628334866606], "brevity_penalty": 0.004732929094907735, "length_ratio": 0.15740072202166064, "translation_length": 1090, "reference_length": 6925}
{"bleu": 0.5786676874577258, "precisions": [0.8050847457627118, 0.7272727272727273, 0.7002427184466019, 0.6731470230862697], "brevity_penalty": 0.7983712229266925, "length_ratio": 0.8162055335968379, "translation_length": 826, "reference_length": 1012}
{"bleu": 0.1457667435558859, "precisions": [0.1626865671641791, 0.14947683109118087, 0.1407185628742515, 0.13193403298350825], "brevity_penalty": 1.0, "length_ratio": 4.926470588235294, "translation_length": 670, "reference_length": 136}
{"bleu": 0.0, "precisions": [0.2222222222222222, 0.0, 0.0, 0.0], "brevity_penalty": 2.303378476681455e-09, "length_ratio": 0.047872340425531915, "translation_length": 9, "reference_length": 188}
{"bleu": 0.0, "precisions": [0.2556390977443609, 0.0, 0.0, 0.0], "brevity_penalty": 0.2616226369849343, "length_ratio": 0.4271948608137045, "translation_length": 798, "reference_length": 1868}
{"bleu": 0.0, "precisions": [0.0, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 135.71428571428572, "translation_length": 950, "reference_length": 7}
{"bleu": 0.5332663067192828, "precisions": [0.5759345794392523, 0.5345029239766081, 0.5199063231850117, 0.5052754982415005], "brevity_penalty": 1.0, "length_ratio": 1.5793357933579335, "translation_length": 856, "reference_length": 542}
{"bleu": 0.9706016810185693, "precisions": [0.9892857142857143, 0.9892729439809297, 0.9892601431980907, 0.989247311827957], "brevity_penalty": 0.9811326405284632, "length_ratio": 0.9813084112149533, "translation_length": 840, "reference_length": 856}
{"bleu": 0.0076764130846227446, "precisions": [0.026851851851851852, 0.008341056533827619, 0.0055658627087198514, 0.002785515320334262], "brevity_penalty": 1.0, "length_ratio": 19.285714285714285, "translation_length": 1080, "reference_length": 56}
{"bleu": 0.0671746022977193, "precisions": [0.4444444444444444, 0.15056179775280898, 0.09673790776152981, 0.04842342342342342], "brevity_penalty": 0.5048460823944081, "length_ratio": 0.594, "translation_length": 891, "reference_length": 1500}
{"bleu": 0.04351065564205741, "precisions": [0.6435331230283912, 0.47368421052631576, 0.4636459430979979, 0.45358649789029537], "brevity_penalty": 0.08647071071679176, "length_ratio": 0.2900274473924977, "translation_length": 951, "reference_length": 3279}
{"bleu": 0.0, "precisions": [0.0, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 7.9186046511627906, "translation_length": 2043, "reference_length": 258}
{"bleu": 0.028999757684749817, "precisions": [0.030558482613277135, 0.029535864978902954, 0.028511087645195353, 0.02748414376321353], "brevity_penalty": 1.0, "length_ratio": 25.64864864864865, "translation_length": 949, "reference_length": 37}
{"bleu": 0.0, "precisions": [0.0, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 101.55555555555556, "translation_length": 914, "reference_length": 9}
{"bleu": 0.0, "precisions": [0.1, 0.0, 0.0, 0.0], "brevity_penalty": 9.237449661970594e-09, "length_ratio": 0.05128205128205128, "translation_length": 10, "reference_length": 195}
{"bleu": 0.0, "precisions": [0.013157894736842105, 0.0032948929159802307, 0.0016501650165016502, 0.0], "brevity_penalty": 1.0, "length_ratio": 27.636363636363637, "translation_length": 608, "reference_length": 22}
{"bleu": 0.0, "precisions": [0.1, 0.0, 0.0, 0.0], "brevity_penalty": 0.14956861922263506, "length_ratio": 0.3448275862068966, "translation_length": 10, "reference_length": 29}
{"bleu": 0.0006848476810076277, "precisions": [0.7321688500727802, 0.45335276967930027, 0.4131386861313869, 0.37280701754385964], "brevity_penalty": 0.0014402474782956815, "length_ratio": 0.1325742956387495, "translation_length": 687, "reference_length": 5182}
{"bleu": 0.0, "precisions": [0.013846153846153847, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 6.310679611650485, "translation_length": 650, "reference_length": 103}
{"bleu": 0.0, "precisions": [0.13043478260869565, 0.0, 0.0, 0.0], "brevity_penalty": 0.41913374169932255, "length_ratio": 0.5348837209302325, "translation_length": 23, "reference_length": 43}
{"bleu": 1.634599054263854e-05, "precisions": [0.8888888888888888, 0.6261682242990654, 0.5849056603773585, 0.5523809523809524], "brevity_penalty": 2.51012580242973e-05, "length_ratio": 0.08626198083067092, "translation_length": 108, "reference_length": 1252}
{"bleu": 0.0, "precisions": [0.08, 0.0, 0.0, 0.0], "brevity_penalty": 7.810824733562767e-06, "length_ratio": 0.07836990595611286, "translation_length": 25, "reference_length": 319}
{"bleu": 0.0, "precisions": [0.058823529411764705, 0.0, 0.0, 0.0], "brevity_penalty": 0.7903383629814982, "length_ratio": 0.8095238095238095, "translation_length": 17, "reference_length": 21}
{"bleu": 0.0, "precisions": [0.35714285714285715, 0.0, 0.0, 0.0], "brevity_penalty": 6.711415863331494e-07, "length_ratio": 0.06572769953051644, "translation_length": 14, "reference_length": 213}
{"bleu": 1.730540932851365e-19, "precisions": [0.8933333333333333, 0.7972972972972973, 0.7808219178082192, 0.7638888888888888], "brevity_penalty": 2.143521634806862e-19, "length_ratio": 0.022734161867232493, "translation_length": 75, "reference_length": 3299}
{"bleu": 0.0, "precisions": [0.15384615384615385, 0.0, 0.0, 0.0], "brevity_penalty": 0.08530361363583897, "length_ratio": 0.28888888888888886, "translation_length": 13, "reference_length": 45}
{"error": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"}
{"bleu": 0.0, "precisions": [0.2, 0.0, 0.0, 0.0], "brevity_penalty": 0.029207395438494873, "length_ratio": 0.22058823529411764, "translation_length": 15, "reference_length": 68}
{"bleu": 0.036300454924701187, "precisions": [0.07794871794871795, 0.039014373716632446, 0.02774922918807811, 0.0205761316872428], "brevity_penalty": 1.0, "length_ratio": 7.677165354330708, "translation_length": 975, "reference_length": 127}
{"bleu": 0.0, "precisions": [0.0017064846416382253, 0.0, 0.0, 0.0], "brevity_penalty": 0.6112062477288437, "length_ratio": 0.6700971983990852, "translation_length": 1172, "reference_length": 1749}
{"bleu": 0.26988673956521764, "precisions": [0.6675824175824175, 0.4718019257221458, 0.43388429752066116, 0.40551724137931033], "brevity_penalty": 0.5562494348298312, "length_ratio": 0.6303030303030303, "translation_length": 728, "reference_length": 1155}
{"bleu": 0.014542319471982283, "precisions": [0.0296220633299285, 0.016359918200409, 0.011258955987717503, 0.00819672131147541], "brevity_penalty": 1.0, "length_ratio": 15.061538461538461, "translation_length": 979, "reference_length": 65}
{"bleu": 0.0, "precisions": [0.07692307692307693, 0.0, 0.0, 0.0], "brevity_penalty": 0.5404329964865341, "length_ratio": 0.6190476190476191, "translation_length": 13, "reference_length": 21}
{"bleu": 0.09917991757461825, "precisions": [0.18544366899302095, 0.09081836327345309, 0.08091908091908091, 0.071], "brevity_penalty": 1.0, "length_ratio": 2.3937947494033414, "translation_length": 1003, "reference_length": 419}
{"bleu": 0.023408325247067825, "precisions": [0.21235521235521235, 0.015503875968992248, 0.011673151750972763, 0.0078125], "brevity_penalty": 1.0, "length_ratio": 1.85, "translation_length": 259, "reference_length": 140}
{"bleu": 0.0, "precisions": [0.12, 0.0, 0.0, 0.0], "brevity_penalty": 0.4867522559599717, "length_ratio": 0.5813953488372093, "translation_length": 25, "reference_length": 43}
{"error": "CUDA out of memory. Tried to allocate 576.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 578.69 MiB is free. Including non-PyTorch memory, this process has 10.18 GiB memory in use. Of the allocated memory 9.91 GiB is allocated by PyTorch, and 62.31 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"error": "CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 138.69 MiB is free. Including non-PyTorch memory, this process has 10.61 GiB memory in use. Of the allocated memory 10.35 GiB is allocated by PyTorch, and 53.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"bleu": 0.0, "precisions": [0.027283511269276393, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 3.0654545454545454, "translation_length": 843, "reference_length": 275}
{"bleu": 0.0, "precisions": [0.04, 0.0, 0.0, 0.0], "brevity_penalty": 0.039163895098987066, "length_ratio": 0.2358490566037736, "translation_length": 25, "reference_length": 106}
{"bleu": 0.22240022547047036, "precisions": [0.31654676258992803, 0.2318840579710145, 0.19708029197080293, 0.16911764705882354], "brevity_penalty": 1.0, "length_ratio": 2.106060606060606, "translation_length": 139, "reference_length": 66}
{"error": "Unsloth: input length 241975 + max_new_tokens 1024 exceeds the maximum sequence length of 131072!\nYou will need to do long context extension by increasing the `max_seq_length` in `FastLanguageModel.from_pretrained`."}
{"bleu": 0.5421923934334224, "precisions": [0.6540880503144654, 0.5253164556962026, 0.5095541401273885, 0.4935897435897436], "brevity_penalty": 1.0, "length_ratio": 1.2045454545454546, "translation_length": 159, "reference_length": 132}
{"error": "CUDA out of memory. Tried to allocate 304.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 120.69 MiB is free. Including non-PyTorch memory, this process has 10.62 GiB memory in use. Of the allocated memory 10.19 GiB is allocated by PyTorch, and 236.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"bleu": 0.0, "precisions": [0.03597883597883598, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 9.264705882352942, "translation_length": 945, "reference_length": 102}
{"bleu": 0.0, "precisions": [0.0, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 3.2857142857142856, "translation_length": 23, "reference_length": 7}
{"bleu": 0.0, "precisions": [0.11764705882352941, 0.0, 0.0, 0.0], "brevity_penalty": 6.571890466284153e-07, "length_ratio": 0.06563706563706563, "translation_length": 17, "reference_length": 259}
{"bleu": 0.0, "precisions": [0.34615384615384615, 0.0, 0.0, 0.0], "brevity_penalty": 4.1115786849912614e-47, "length_ratio": 0.00927577595433464, "translation_length": 26, "reference_length": 2803}
{"bleu": 0.0304249951608961, "precisions": [0.08655126498002663, 0.02666666666666667, 0.021361815754339118, 0.017379679144385027], "brevity_penalty": 1.0, "length_ratio": 3.3083700440528636, "translation_length": 751, "reference_length": 227}
{"bleu": 0.0, "precisions": [0.09523809523809523, 0.0, 0.0, 0.0], "brevity_penalty": 0.8668778997501817, "length_ratio": 0.875, "translation_length": 21, "reference_length": 24}
{"bleu": 0.0, "precisions": [0.13333333333333333, 0.0, 0.0, 0.0], "brevity_penalty": 0.1266071027890836, "length_ratio": 0.32608695652173914, "translation_length": 15, "reference_length": 46}
{"bleu": 0.0, "precisions": [0.2, 0.0, 0.0, 0.0], "brevity_penalty": 5.7346784092083264e-18, "length_ratio": 0.02457002457002457, "translation_length": 10, "reference_length": 407}
{"bleu": 0.17294551898767674, "precisions": [0.3592233009708738, 0.1587473002159827, 0.14702702702702702, 0.13528138528138528], "brevity_penalty": 0.9423946191458162, "length_ratio": 0.9439918533604889, "translation_length": 927, "reference_length": 982}
{"bleu": 0.0, "precisions": [0.27906976744186046, 0.0, 0.0, 0.0], "brevity_penalty": 0.7390973893525953, "length_ratio": 0.7678571428571429, "translation_length": 43, "reference_length": 56}
{"bleu": 0.1707197862545001, "precisions": [0.19906542056074766, 0.17586529466791395, 0.16385767790262173, 0.14807872539831302], "brevity_penalty": 1.0, "length_ratio": 4.421487603305785, "translation_length": 1070, "reference_length": 242}
{"bleu": 0.007251944954967816, "precisions": [0.011363636363636364, 0.007585335018963337, 0.006329113924050633, 0.005069708491761723], "brevity_penalty": 1.0, "length_ratio": 29.333333333333332, "translation_length": 792, "reference_length": 27}
{"bleu": 0.0, "precisions": [0.15384615384615385, 0.0, 0.0, 0.0], "brevity_penalty": 0.005777142164111217, "length_ratio": 0.1625, "translation_length": 13, "reference_length": 80}
{"bleu": 0.0, "precisions": [0.07692307692307693, 0.0, 0.0, 0.0], "brevity_penalty": 0.09949058049485841, "length_ratio": 0.3023255813953488, "translation_length": 13, "reference_length": 43}
{"bleu": 0.0, "precisions": [0.11764705882352941, 0.0, 0.0, 0.0], "brevity_penalty": 0.5235834657149969, "length_ratio": 0.6071428571428571, "translation_length": 17, "reference_length": 28}
{"bleu": 0.0, "precisions": [0.07692307692307693, 0.0, 0.0, 0.0], "brevity_penalty": 0.1988141887380742, "length_ratio": 0.38235294117647056, "translation_length": 13, "reference_length": 34}
{"bleu": 6.941604834797188e-12, "precisions": [0.3856837606837607, 0.014973262032085561, 0.0053533190578158455, 0.0010718113612004287], "brevity_penalty": 5.145025971171134e-10, "length_ratio": 0.044667143879742306, "translation_length": 936, "reference_length": 20955}
{"bleu": 0.0, "precisions": [0.15384615384615385, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 1.0655737704918034, "translation_length": 65, "reference_length": 61}
{"bleu": 0.0, "precisions": [0.05555555555555555, 0.0, 0.0, 0.0], "brevity_penalty": 0.846481724890614, "length_ratio": 0.8571428571428571, "translation_length": 18, "reference_length": 21}
{"bleu": 0.0, "precisions": [0.11363636363636363, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 1.5172413793103448, "translation_length": 44, "reference_length": 29}
{"bleu": 0.050230736506209794, "precisions": [0.05926544240400668, 0.05179615705931495, 0.047658862876254184, 0.04351464435146443], "brevity_penalty": 1.0, "length_ratio": 14.790123456790123, "translation_length": 1198, "reference_length": 81}
{"bleu": 0.0, "precisions": [0.4993894993894994, 0.0, 0.0, 0.0], "brevity_penalty": 0.0002555012458305531, "length_ratio": 0.1078483012904925, "translation_length": 819, "reference_length": 7594}
{"bleu": 0.09278143271417989, "precisions": [0.19729206963249515, 0.09390125847047434, 0.07364341085271318, 0.05431619786614937], "brevity_penalty": 1.0, "length_ratio": 1.3842034805890227, "translation_length": 1034, "reference_length": 747}
{"bleu": 0.0, "precisions": [0.13333333333333333, 0.0, 0.0, 0.0], "brevity_penalty": 0.28176928909495835, "length_ratio": 0.4411764705882353, "translation_length": 15, "reference_length": 34}
{"error": "CUDA out of memory. Tried to allocate 236.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 238.69 MiB is free. Including non-PyTorch memory, this process has 10.51 GiB memory in use. Of the allocated memory 10.11 GiB is allocated by PyTorch, and 196.82 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"bleu": 0.0, "precisions": [0.35714285714285715, 0.0, 0.0, 0.0], "brevity_penalty": 6.599138626593368e-06, "length_ratio": 0.07734806629834254, "translation_length": 14, "reference_length": 181}
{"bleu": 0.2722716491946002, "precisions": [0.555439330543933, 0.38848167539267014, 0.36163522012578614, 0.3410283315844701], "brevity_penalty": 0.6741170864186505, "length_ratio": 0.7171792948237059, "translation_length": 956, "reference_length": 1333}
{"bleu": 0.0, "precisions": [0.07692307692307693, 0.0, 0.0, 0.0], "brevity_penalty": 0.08530361363583897, "length_ratio": 0.28888888888888886, "translation_length": 13, "reference_length": 45}
{"bleu": 0.036998067665729356, "precisions": [0.9882629107981221, 0.9776733254994124, 0.9670588235294117, 0.9564193168433451], "brevity_penalty": 0.038052848223348396, "length_ratio": 0.234259004674182, "translation_length": 852, "reference_length": 3637}
{"bleu": 0.0, "precisions": [0.2797270955165692, 0.018536585365853658, 0.0, 0.0], "brevity_penalty": 0.7837503681165506, "length_ratio": 0.8040752351097179, "translation_length": 1026, "reference_length": 1276}
{"bleu": 0.09753682726684024, "precisions": [0.10093457943925234, 0.09822263797942002, 0.09644194756554307, 0.09465791940018745], "brevity_penalty": 1.0, "length_ratio": 9.385964912280702, "translation_length": 1070, "reference_length": 114}
{"bleu": 0.0, "precisions": [0.05, 0.0, 0.0, 0.0], "brevity_penalty": 0.951229424500714, "length_ratio": 0.9523809523809523, "translation_length": 20, "reference_length": 21}
{"error": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"}
{"bleu": 0.0, "precisions": [0.015209125475285171, 0.0012690355329949238, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 12.523809523809524, "translation_length": 789, "reference_length": 63}
{"bleu": 0.022878739224653773, "precisions": [0.0242152466367713, 0.02333931777378815, 0.022461814914645103, 0.02158273381294964], "brevity_penalty": 1.0, "length_ratio": 32.794117647058826, "translation_length": 1115, "reference_length": 34}
{"bleu": 4.6687616673740156e-14, "precisions": [0.756578947368421, 0.5298013245033113, 0.47333333333333333, 0.436241610738255], "brevity_penalty": 8.704349827321543e-14, "length_ratio": 0.032182934575481686, "translation_length": 152, "reference_length": 4723}
{"bleu": 0.0, "precisions": [0.058823529411764705, 0.0, 0.0, 0.0], "brevity_penalty": 0.21666307870822255, "length_ratio": 0.3953488372093023, "translation_length": 17, "reference_length": 43}
{"error": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"}
{"bleu": 0.012760000512022165, "precisions": [0.01744186046511628, 0.013969732246798603, 0.011655011655011656, 0.009334889148191364], "brevity_penalty": 1.0, "length_ratio": 34.4, "translation_length": 860, "reference_length": 25}
{"bleu": 1.9272579049117907e-05, "precisions": [0.6206030150753769, 0.4744341994970662, 0.4110738255033557, 0.36272040302267], "brevity_penalty": 4.210361792859337e-05, "length_ratio": 0.09029038112522686, "translation_length": 1194, "reference_length": 13224}
{"bleu": 0.004136806534754048, "precisions": [0.96440489432703, 0.9276169265033407, 0.8907469342251951, 0.8537946428571429], "brevity_penalty": 0.004554925234240283, "length_ratio": 0.15645666550643927, "translation_length": 899, "reference_length": 5746}
{"bleu": 0.0, "precisions": [0.1, 0.0, 0.0, 0.0], "brevity_penalty": 0.12873490358780418, "length_ratio": 0.32786885245901637, "translation_length": 20, "reference_length": 61}
{"bleu": 0.13938364767555214, "precisions": [0.20029133284777859, 0.141399416909621, 0.12253829321663019, 0.10875912408759124], "brevity_penalty": 1.0, "length_ratio": 3.968208092485549, "translation_length": 1373, "reference_length": 346}
{"bleu": 0.0043219406357046875, "precisions": [0.014134275618374558, 0.00589622641509434, 0.0035419126328217238, 0.001182033096926714], "brevity_penalty": 1.0, "length_ratio": 29.275862068965516, "translation_length": 849, "reference_length": 29}
{"bleu": 0.0, "precisions": [0.25, 0.0, 0.0, 0.0], "brevity_penalty": 5.715007736466731e-07, "length_ratio": 0.06504065040650407, "translation_length": 8, "reference_length": 123}
{"error": "CUDA out of memory. Tried to allocate 304.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 218.69 MiB is free. Including non-PyTorch memory, this process has 10.53 GiB memory in use. Of the allocated memory 10.17 GiB is allocated by PyTorch, and 152.50 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"error": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 18.69 MiB is free. Including non-PyTorch memory, this process has 10.72 GiB memory in use. Of the allocated memory 10.36 GiB is allocated by PyTorch, and 158.21 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"bleu": 0.017299784512718026, "precisions": [0.5982062780269058, 0.43177737881508077, 0.42857142857142855, 0.427158273381295], "brevity_penalty": 0.03709880932359339, "length_ratio": 0.23287385129490393, "translation_length": 1115, "reference_length": 4788}
{"bleu": 0.011181125831142067, "precisions": [0.6702937976060935, 0.30718954248366015, 0.23446019629225737, 0.17139737991266377], "brevity_penalty": 0.03707228048472527, "length_ratio": 0.2328350646060299, "translation_length": 919, "reference_length": 3947}
{"bleu": 0.003953217797446228, "precisions": [0.6022988505747127, 0.23475258918296893, 0.2108294930875576, 0.18685121107266436], "brevity_penalty": 0.014470659187626553, "length_ratio": 0.19099890230515917, "translation_length": 870, "reference_length": 4555}
{"bleu": 0.0, "precisions": [0.2, 0.0, 0.0, 0.0], "brevity_penalty": 0.00016658581098763324, "length_ratio": 0.10309278350515463, "translation_length": 10, "reference_length": 97}
{"bleu": 0.6612986223656377, "precisions": [0.6618625277161863, 0.6614872364039955, 0.6611111111111111, 0.660734149054505], "brevity_penalty": 1.0, "length_ratio": 1.4933774834437086, "translation_length": 902, "reference_length": 604}
{"bleu": 0.0, "precisions": [0.043968432919954906, 0.001128668171557562, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 8.696078431372548, "translation_length": 887, "reference_length": 102}
{"bleu": 0.0, "precisions": [0.12, 0.0, 0.0, 0.0], "brevity_penalty": 0.027323722447292545, "length_ratio": 0.21739130434782608, "translation_length": 25, "reference_length": 115}
{"bleu": 0.0, "precisions": [0.04918032786885246, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 4.265734265734266, "translation_length": 610, "reference_length": 143}
{"bleu": 0.01730497086270937, "precisions": [0.1639111950873878, 0.06805293005671077, 0.054373522458628844, 0.05250709555345317], "brevity_penalty": 0.2303592441679405, "length_ratio": 0.4051674641148325, "translation_length": 2117, "reference_length": 5225}
{"bleu": 0.0009090956851161612, "precisions": [0.282336578581363, 0.06406685236768803, 0.03626220362622036, 0.013966480446927373], "brevity_penalty": 0.016524341822649904, "length_ratio": 0.1959662033251567, "translation_length": 719, "reference_length": 3669}
{"bleu": 0.0, "precisions": [0.3018292682926829, 0.12366412213740458, 0.022935779816513763, 0.0], "brevity_penalty": 5.856186569527315e-05, "length_ratio": 0.0930628457937296, "translation_length": 656, "reference_length": 7049}
{"bleu": 0.017327467165611076, "precisions": [0.23294346978557504, 0.02829268292682927, 0.0185546875, 0.009775171065493646], "brevity_penalty": 0.5240339375668694, "length_ratio": 0.6074600355239786, "translation_length": 1026, "reference_length": 1689}
{"bleu": 0.0, "precisions": [0.23809523809523808, 0.0, 0.0, 0.0], "brevity_penalty": 2.4861831257874807e-06, "length_ratio": 0.07191780821917808, "translation_length": 21, "reference_length": 292}
{"bleu": 2.7939895525503848e-05, "precisions": [0.7, 0.3877551020408163, 0.375, 0.3617021276595745], "brevity_penalty": 6.378452193155948e-05, "length_ratio": 0.09380863039399624, "translation_length": 50, "reference_length": 533}
{"error": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"}
{"bleu": 0.0, "precisions": [0.030162412993039442, 0.0069767441860465115, 0.002331002331002331, 0.0], "brevity_penalty": 0.06516332497618357, "length_ratio": 0.26803482587064675, "translation_length": 431, "reference_length": 1608}
{"bleu": 0.0, "precisions": [0.0004878048780487805, 0.0, 0.0, 0.0], "brevity_penalty": 0.01249484619130736, "length_ratio": 0.18578937828529998, "translation_length": 2050, "reference_length": 11034}
{"bleu": 0.05288260367202347, "precisions": [0.06239015817223199, 0.05452946350043975, 0.05017605633802817, 0.04581497797356828], "brevity_penalty": 1.0, "length_ratio": 14.049382716049383, "translation_length": 1138, "reference_length": 81}
{"bleu": 0.12834388104585512, "precisions": [0.27608346709470305, 0.13815261044176708, 0.10048231511254019, 0.07079646017699115], "brevity_penalty": 1.0, "length_ratio": 1.3499458288190682, "translation_length": 1246, "reference_length": 923}
{"bleu": 0.03109089693639789, "precisions": [0.4078341013824885, 0.17301038062283736, 0.1558891454965358, 0.14566473988439307], "brevity_penalty": 0.1554002624669145, "length_ratio": 0.3494363929146538, "translation_length": 868, "reference_length": 2484}
{"bleu": 0.6590904830993239, "precisions": [0.9840881272949816, 0.9767156862745098, 0.9705521472392638, 0.9643734643734644], "brevity_penalty": 0.6767503208545177, "length_ratio": 0.7191901408450704, "translation_length": 817, "reference_length": 1136}
{"bleu": 0.0, "precisions": [0.0851063829787234, 0.0, 0.0, 0.0], "brevity_penalty": 0.39212672523825076, "length_ratio": 0.5164835164835165, "translation_length": 47, "reference_length": 91}
{"bleu": 0.08659831652663039, "precisions": [0.09182736455463728, 0.08731617647058823, 0.08463661453541858, 0.08287292817679558], "brevity_penalty": 1.0, "length_ratio": 9.81081081081081, "translation_length": 1089, "reference_length": 111}
{"bleu": 0.0, "precisions": [0.0031620553359683794, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 52.708333333333336, "translation_length": 1265, "reference_length": 24}
{"bleu": 0.0, "precisions": [0.01485148514851485, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 25.25, "translation_length": 1010, "reference_length": 40}
{"bleu": 0.0, "precisions": [0.2361863488624052, 0.019522776572668113, 0.0054288816503800215, 0.0], "brevity_penalty": 0.0008452825115367129, "length_ratio": 0.12382613361953314, "translation_length": 923, "reference_length": 7454}
{"bleu": 1.4613982793299654e-05, "precisions": [0.5864661654135338, 0.38924731182795697, 0.2917115177610334, 0.2435344827586207], "brevity_penalty": 4.0951794365224324e-05, "length_ratio": 0.09006481571055432, "translation_length": 931, "reference_length": 10337}
{"bleu": 0.0, "precisions": [0.25, 0.0, 0.0, 0.0], "brevity_penalty": 2.1445408316589164e-05, "length_ratio": 0.0851063829787234, "translation_length": 8, "reference_length": 94}
{"error": "CUDA out of memory. Tried to allocate 206.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 94.69 MiB is free. Including non-PyTorch memory, this process has 10.65 GiB memory in use. Of the allocated memory 10.28 GiB is allocated by PyTorch, and 164.45 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"error": "CUDA out of memory. Tried to allocate 58.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 54.69 MiB is free. Including non-PyTorch memory, this process has 10.69 GiB memory in use. Of the allocated memory 10.34 GiB is allocated by PyTorch, and 147.69 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"bleu": 0.0, "precisions": [0.1, 0.0, 0.0, 0.0], "brevity_penalty": 0.0022428677194858034, "length_ratio": 0.14084507042253522, "translation_length": 10, "reference_length": 71}
{"bleu": 0.0, "precisions": [0.058823529411764705, 0.0, 0.0, 0.0], "brevity_penalty": 0.7903383629814982, "length_ratio": 0.8095238095238095, "translation_length": 17, "reference_length": 21}
{"bleu": 0.29137947355439525, "precisions": [0.4877450980392157, 0.30920245398773005, 0.3046683046683047, 0.3025830258302583], "brevity_penalty": 0.8485589796160492, "length_ratio": 0.8589473684210527, "translation_length": 816, "reference_length": 950}
{"bleu": 0.015443889272608036, "precisions": [0.958904109589041, 0.9027777777777778, 0.9014084507042254, 0.9], "brevity_penalty": 0.01687044877500055, "length_ratio": 0.1967654986522911, "translation_length": 73, "reference_length": 371}
{"bleu": 0.025469221889512183, "precisions": [0.42528735632183906, 0.0741687979539642, 0.04737516005121639, 0.04358974358974359], "brevity_penalty": 0.283501915714318, "length_ratio": 0.4423728813559322, "translation_length": 783, "reference_length": 1770}
{"bleu": 0.22018770764696785, "precisions": [0.4948921679909194, 0.34204545454545454, 0.31285551763367464, 0.28815489749430523], "brevity_penalty": 0.6264725018709117, "length_ratio": 0.6813611755607115, "translation_length": 881, "reference_length": 1293}
{"bleu": 0.0, "precisions": [0.07692307692307693, 0.0, 0.0, 0.0], "brevity_penalty": 0.09949058049485841, "length_ratio": 0.3023255813953488, "translation_length": 13, "reference_length": 43}
{"bleu": 0.0047639003648121975, "precisions": [0.9375639713408394, 0.8975409836065574, 0.8646153846153846, 0.8316221765913757], "brevity_penalty": 0.005401465604797546, "length_ratio": 0.16074366567949983, "translation_length": 977, "reference_length": 6078}
{"bleu": 0.07277668016463382, "precisions": [0.1357615894039735, 0.07845303867403315, 0.05530973451327434, 0.047619047619047616], "brevity_penalty": 1.0, "length_ratio": 4.253521126760563, "translation_length": 906, "reference_length": 213}
{"bleu": 0.0, "precisions": [0.2, 0.0, 0.0, 0.0], "brevity_penalty": 0.740818220681718, "length_ratio": 0.7692307692307693, "translation_length": 10, "reference_length": 13}
{"bleu": 0.0, "precisions": [0.0004992511233150275, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 14.514492753623188, "translation_length": 2003, "reference_length": 138}
{"bleu": 0.0, "precisions": [0.11764705882352941, 0.0, 0.0, 0.0], "brevity_penalty": 0.6624801353939264, "length_ratio": 0.7083333333333334, "translation_length": 17, "reference_length": 24}
{"bleu": 0.0, "precisions": [0.006986027944111776, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 21.782608695652176, "translation_length": 1002, "reference_length": 46}
{"bleu": 0.01311134086920541, "precisions": [0.07049345417925479, 0.017137096774193547, 0.006054490413723511, 0.00404040404040404], "brevity_penalty": 1.0, "length_ratio": 3.650735294117647, "translation_length": 993, "reference_length": 272}
{"bleu": 0.13069274005205833, "precisions": [0.7105978260869565, 0.5591836734693878, 0.4659400544959128, 0.39836289222373805], "brevity_penalty": 0.2507867822629451, "length_ratio": 0.41961231470923605, "translation_length": 736, "reference_length": 1754}
{"bleu": 0.0, "precisions": [0.010089686098654708, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 29.733333333333334, "translation_length": 892, "reference_length": 30}
{"bleu": 0.0, "precisions": [0.01639344262295082, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 7.176470588235294, "translation_length": 732, "reference_length": 102}
{"error": "CUDA out of memory. Tried to allocate 504.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 272.69 MiB is free. Including non-PyTorch memory, this process has 10.48 GiB memory in use. Of the allocated memory 10.10 GiB is allocated by PyTorch, and 171.13 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"bleu": 0.0, "precisions": [0.024390243902439025, 0.0048828125, 0.0009775171065493646, 0.0], "brevity_penalty": 1.0, "length_ratio": 13.85135135135135, "translation_length": 1025, "reference_length": 74}
{"bleu": 0.006617321137271146, "precisions": [0.013902681231380337, 0.006958250497017893, 0.004975124378109453, 0.00398406374501992], "brevity_penalty": 1.0, "length_ratio": 28.771428571428572, "translation_length": 1007, "reference_length": 35}
{"bleu": 0.0, "precisions": [0.15789473684210525, 0.0, 0.0, 0.0], "brevity_penalty": 0.5907775139012316, "length_ratio": 0.6551724137931034, "translation_length": 19, "reference_length": 29}
{"bleu": 0.3954560944724798, "precisions": [0.4309623430962343, 0.4036312849162011, 0.38461538461538464, 0.36554621848739494], "brevity_penalty": 1.0, "length_ratio": 1.8968253968253967, "translation_length": 717, "reference_length": 378}
{"bleu": 0.0, "precisions": [0.2, 0.0, 0.0, 0.0], "brevity_penalty": 0.04735892439114093, "length_ratio": 0.24691358024691357, "translation_length": 20, "reference_length": 81}
{"bleu": 0.0, "precisions": [0.10638297872340426, 0.0, 0.0, 0.0], "brevity_penalty": 0.9381646735450738, "length_ratio": 0.94, "translation_length": 47, "reference_length": 50}
{"bleu": 0.0, "precisions": [0.0, 0.0, 0.0, 0.0], "brevity_penalty": 0.6703200460356393, "length_ratio": 0.7142857142857143, "translation_length": 10, "reference_length": 14}
{"bleu": 2.8579766638709842e-06, "precisions": [0.5781563126252505, 0.12637913741223672, 0.0783132530120482, 0.06432160804020101], "brevity_penalty": 2.0633859750713805e-05, "length_ratio": 0.08482787930301743, "translation_length": 998, "reference_length": 11765}
{"bleu": 0.0, "precisions": [0.25, 0.0, 0.0, 0.0], "brevity_penalty": 9.693298441056218e-105, "length_ratio": 0.004158004158004158, "translation_length": 36, "reference_length": 8658}
{"bleu": 0.0, "precisions": [0.15, 0.0, 0.0, 0.0], "brevity_penalty": 0.5769498103804866, "length_ratio": 0.6451612903225806, "translation_length": 20, "reference_length": 31}
{"bleu": 0.0, "precisions": [0.20018365472910926, 0.0009191176470588235, 0.0, 0.0], "brevity_penalty": 0.03454567522710446, "length_ratio": 0.22907025662599917, "translation_length": 1089, "reference_length": 4754}
{"bleu": 0.02161886060751086, "precisions": [0.044284243048403706, 0.02268041237113402, 0.017543859649122806, 0.012396694214876033], "brevity_penalty": 1.0, "length_ratio": 9.160377358490566, "translation_length": 971, "reference_length": 106}
{"bleu": 0.27078920476784707, "precisions": [0.3490136570561457, 0.2735562310030395, 0.2496194824961948, 0.22560975609756098], "brevity_penalty": 1.0, "length_ratio": 1.1001669449081803, "translation_length": 659, "reference_length": 599}
{"bleu": 0.0, "precisions": [0.0029910269192422734, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 71.64285714285714, "translation_length": 1003, "reference_length": 14}
{"bleu": 0.013992305881393672, "precisions": [0.07190412782956059, 0.018666666666666668, 0.010680907877169559, 0.00267379679144385], "brevity_penalty": 1.0, "length_ratio": 4.908496732026144, "translation_length": 751, "reference_length": 153}
{"bleu": 0.0, "precisions": [0.0, 0.0, 0.0, 0.0], "brevity_penalty": 0.4493289641172217, "length_ratio": 0.5555555555555556, "translation_length": 10, "reference_length": 18}
{"bleu": 0.0, "precisions": [0.0014484356894553883, 0.0002897291032884253, 0.0001448855404230658, 0.0], "brevity_penalty": 1.0, "length_ratio": 197.25714285714287, "translation_length": 6904, "reference_length": 35}
{"bleu": 0.05742874514484352, "precisions": [0.9776902887139107, 0.9513797634691196, 0.9263157894736842, 0.9011857707509882], "brevity_penalty": 0.06117826775358215, "length_ratio": 0.26357661708751295, "translation_length": 762, "reference_length": 2891}
{"bleu": 0.0, "precisions": [0.10719754977029096, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 1.1894353369763206, "translation_length": 653, "reference_length": 549}
{"bleu": 0.0, "precisions": [0.012477718360071301, 0.0, 0.0, 0.0], "brevity_penalty": 0.0006177151890027683, "length_ratio": 0.11919685541272708, "translation_length": 1122, "reference_length": 9413}
{"bleu": 0.0, "precisions": [0.058823529411764705, 0.0, 0.0, 0.0], "brevity_penalty": 0.49367278838913026, "length_ratio": 0.5862068965517241, "translation_length": 17, "reference_length": 29}
{"bleu": 0.006100883703252318, "precisions": [0.0794392523364486, 0.021052631578947368, 0.01639344262295082, 0.012895662368112544], "brevity_penalty": 0.2501951617436665, "length_ratio": 0.41919686581782567, "translation_length": 856, "reference_length": 2042}
{"bleu": 0.010561908734412581, "precisions": [0.06168446026097272, 0.011876484560570071, 0.0047562425683709865, 0.0035714285714285713], "brevity_penalty": 1.0, "length_ratio": 6.637795275590551, "translation_length": 843, "reference_length": 127}
{"bleu": 0.0, "precisions": [0.14285714285714285, 0.0, 0.0, 0.0], "brevity_penalty": 0.25133906849616483, "length_ratio": 0.42, "translation_length": 21, "reference_length": 50}
{"bleu": 0.0, "precisions": [0.0, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 3.0, "translation_length": 24, "reference_length": 8}
{"bleu": 0.0, "precisions": [0.07692307692307693, 0.0, 0.0, 0.0], "brevity_penalty": 5.547604783137187e-26, "length_ratio": 0.016905071521456438, "translation_length": 13, "reference_length": 769}
{"bleu": 0.041025196918641475, "precisions": [0.06389452332657201, 0.04060913705583756, 0.03353658536585366, 0.03255340793489318], "brevity_penalty": 1.0, "length_ratio": 3.8515625, "translation_length": 986, "reference_length": 256}
{"bleu": 0.12193951969929238, "precisions": [0.9572192513368984, 0.9236947791164659, 0.9048257372654156, 0.8926174496644296], "brevity_penalty": 0.13264837139465618, "length_ratio": 0.33111996458610005, "translation_length": 748, "reference_length": 2259}
{"bleu": 0.0, "precisions": [0.004766444232602479, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 61.705882352941174, "translation_length": 1049, "reference_length": 17}
{"bleu": 0.0, "precisions": [0.07692307692307693, 0.0, 0.0, 0.0], "brevity_penalty": 0.5404329964865341, "length_ratio": 0.6190476190476191, "translation_length": 13, "reference_length": 21}
{"bleu": 0.13303437989261152, "precisions": [0.29441624365482233, 0.15593220338983052, 0.12563667232597622, 0.09523809523809523], "brevity_penalty": 0.8689758607306973, "length_ratio": 0.8768545994065282, "translation_length": 591, "reference_length": 674}
{"bleu": 0.028395945550468003, "precisions": [0.14381591562799617, 0.03262955854126679, 0.01440922190201729, 0.009615384615384616], "brevity_penalty": 1.0, "length_ratio": 2.5753086419753086, "translation_length": 1043, "reference_length": 405}
{"bleu": 0.012930520588209825, "precisions": [0.022244191794364803, 0.012858555885262116, 0.010390895596239486, 0.009405940594059406], "brevity_penalty": 1.0, "length_ratio": 5.38031914893617, "translation_length": 2023, "reference_length": 376}
{"bleu": 0.0, "precisions": [0.037241379310344824, 0.0055248618784530384, 0.0027662517289073307, 0.0], "brevity_penalty": 1.0, "length_ratio": 11.885245901639344, "translation_length": 725, "reference_length": 61}
{"error": "CUDA out of memory. Tried to allocate 296.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 274.69 MiB is free. Including non-PyTorch memory, this process has 10.47 GiB memory in use. Of the allocated memory 10.04 GiB is allocated by PyTorch, and 231.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"error": "CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 12.69 MiB is free. Including non-PyTorch memory, this process has 10.73 GiB memory in use. Of the allocated memory 10.43 GiB is allocated by PyTorch, and 99.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"bleu": 0.0, "precisions": [0.21189979123173278, 0.003134796238244514, 0.0, 0.0], "brevity_penalty": 0.3448250255795695, "length_ratio": 0.48432760364004046, "translation_length": 958, "reference_length": 1978}
{"bleu": 0.048079831173912235, "precisions": [0.055379746835443035, 0.05071315372424723, 0.046031746031746035, 0.04133545310015898], "brevity_penalty": 1.0, "length_ratio": 18.057142857142857, "translation_length": 632, "reference_length": 35}
{"bleu": 0.077735212106618, "precisions": [0.18306351183063513, 0.08354114713216958, 0.056179775280898875, 0.0425], "brevity_penalty": 1.0, "length_ratio": 2.102094240837696, "translation_length": 803, "reference_length": 382}
{"bleu": 0.0, "precisions": [0.0, 0.0, 0.0, 0.0], "brevity_penalty": 0.08677432947392927, "length_ratio": 0.2903225806451613, "translation_length": 9, "reference_length": 31}
{"bleu": 0.0, "precisions": [0.0, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 1.0769230769230769, "translation_length": 42, "reference_length": 39}
{"bleu": 0.0, "precisions": [0.21739130434782608, 0.0, 0.0, 0.0], "brevity_penalty": 3.3428204802123657e-06, "length_ratio": 0.07348242811501597, "translation_length": 23, "reference_length": 313}
{"bleu": 0.0, "precisions": [0.1864406779661017, 0.0, 0.0, 0.0], "brevity_penalty": 9.815774063096941e-22, "length_ratio": 0.020254033642293168, "translation_length": 59, "reference_length": 2913}
{"bleu": 0.0, "precisions": [0.1, 0.0, 0.0, 0.0], "brevity_penalty": 8.29381916075737e-06, "length_ratio": 0.07874015748031496, "translation_length": 10, "reference_length": 127}
{"bleu": 0.0, "precisions": [0.1, 0.0, 0.0, 0.0], "brevity_penalty": 0.036883167401240015, "length_ratio": 0.23255813953488372, "translation_length": 10, "reference_length": 43}
{"bleu": 0.03838463514725478, "precisions": [0.04083885209713024, 0.03867403314917127, 0.03761061946902655, 0.036544850498338874], "brevity_penalty": 1.0, "length_ratio": 21.069767441860463, "translation_length": 906, "reference_length": 43}
{"bleu": 0.1302853196657505, "precisions": [0.2978723404255319, 0.16129032258064516, 0.13043478260869565, 0.0989010989010989], "brevity_penalty": 0.8257284094045387, "length_ratio": 0.8392857142857143, "translation_length": 94, "reference_length": 112}
{"bleu": 2.9480112222630955e-06, "precisions": [0.4851104707012488, 0.29423076923076924, 0.17709335899903753, 0.08092485549132948], "brevity_penalty": 1.3862000062874214e-05, "length_ratio": 0.08205896263597667, "translation_length": 1041, "reference_length": 12686}
{"error": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"}
{"bleu": 0.0, "precisions": [0.0032258064516129032, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 11.481481481481481, "translation_length": 310, "reference_length": 27}
{"bleu": 0.0, "precisions": [0.07692307692307693, 0.0, 0.0, 0.0], "brevity_penalty": 0.08530361363583897, "length_ratio": 0.28888888888888886, "translation_length": 13, "reference_length": 45}
{"bleu": 0.010443258515998437, "precisions": [0.08243727598566308, 0.025119617224880382, 0.004790419161676647, 0.001199040767386091], "brevity_penalty": 1.0, "length_ratio": 6.293233082706767, "translation_length": 837, "reference_length": 133}
{"bleu": 0.1397279014324988, "precisions": [0.4217002237136465, 0.20492721164613661, 0.15134529147982062, 0.1414141414141414], "brevity_penalty": 0.6737780087935751, "length_ratio": 0.7169206094627105, "translation_length": 894, "reference_length": 1247}
{"error": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"}
{"bleu": 0.0, "precisions": [0.012218045112781954, 0.0009407337723424271, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 21.714285714285715, "translation_length": 1064, "reference_length": 49}
{"bleu": 0.017318440081598434, "precisions": [0.033066132264529056, 0.0160481444332999, 0.014056224899598393, 0.012060301507537688], "brevity_penalty": 1.0, "length_ratio": 14.46376811594203, "translation_length": 998, "reference_length": 69}
{"bleu": 0.0775441358725682, "precisions": [0.08152173913043478, 0.07891156462585033, 0.07629427792915532, 0.07366984993178717], "brevity_penalty": 1.0, "length_ratio": 10.985074626865671, "translation_length": 736, "reference_length": 67}
{"bleu": 0.011824531915027381, "precisions": [0.02064896755162242, 0.010826771653543307, 0.009852216748768473, 0.008875739644970414], "brevity_penalty": 1.0, "length_ratio": 5.409574468085107, "translation_length": 1017, "reference_length": 188}
{"bleu": 0.02108890571162528, "precisions": [0.4161849710982659, 0.12790697674418605, 0.05847953216374269, 0.041176470588235294], "brevity_penalty": 0.19819634270060826, "length_ratio": 0.3818984547461369, "translation_length": 173, "reference_length": 453}
{"error": "Unsloth: input length 307022 + max_new_tokens 1024 exceeds the maximum sequence length of 131072!\nYou will need to do long context extension by increasing the `max_seq_length` in `FastLanguageModel.from_pretrained`."}
{"error": "Unsloth: input length 661167 + max_new_tokens 1024 exceeds the maximum sequence length of 131072!\nYou will need to do long context extension by increasing the `max_seq_length` in `FastLanguageModel.from_pretrained`."}
{"bleu": 9.311743909012624e-32, "precisions": [0.9354838709677419, 0.8, 0.7586206896551724, 0.7142857142857143], "brevity_penalty": 1.1668775623304045e-31, "length_ratio": 0.013845466726217061, "translation_length": 31, "reference_length": 2239}
{"bleu": 0.008410543673356207, "precisions": [0.027544910179640718, 0.008393285371702638, 0.006002400960384154, 0.003605769230769231], "brevity_penalty": 1.0, "length_ratio": 18.555555555555557, "translation_length": 835, "reference_length": 45}
{"bleu": 0.009596964593258326, "precisions": [0.06209150326797386, 0.007633587786259542, 0.0054585152838427945, 0.003278688524590164], "brevity_penalty": 1.0, "length_ratio": 6.60431654676259, "translation_length": 918, "reference_length": 139}
{"bleu": 0.0, "precisions": [0.04142011834319527, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 6.671052631578948, "translation_length": 1014, "reference_length": 152}
{"bleu": 0.0, "precisions": [0.004878048780487805, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 22.77777777777778, "translation_length": 1025, "reference_length": 45}
{"error": "Unsloth: input length 241184 + max_new_tokens 1024 exceeds the maximum sequence length of 131072!\nYou will need to do long context extension by increasing the `max_seq_length` in `FastLanguageModel.from_pretrained`."}
{"bleu": 0.0, "precisions": [0.06666666666666667, 0.0, 0.0, 0.0], "brevity_penalty": 0.6703200460356393, "length_ratio": 0.7142857142857143, "translation_length": 15, "reference_length": 21}
{"bleu": 2.0603655352103393e-06, "precisions": [0.9294117647058824, 0.8928571428571429, 0.891566265060241, 0.8902439024390244], "brevity_penalty": 2.287078557068446e-06, "length_ratio": 0.07148864592094197, "translation_length": 85, "reference_length": 1189}
{"error": "CUDA out of memory. Tried to allocate 544.00 MiB. GPU 0 has a total capacity of 10.75 GiB of which 376.69 MiB is free. Including non-PyTorch memory, this process has 10.38 GiB memory in use. Of the allocated memory 10.05 GiB is allocated by PyTorch, and 118.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"}
{"bleu": 0.0, "precisions": [0.3333333333333333, 0.0, 0.0, 0.0], "brevity_penalty": 9.137872601482476e-22, "length_ratio": 0.020224719101123594, "translation_length": 9, "reference_length": 445}
{"bleu": 0.008269688129403532, "precisions": [0.0417940876656473, 0.007142857142857143, 0.005107252298263534, 0.003067484662576687], "brevity_penalty": 1.0, "length_ratio": 8.456896551724139, "translation_length": 981, "reference_length": 116}
{"bleu": 0.0, "precisions": [0.0019193857965451055, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 97.6875, "translation_length": 1563, "reference_length": 16}
{"bleu": 0.0836520559701697, "precisions": [0.38358974358974357, 0.2741273100616016, 0.22404933196300103, 0.17386831275720166], "brevity_penalty": 0.33065932386456764, "length_ratio": 0.47468354430379744, "translation_length": 975, "reference_length": 2054}
{"bleu": 0.0, "precisions": [0.17391304347826086, 0.0, 0.0, 0.0], "brevity_penalty": 0.006737946999085467, "length_ratio": 0.16666666666666666, "translation_length": 23, "reference_length": 138}
{"bleu": 7.766144392196875e-06, "precisions": [0.1577319587628866, 0.03611971104231166, 0.029958677685950414, 0.02688728024819028], "brevity_penalty": 0.0001677923247982393, "length_ratio": 0.10316953839608593, "translation_length": 970, "reference_length": 9402}
{"bleu": 0.03884567846671197, "precisions": [0.041379310344827586, 0.039723661485319514, 0.03806228373702422, 0.036395147313691506], "brevity_penalty": 1.0, "length_ratio": 18.70967741935484, "translation_length": 580, "reference_length": 31}
{"bleu": 0.0, "precisions": [0.017676767676767676, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 26.4, "translation_length": 792, "reference_length": 30}
{"bleu": 2.6886767058117117e-152, "precisions": [0.7209302325581395, 0.23809523809523808, 0.0975609756097561, 0.05], "brevity_penalty": 1.5805784736031438e-151, "length_ratio": 0.0028716441832509682, "translation_length": 43, "reference_length": 14974}
{"bleu": 0.0, "precisions": [0.07692307692307693, 0.0, 0.0, 0.0], "brevity_penalty": 0.08530361363583897, "length_ratio": 0.28888888888888886, "translation_length": 13, "reference_length": 45}
{"bleu": 0.0, "precisions": [0.0, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 2.4285714285714284, "translation_length": 17, "reference_length": 7}
{"bleu": 0.0, "precisions": [0.16666666666666666, 0.0, 0.0, 0.0], "brevity_penalty": 3.398267819495071e-09, "length_ratio": 0.04878048780487805, "translation_length": 18, "reference_length": 369}
{"bleu": 0.0971049591197442, "precisions": [0.11204481792717087, 0.09532710280373832, 0.09260991580916744, 0.0898876404494382], "brevity_penalty": 1.0, "length_ratio": 5.549222797927461, "translation_length": 1071, "reference_length": 193}
{"bleu": 0.0, "precisions": [0.07692307692307693, 0.0, 0.0, 0.0], "brevity_penalty": 0.8574039191604413, "length_ratio": 0.8666666666666667, "translation_length": 13, "reference_length": 15}
{"bleu": 0.0, "precisions": [0.045454545454545456, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 1.0476190476190477, "translation_length": 22, "reference_length": 21}
{"bleu": 0.020540406858030078, "precisions": [0.024110218140068886, 0.021839080459770115, 0.019562715765247412, 0.01728110599078341], "brevity_penalty": 1.0, "length_ratio": 30.03448275862069, "translation_length": 871, "reference_length": 29}
{"bleu": 0.0, "precisions": [0.046753246753246755, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 3.5, "translation_length": 770, "reference_length": 220}
{"bleu": 0.04228314605431585, "precisions": [0.044063647490820076, 0.0428921568627451, 0.04171779141104295, 0.04054054054054054], "brevity_penalty": 1.0, "length_ratio": 19.0, "translation_length": 817, "reference_length": 43}
{"bleu": 0.011365620517127348, "precisions": [0.0927734375, 0.007820136852394917, 0.005870841487279843, 0.0039177277179236044], "brevity_penalty": 1.0, "length_ratio": 3.7236363636363636, "translation_length": 1024, "reference_length": 275}
{"bleu": 0.0, "precisions": [0.008188331627430911, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 54.27777777777778, "translation_length": 977, "reference_length": 18}
{"bleu": 0.0, "precisions": [0.5714285714285714, 0.0, 0.0, 0.0], "brevity_penalty": 4.224157638199146e-60, "length_ratio": 0.007261410788381743, "translation_length": 7, "reference_length": 964}
{"bleu": 0.2848803786141389, "precisions": [0.4789391575663027, 0.25625, 0.23943661971830985, 0.22413793103448276], "brevity_penalty": 1.0, "length_ratio": 1.1826568265682658, "translation_length": 641, "reference_length": 542}
{"bleu": 1.010017253925352e-09, "precisions": [0.9382716049382716, 0.9, 0.8987341772151899, 0.8974358974358975], "brevity_penalty": 1.1118017375133895e-09, "length_ratio": 0.04625928041119361, "translation_length": 81, "reference_length": 1751}
{"bleu": 0.3843459630493653, "precisions": [0.3854166666666667, 0.3847045191193511, 0.3839907192575406, 0.3832752613240418], "brevity_penalty": 1.0, "length_ratio": 2.541176470588235, "translation_length": 864, "reference_length": 340}
{"bleu": 0.0, "precisions": [0.10436893203883495, 0.0036452004860267314, 0.0012165450121654502, 0.0], "brevity_penalty": 1.0, "length_ratio": 1.1444444444444444, "translation_length": 824, "reference_length": 720}
{"bleu": 0.0, "precisions": [0.00145985401459854, 0.0, 0.0, 0.0], "brevity_penalty": 0.2377593264477445, "length_ratio": 0.4104254044337927, "translation_length": 685, "reference_length": 1669}
{"bleu": 0.0, "precisions": [0.1044776119402985, 0.0, 0.0, 0.0], "brevity_penalty": 0.6020211589297388, "length_ratio": 0.6633663366336634, "translation_length": 67, "reference_length": 101}
{"bleu": 0.0, "precisions": [0.8076923076923077, 0.24, 0.041666666666666664, 0.0], "brevity_penalty": 1.3354592765447732e-222, "length_ratio": 0.0019535652565932826, "translation_length": 26, "reference_length": 13309}
{"bleu": 0.0, "precisions": [0.23076923076923078, 0.0, 0.0, 0.0], "brevity_penalty": 5.492956807014363e-43, "length_ratio": 0.010172143974960876, "translation_length": 13, "reference_length": 1278}
{"bleu": 0.0, "precisions": [0.14814814814814814, 0.0, 0.0, 0.0], "brevity_penalty": 0.007529784255650023, "length_ratio": 0.16981132075471697, "translation_length": 27, "reference_length": 159}
{"error": "Unsloth: input length 161192 + max_new_tokens 1024 exceeds the maximum sequence length of 131072!\nYou will need to do long context extension by increasing the `max_seq_length` in `FastLanguageModel.from_pretrained`."}
{"bleu": 0.030306180508802752, "precisions": [0.6114592658907789, 0.45698924731182794, 0.4349775784753363, 0.41741472172351884], "brevity_penalty": 0.06385636153716397, "length_ratio": 0.2665871121718377, "translation_length": 1117, "reference_length": 4190}
{"bleu": 0.12311474381421206, "precisions": [0.3078817733990148, 0.10928512736236648, 0.08552631578947369, 0.07983539094650206], "brevity_penalty": 1.0, "length_ratio": 1.3473451327433628, "translation_length": 1218, "reference_length": 904}
{"bleu": 0.0006945358849174598, "precisions": [0.9473684210526315, 0.9066666666666666, 0.9054054054054054, 0.9041095890410958], "brevity_penalty": 0.0007584675618178816, "length_ratio": 0.12218649517684887, "translation_length": 76, "reference_length": 622}
{"bleu": 0.18506668093327724, "precisions": [0.9158878504672897, 0.8475210477081384, 0.8061797752808989, 0.7647610121836926], "brevity_penalty": 0.22250543775687054, "length_ratio": 0.39955190440627336, "translation_length": 1070, "reference_length": 2678}
{"bleu": 0.0, "precisions": [0.125, 0.0, 0.0, 0.0], "brevity_penalty": 0.012588142242433998, "length_ratio": 0.18604651162790697, "translation_length": 8, "reference_length": 43}
{"bleu": 0.0, "precisions": [0.465016146393972, 0.02478448275862069, 0.0, 0.0], "brevity_penalty": 3.403759228728488e-09, "length_ratio": 0.04878433020007352, "translation_length": 929, "reference_length": 19043}
{"bleu": 0.0, "precisions": [0.1, 0.0, 0.0, 0.0], "brevity_penalty": 0.0005004514334406108, "length_ratio": 0.11627906976744186, "translation_length": 10, "reference_length": 86}
{"bleu": 0.0, "precisions": [0.19047619047619047, 0.0, 0.0, 0.0], "brevity_penalty": 7.3270263916878265e-87, "length_ratio": 0.005016722408026756, "translation_length": 21, "reference_length": 4186}
{"bleu": 0.0, "precisions": [0.0, 0.0, 0.0, 0.0], "brevity_penalty": 0.03877420783172201, "length_ratio": 0.23529411764705882, "translation_length": 8, "reference_length": 34}
{"bleu": 0.15166668578754292, "precisions": [0.6735159817351598, 0.6681922196796338, 0.6674311926605505, 0.6666666666666666], "brevity_penalty": 0.2267248151757424, "length_ratio": 0.4025735294117647, "translation_length": 438, "reference_length": 1088}
{"bleu": 0.0, "precisions": [0.0, 0.0, 0.0, 0.0], "brevity_penalty": 0.27253179303401254, "length_ratio": 0.43478260869565216, "translation_length": 10, "reference_length": 23}
{"bleu": 0.0, "precisions": [0.1, 0.0, 0.0, 0.0], "brevity_penalty": 1.3674196065680938e-05, "length_ratio": 0.08196721311475409, "translation_length": 10, "reference_length": 122}
{"bleu": 0.0, "precisions": [0.35714285714285715, 0.0, 0.0, 0.0], "brevity_penalty": 9.07676636045989e-63, "length_ratio": 0.006951340615690168, "translation_length": 14, "reference_length": 2014}
{"bleu": 0.0, "precisions": [0.15384615384615385, 0.0, 0.0, 0.0], "brevity_penalty": 1.7587922024243116e-25, "length_ratio": 0.017241379310344827, "translation_length": 13, "reference_length": 754}
{"bleu": 0.018951126589338632, "precisions": [0.08892921960072596, 0.01818181818181818, 0.01092896174863388, 0.0072992700729927005], "brevity_penalty": 1.0, "length_ratio": 6.191011235955056, "translation_length": 551, "reference_length": 89}
{"bleu": 0.07296176766066335, "precisions": [0.4054848188050931, 0.20588235294117646, 0.13837095191364082, 0.07072691552062868], "brevity_penalty": 0.4315583310762873, "length_ratio": 0.5433741351782864, "translation_length": 1021, "reference_length": 1879}
{"bleu": 2.231401977237173e-07, "precisions": [0.5882352941176471, 0.3125, 0.26666666666666666, 0.21428571428571427], "brevity_penalty": 6.970068570852936e-07, "length_ratio": 0.06589147286821706, "translation_length": 17, "reference_length": 258}
{"bleu": 0.0, "precisions": [0.0009799118079372856, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 75.5925925925926, "translation_length": 2041, "reference_length": 27}
{"bleu": 0.0, "precisions": [0.11764705882352941, 0.0, 0.0, 0.0], "brevity_penalty": 0.04426118996980666, "length_ratio": 0.24285714285714285, "translation_length": 17, "reference_length": 70}
{"bleu": 0.0, "precisions": [0.07692307692307693, 0.0, 0.0, 0.0], "brevity_penalty": 7.061551457952061e-39, "length_ratio": 0.011255411255411256, "translation_length": 13, "reference_length": 1155}
{"bleu": 0.0, "precisions": [0.022198731501057084, 0.0021164021164021165, 0.001059322033898305, 0.0], "brevity_penalty": 1.0, "length_ratio": 11.679012345679013, "translation_length": 946, "reference_length": 81}
{"bleu": 0.0, "precisions": [0.0004889975550122249, 0.0, 0.0, 0.0], "brevity_penalty": 0.37976024525473395, "length_ratio": 0.5080745341614907, "translation_length": 2045, "reference_length": 4025}
{"bleu": 5.204614879866456e-122, "precisions": [0.6060606060606061, 0.15625, 0.0967741935483871, 0.03333333333333333], "brevity_penalty": 3.936814957056148e-121, "length_ratio": 0.003593988237856676, "translation_length": 33, "reference_length": 9182}
{"bleu": 0.04879124317456138, "precisions": [0.9047619047619048, 0.8674698795180723, 0.8414634146341463, 0.8271604938271605], "brevity_penalty": 0.05675295127950596, "length_ratio": 0.25846153846153846, "translation_length": 84, "reference_length": 325}
{"bleu": 0.003765538923027153, "precisions": [0.041743970315398886, 0.002785515320334262, 0.0018587360594795538, 0.0009302325581395349], "brevity_penalty": 1.0, "length_ratio": 7.538461538461538, "translation_length": 1078, "reference_length": 143}
{"bleu": 0.31454810559557456, "precisions": [0.8771929824561403, 0.7553324968632371, 0.6344221105527639, 0.5132075471698113], "brevity_penalty": 0.4615416603987768, "length_ratio": 0.5639575971731449, "translation_length": 798, "reference_length": 1415}
{"bleu": 0.0, "precisions": [0.0, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 145.14285714285714, "translation_length": 1016, "reference_length": 7}
{"bleu": 0.037443126017164145, "precisions": [0.7024221453287197, 0.42032332563510394, 0.39075144508670523, 0.37962962962962965], "brevity_penalty": 0.0818486469342056, "length_ratio": 0.28547909120842935, "translation_length": 867, "reference_length": 3037}
{"bleu": 0.3550861292174889, "precisions": [0.37440191387559807, 0.35568862275449104, 0.3489208633093525, 0.34213685474189676], "brevity_penalty": 1.0, "length_ratio": 2.458823529411765, "translation_length": 836, "reference_length": 340}
{"bleu": 0.0, "precisions": [0.0, 0.0, 0.0, 0.0], "brevity_penalty": 1.0, "length_ratio": 1.5714285714285714, "translation_length": 22, "reference_length": 14}
